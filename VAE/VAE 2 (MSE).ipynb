{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = \"../Data/Raw/\"\n",
    "PROCESSED_DATA_DIR = \"../Data/Processed/VAE/\"\n",
    "\n",
    "class CovariateDataset(Dataset):\n",
    "    def __init__(self, file_name_pattern, file_name_args):\n",
    "        self.file_name = file_name_pattern.format(*file_name_args)\n",
    "        self.data = np.loadtxt(RAW_DATA_DIR + self.file_name + \".csv\", delimiter=\",\")[:, 1:] # remove bias\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index].astype(float), 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def save_processed_data(self, data):\n",
    "        name = PROCESSED_DATA_DIR + self.file_name+\".csv\"\n",
    "        np.savetxt(name, data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Based on an example from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "# Extended to place a different prior on binary vs normal vars\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "ZDIMS = 4 # latent dimensions\n",
    "INTERMEDIATE_DIMS = 32\n",
    "FEATURES = 10\n",
    "DIAG_VAR = True\n",
    "\n",
    "BINARY = [0, 2, 5, 7, 8]\n",
    "NORMAL = [1, 3, 5, 6, 9]\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # ENCODER LAYERS\n",
    "        self.dense1 = nn.Linear(FEATURES, INTERMEDIATE_DIMS)\n",
    "        self.dense2_1 = nn.Linear(INTERMEDIATE_DIMS, ZDIMS)  # mu layer\n",
    "        self.dense2_2 = nn.Linear(INTERMEDIATE_DIMS, ZDIMS)  # logvariance layer\n",
    "        \n",
    "        # this last layer bottlenecks through ZDIMS connections\n",
    "\n",
    "        # DECODER LAYERS\n",
    "        self.dense3 = nn.Linear(ZDIMS, INTERMEDIATE_DIMS)\n",
    "        self.dense4 = nn.Linear(INTERMEDIATE_DIMS, FEATURES)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.dense1(x))\n",
    "        return self.dense2_1(h1), self.dense2_2(h1) #mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # If we sampled directly from the latent distribution\n",
    "            # we wouldn't be able to backprop the results because\n",
    "            # there is no clear grad on the distribution\n",
    "\n",
    "            # This reparam samples from a unit gaussian and then scales\n",
    "            # by the latent parameters giving a defined route to backprop.\n",
    "\n",
    "            std = logvar.mul(0.5).exp_() \n",
    "\n",
    "            # Sample from a unit gaussian with dimensions matching\n",
    "            # the latent space.\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "\n",
    "            return eps.mul(std).add_(mu) # rescale and return\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.dense3(z))\n",
    "        mu_out = self.dense4(h3)# Deleted: self.sigmoid(self.dense4(h3))\n",
    "        \n",
    "        return mu_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, FEATURES))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        mu_out = self.decode(z)\n",
    "        return mu_out, mu, logvar\n",
    "\n",
    "def loss_function(recon_batch_mu, batch_x, mu_latent, logvar_latent):\n",
    "    \n",
    "    # MSE: how good is the reconstruction in terms of\n",
    "    mse_loss = nn.MSELoss(size_average=False)\n",
    "    recon_loss = mse_loss(recon_batch_mu, batch_x)\n",
    "    \n",
    "    recon_loss /= batch_x.size()[0]\n",
    "    \n",
    "    # KLD is Kullbackâ€“Leibler divergence. Regularize VAE by\n",
    "    # penalizing divergence from the prior\n",
    "\n",
    "    # See Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    KLD = -0.5 * torch.sum(1 + logvar_latent - mu_latent.pow(2) - logvar_latent.exp())\n",
    "    # Normalise by same number of elements as in reconstruction\n",
    "    KLD /= batch_x.size()[0] * FEATURES\n",
    "    \n",
    "#     print(\"RL\", recon_loss)\n",
    "#     print(\"KLD\", KLD)\n",
    "    return recon_loss + KLD\n",
    "\n",
    "def train(model, optimizer, epoch, data_loader, log_results=False):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):\n",
    "        data = Variable(data)\n",
    "        data = data.float()\n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon_data, mu_latent, logvar_latent = model(data)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = loss_function(recon_data, data, mu_latent, logvar_latent)\n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        # Find the gradient and descend\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if log_results:\n",
    "        print('====> Epoch: {} Average loss: {:.8f}'.format(\n",
    "              epoch, train_loss / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Based on an example from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "# Extended to place a different prior on binary vs normal vars\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "ZDIMS = 4 # latent dimensions\n",
    "INTERMEDIATE_DIMS = 32\n",
    "FEATURES = 10\n",
    "DIAG_VAR = True\n",
    "\n",
    "BINARY = [0, 2, 5, 7, 8]\n",
    "NORMAL = [1, 3, 5, 6, 9]\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "class ModifiedVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedVAE, self).__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # ENCODER LAYERS\n",
    "        self.dense1 = nn.Linear(FEATURES, INTERMEDIATE_DIMS)\n",
    "        self.dense2_1 = nn.Linear(INTERMEDIATE_DIMS, ZDIMS)  # mu layer\n",
    "        self.dense2_2 = nn.Linear(INTERMEDIATE_DIMS, ZDIMS)  # logvariance layer\n",
    "        \n",
    "        # this last layer bottlenecks through ZDIMS connections\n",
    "\n",
    "        # DECODER LAYERS\n",
    "        self.dense3 = nn.Linear(ZDIMS, INTERMEDIATE_DIMS)\n",
    "        self.dense4 = nn.Linear(INTERMEDIATE_DIMS, len(BINARY))\n",
    "        self.dense5 = nn.Linear(INTERMEDIATE_DIMS, len(NORMAL))\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.dense1(x))\n",
    "        return self.dense2_1(h1), self.dense2_2(h1) #mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # If we sampled directly from the latent distribution\n",
    "            # we wouldn't be able to backprop the results because\n",
    "            # there is no clear grad on the distribution\n",
    "\n",
    "            # This reparam samples from a unit gaussian and then scales\n",
    "            # by the latent parameters giving a defined route to backprop.\n",
    "\n",
    "            std = logvar.mul(0.5).exp_() \n",
    "\n",
    "            # Sample from a unit gaussian with dimensions matching\n",
    "            # the latent space.\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "\n",
    "            return eps.mul(std).add_(mu) # rescale and return\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.dense3(z))\n",
    "        binary_mu_out = self.sigmoid(self.dense4(h3))\n",
    "        normal_mu_out = self.dense5(h3)\n",
    "        \n",
    "        return binary_mu_out, normal_mu_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_mu, latent_logvar = self.encode(x.view(-1, FEATURES))\n",
    "        z = self.reparameterize(latent_mu, latent_logvar)\n",
    "        binary_mu_out, normal_mu_out = self.decode(z)\n",
    "        return binary_mu_out, normal_mu_out, latent_mu, latent_logvar\n",
    "\n",
    "def loss_function(recon_binary_mu, recon_normal_mu, batch_x, mu_latent, logvar_latent):\n",
    "    \n",
    "    # MSE: how good is the reconstruction in terms of\n",
    "    mse_loss = nn.MSELoss(size_average=False)\n",
    "    normal_recon_loss = mse_loss(recon_normal_mu, batch_x[:, NORMAL])\n",
    "    normal_recon_loss /= (batch_x.size()[0])\n",
    "    \n",
    "    # Cross Entropy:\n",
    "    BCE = F.binary_cross_entropy(recon_binary_mu, batch_x[:, BINARY], size_average=False)\n",
    "    BCE /= (batch_x.size()[0])\n",
    "    \n",
    "    # KLD is Kullbackâ€“Leibler divergence. Regularize VAE by\n",
    "    # penalizing divergence from the prior\n",
    "\n",
    "    # See Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    KLD = -0.5 * torch.sum(1 + logvar_latent - mu_latent.pow(2) - logvar_latent.exp())\n",
    "    # Normalise by same number of elements as in reconstruction\n",
    "    KLD /= batch_x.size()[0] * FEATURES\n",
    "    \n",
    "#     print(\"RL\", normal_recon_loss.data.cpu().numpy()[0])\n",
    "#     print(\"BCE\", BCE.data.cpu().numpy()[0])\n",
    "#     print(\"KLD\", KLD.data.cpu().numpy()[0])\n",
    "    \n",
    "    return normal_recon_loss + BCE + KLD\n",
    "\n",
    "def train(model, optimizer, epoch, data_loader, log_results=False):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):\n",
    "        data = Variable(data)\n",
    "        data = data.float()\n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        binary_mu_out, normal_mu_out, mu_latent, logvar_latent = model(data)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = loss_function(binary_mu_out, normal_mu_out, data, mu_latent, logvar_latent)\n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        # Find the gradient and descend\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if log_results:\n",
    "        print('====> Epoch: {} Average loss: {:.8f}'.format(\n",
    "              epoch, train_loss / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Process Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model_class, dataset, dataset_number, verbose=True):\n",
    "    model = model_class()\n",
    "    if CUDA:\n",
    "        model = model.cuda()\n",
    "\n",
    "    num_epochs = 10000\n",
    "    batch_size = 1000\n",
    "    learning_rate = 1e-2\n",
    "    lr_sched = False\n",
    "         \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [int(num_epochs/4), int(3*num_epochs/4)], gamma=0.1)\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        checkpoint_interval = int(num_epochs/10)\n",
    "        \n",
    "        if lr_sched:\n",
    "            scheduler.step()\n",
    "\n",
    "        log = False\n",
    "        if epoch%checkpoint_interval == 0:\n",
    "            log = True\n",
    "            \n",
    "        train(model, optimizer, epoch, data_loader, log_results=log)\n",
    "    \n",
    "\n",
    "    torch.save(model.state_dict(), \"../Models/VAE_{}.pth\".format(dataset_number))\n",
    "    \n",
    "    # Show reconstruction\n",
    "    model.eval()\n",
    "    print(\"Training state: \", model.training)\n",
    "    \n",
    "    original_data,_ = next(iter(data_loader))\n",
    "    original_data = Variable(original_data)\n",
    "    original_data = original_data.float()\n",
    "    if CUDA:\n",
    "        original_data = original_data.cuda()\n",
    "        \n",
    "    binary_mu_out, normal_mu_out, mu_latent, logvar_latent = model(original_data)\n",
    "    \n",
    "    return model, original_data, binary_mu_out, normal_mu_out, mu_latent, logvar_latent\n",
    "\n",
    "def encode_data(model, dataset):\n",
    "    all_data = torch.from_numpy(dataset.data)\n",
    "    all_data = Variable(all_data)\n",
    "    all_data = all_data.float()\n",
    "    \n",
    "    if CUDA:\n",
    "        all_data = all_data.cuda()\n",
    "\n",
    "    latent_mu, latent_var = model.encode(all_data)\n",
    "    \n",
    "    if CUDA:\n",
    "        latent_mu = latent_mu.cpu()\n",
    "        latent_var = latent_var.cpu()\n",
    "        \n",
    "    data = np.hstack([latent_mu.data.numpy(), latent_var.data.numpy()])\n",
    "    dataset.save_processed_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 Average loss: 0.00720191\n",
      "====> Epoch: 20 Average loss: 0.00628152\n",
      "====> Epoch: 30 Average loss: 0.00559646\n",
      "====> Epoch: 40 Average loss: 0.00534063\n",
      "====> Epoch: 50 Average loss: 0.00487277\n",
      "====> Epoch: 60 Average loss: 0.00463829\n",
      "====> Epoch: 70 Average loss: 0.00453654\n",
      "====> Epoch: 80 Average loss: 0.00445161\n",
      "====> Epoch: 90 Average loss: 0.00436299\n",
      "====> Epoch: 100 Average loss: 0.00429253\n",
      "Training state:  False\n"
     ]
    }
   ],
   "source": [
    "dataset = CovariateDataset(\"n_{}_model_{}_v_{}_covar_data\", [1000, \"A_add_lin\", 1])\n",
    "trained_model, original_data, binary_mu_out, normal_mu_out, mu_latent, logvar_latent = \\\n",
    "    train_model(ModifiedVAE, dataset, 1,verbose=True)\n",
    "\n",
    "encode_data(trained_model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal: [1.0, 1.0, 1.0, -1.32, 0.02, 1.0, -0.35, 0.0, 1.0, -0.93]\n",
      "Mu out: [1.0, 1.15, 1.0, -1.01, 0.0, 0.98, -0.18, 0.12, 1.0, -1.18]\n",
      "Mu Latent: [1.1, -0.14, 1.1, -0.44]\n",
      "Std latent: [0.06, 0.03, 0.06, 0.07]\n",
      "\n",
      "Orginal: [1.0, -1.1, 0.0, -0.12, -0.33, 0.0, -2.12, 0.0, 0.0, -1.77]\n",
      "Mu out: [0.36, -1.32, 0.02, 0.53, 0.0, 0.01, -2.24, 0.0, 0.0, -1.25]\n",
      "Mu Latent: [-1.4, -0.21, 1.64, 1.06]\n",
      "Std latent: [0.09, 0.03, 0.07, 0.1]\n",
      "\n",
      "Orginal: [0.0, -2.11, 0.0, 0.21, 0.58, 0.0, 0.38, 0.0, 0.0, 1.91]\n",
      "Mu out: [0.02, -1.66, 0.01, 0.41, 50774.44, 0.08, 0.46, 0.0, 0.0, 1.06]\n",
      "Mu Latent: [-1.34, 0.32, 0.84, 1.66]\n",
      "Std latent: [0.06, 0.03, 0.04, 0.07]\n",
      "\n",
      "Orginal: [1.0, 1.29, 0.0, 0.74, 0.57, 1.0, -1.03, 1.0, 0.0, -1.44]\n",
      "Mu out: [1.0, 1.09, 0.0, 0.78, 0.0, 0.97, -0.86, 1.0, 0.0, -1.25]\n",
      "Mu Latent: [0.34, -2.01, 0.03, -1.48]\n",
      "Std latent: [0.05, 0.19, 0.03, 0.16]\n",
      "\n",
      "Orginal: [1.0, -1.29, 1.0, 0.23, 1.92, 1.0, 1.66, 0.0, 1.0, 1.65]\n",
      "Mu out: [0.98, -1.53, 1.0, 0.19, 0.0, 0.97, 1.58, 0.0, 1.0, 1.81]\n",
      "Mu Latent: [-1.46, 0.14, -0.42, -0.0]\n",
      "Std latent: [0.05, 0.04, 0.05, 0.04]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mu_out = torch.Tensor(1000, 10)\n",
    "\n",
    "for index in BINARY:\n",
    "    mu_out[:, index] = binary_mu_out[:, BINARY.index(index)].data.cpu()\n",
    "    \n",
    "for index in NORMAL:\n",
    "    mu_out[:, index] = normal_mu_out[:, NORMAL.index(index)].data.cpu()\n",
    "    \n",
    "for i in np.random.choice(list(range(1000)), size=5, ):\n",
    "    print(\"Orginal:\", list(np.round(original_data[i].data.cpu().numpy(), 2)))\n",
    "    print(\"Mu out:\", list(np.round(mu_out[i].numpy(), 2)))\n",
    "    print(\"Mu Latent:\", list(np.round(mu_latent[i].data.cpu().numpy(), 2)))\n",
    "    print(\"Std latent:\", list(np.round(logvar_latent[i].mul(0.5).exp().data.cpu().numpy(), 2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run for Dataset 25\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00266409\n",
      "====> Epoch: 2000 Average loss: 0.00196114\n",
      "====> Epoch: 3000 Average loss: 0.00178170\n",
      "====> Epoch: 4000 Average loss: 0.00167729\n",
      "====> Epoch: 5000 Average loss: 0.00164007\n",
      "====> Epoch: 6000 Average loss: 0.00160345\n",
      "====> Epoch: 7000 Average loss: 0.00160483\n",
      "====> Epoch: 8000 Average loss: 0.00157590\n",
      "====> Epoch: 9000 Average loss: 0.00156361\n",
      "====> Epoch: 10000 Average loss: 0.00153984\n",
      "Training state:  False\n",
      "---- Done in  176.25343918800354  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00287171\n",
      "====> Epoch: 2000 Average loss: 0.00261413\n",
      "====> Epoch: 3000 Average loss: 0.00252240\n",
      "====> Epoch: 4000 Average loss: 0.00245967\n",
      "====> Epoch: 5000 Average loss: 0.00229948\n",
      "====> Epoch: 6000 Average loss: 0.00223907\n",
      "====> Epoch: 7000 Average loss: 0.00218793\n",
      "====> Epoch: 8000 Average loss: 0.00215036\n",
      "====> Epoch: 9000 Average loss: 0.00208439\n",
      "====> Epoch: 10000 Average loss: 0.00206078\n",
      "Training state:  False\n",
      "---- Done in  74.69603300094604  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00293841\n",
      "====> Epoch: 2000 Average loss: 0.00262392\n",
      "====> Epoch: 3000 Average loss: 0.00252926\n",
      "====> Epoch: 4000 Average loss: 0.00246943\n",
      "====> Epoch: 5000 Average loss: 0.00246275\n",
      "====> Epoch: 6000 Average loss: 0.00242101\n",
      "====> Epoch: 7000 Average loss: 0.00241969\n",
      "====> Epoch: 8000 Average loss: 0.00235830\n",
      "====> Epoch: 9000 Average loss: 0.00236172\n",
      "====> Epoch: 10000 Average loss: 0.00235159\n",
      "Training state:  False\n",
      "---- Done in  78.27859854698181  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00279731\n",
      "====> Epoch: 2000 Average loss: 0.00247886\n",
      "====> Epoch: 3000 Average loss: 0.00225676\n",
      "====> Epoch: 4000 Average loss: 0.00221210\n",
      "====> Epoch: 5000 Average loss: 0.00214874\n",
      "====> Epoch: 6000 Average loss: 0.00212382\n",
      "====> Epoch: 7000 Average loss: 0.00208509\n",
      "====> Epoch: 8000 Average loss: 0.00210634\n",
      "====> Epoch: 9000 Average loss: 0.00206024\n",
      "====> Epoch: 10000 Average loss: 0.00202610\n",
      "Training state:  False\n",
      "---- Done in  191.11520981788635  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00308690\n",
      "====> Epoch: 2000 Average loss: 0.00253061\n",
      "====> Epoch: 3000 Average loss: 0.00230788\n",
      "====> Epoch: 4000 Average loss: 0.00212237\n",
      "====> Epoch: 5000 Average loss: 0.00206604\n",
      "====> Epoch: 6000 Average loss: 0.00199679\n",
      "====> Epoch: 7000 Average loss: 0.00193642\n",
      "====> Epoch: 8000 Average loss: 0.00196578\n",
      "====> Epoch: 9000 Average loss: 0.00191855\n",
      "====> Epoch: 10000 Average loss: 0.00191875\n",
      "Training state:  False\n",
      "---- Done in  195.36989974975586  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00279557\n",
      "====> Epoch: 2000 Average loss: 0.00230144\n",
      "====> Epoch: 3000 Average loss: 0.00223044\n",
      "====> Epoch: 4000 Average loss: 0.00214506\n",
      "====> Epoch: 5000 Average loss: 0.00211508\n",
      "====> Epoch: 6000 Average loss: 0.00214439\n",
      "====> Epoch: 7000 Average loss: 0.00205169\n",
      "====> Epoch: 8000 Average loss: 0.00205855\n",
      "====> Epoch: 9000 Average loss: 0.00205490\n",
      "====> Epoch: 10000 Average loss: 0.00208706\n",
      "Training state:  False\n",
      "---- Done in  201.0463948249817  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00298079\n",
      "====> Epoch: 2000 Average loss: 0.00251990\n",
      "====> Epoch: 3000 Average loss: 0.00241812\n",
      "====> Epoch: 4000 Average loss: 0.00233152\n",
      "====> Epoch: 5000 Average loss: 0.00224661\n",
      "====> Epoch: 6000 Average loss: 0.00219768\n",
      "====> Epoch: 7000 Average loss: 0.00218199\n",
      "====> Epoch: 8000 Average loss: 0.00217054\n",
      "====> Epoch: 9000 Average loss: 0.00211495\n",
      "====> Epoch: 10000 Average loss: 0.00212725\n",
      "Training state:  False\n",
      "---- Done in  202.66983699798584  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 26\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00292405\n",
      "====> Epoch: 2000 Average loss: 0.00251186\n",
      "====> Epoch: 3000 Average loss: 0.00234182\n",
      "====> Epoch: 4000 Average loss: 0.00216094\n",
      "====> Epoch: 5000 Average loss: 0.00201866\n",
      "====> Epoch: 6000 Average loss: 0.00200883\n",
      "====> Epoch: 7000 Average loss: 0.00192565\n",
      "====> Epoch: 8000 Average loss: 0.00190621\n",
      "====> Epoch: 9000 Average loss: 0.00188509\n",
      "====> Epoch: 10000 Average loss: 0.00186186\n",
      "Training state:  False\n",
      "---- Done in  202.29720950126648  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00311484\n",
      "====> Epoch: 2000 Average loss: 0.00276368\n",
      "====> Epoch: 3000 Average loss: 0.00262976\n",
      "====> Epoch: 4000 Average loss: 0.00253165\n",
      "====> Epoch: 5000 Average loss: 0.00247527\n",
      "====> Epoch: 6000 Average loss: 0.00241158\n",
      "====> Epoch: 7000 Average loss: 0.00240508\n",
      "====> Epoch: 8000 Average loss: 0.00241118\n",
      "====> Epoch: 9000 Average loss: 0.00233859\n",
      "====> Epoch: 10000 Average loss: 0.00232654\n",
      "Training state:  False\n",
      "---- Done in  195.91568231582642  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00294650\n",
      "====> Epoch: 2000 Average loss: 0.00245739\n",
      "====> Epoch: 3000 Average loss: 0.00207508\n",
      "====> Epoch: 4000 Average loss: 0.00202429\n",
      "====> Epoch: 5000 Average loss: 0.00193784\n",
      "====> Epoch: 6000 Average loss: 0.00189760\n",
      "====> Epoch: 7000 Average loss: 0.00187236\n",
      "====> Epoch: 8000 Average loss: 0.00190144\n",
      "====> Epoch: 9000 Average loss: 0.00183488\n",
      "====> Epoch: 10000 Average loss: 0.00184221\n",
      "Training state:  False\n",
      "---- Done in  204.11820602416992  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00265714\n",
      "====> Epoch: 2000 Average loss: 0.00238165\n",
      "====> Epoch: 3000 Average loss: 0.00213855\n",
      "====> Epoch: 4000 Average loss: 0.00199778\n",
      "====> Epoch: 5000 Average loss: 0.00193366\n",
      "====> Epoch: 6000 Average loss: 0.00189520\n",
      "====> Epoch: 7000 Average loss: 0.00185746\n",
      "====> Epoch: 8000 Average loss: 0.00181165\n",
      "====> Epoch: 9000 Average loss: 0.00174586\n",
      "====> Epoch: 10000 Average loss: 0.00171001\n",
      "Training state:  False\n",
      "---- Done in  204.30704188346863  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00237457\n",
      "====> Epoch: 2000 Average loss: 0.00199957\n",
      "====> Epoch: 3000 Average loss: 0.00171812\n",
      "====> Epoch: 4000 Average loss: 0.00168578\n",
      "====> Epoch: 5000 Average loss: 0.00161883\n",
      "====> Epoch: 6000 Average loss: 0.00159041\n",
      "====> Epoch: 7000 Average loss: 0.00156696\n",
      "====> Epoch: 8000 Average loss: 0.00156293\n",
      "====> Epoch: 9000 Average loss: 0.00152597\n",
      "====> Epoch: 10000 Average loss: 0.00153669\n",
      "Training state:  False\n",
      "---- Done in  202.7212884426117  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00302763\n",
      "====> Epoch: 2000 Average loss: 0.00258540\n",
      "====> Epoch: 3000 Average loss: 0.00238257\n",
      "====> Epoch: 4000 Average loss: 0.00222108\n",
      "====> Epoch: 5000 Average loss: 0.00217371\n",
      "====> Epoch: 6000 Average loss: 0.00209257\n",
      "====> Epoch: 7000 Average loss: 0.00213731\n",
      "====> Epoch: 8000 Average loss: 0.00205861\n",
      "====> Epoch: 9000 Average loss: 0.00201807\n",
      "====> Epoch: 10000 Average loss: 0.00203647\n",
      "Training state:  False\n",
      "---- Done in  197.91174244880676  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00260662\n",
      "====> Epoch: 2000 Average loss: 0.00202745\n",
      "====> Epoch: 3000 Average loss: 0.00166127\n",
      "====> Epoch: 4000 Average loss: 0.00158036\n",
      "====> Epoch: 5000 Average loss: 0.00152654\n",
      "====> Epoch: 6000 Average loss: 0.00153450\n",
      "====> Epoch: 7000 Average loss: 0.00150852\n",
      "====> Epoch: 8000 Average loss: 0.00149309\n",
      "====> Epoch: 9000 Average loss: 0.00148301\n",
      "====> Epoch: 10000 Average loss: 0.00150517\n",
      "Training state:  False\n",
      "---- Done in  196.0406575202942  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 27\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00314192\n",
      "====> Epoch: 2000 Average loss: 0.00302219\n",
      "====> Epoch: 3000 Average loss: 0.00296659\n",
      "====> Epoch: 4000 Average loss: 0.00286314\n",
      "====> Epoch: 5000 Average loss: 0.00284862\n",
      "====> Epoch: 6000 Average loss: 0.00284169\n",
      "====> Epoch: 7000 Average loss: 0.00283039\n",
      "====> Epoch: 8000 Average loss: 0.00277882\n",
      "====> Epoch: 9000 Average loss: 0.00281918\n",
      "====> Epoch: 10000 Average loss: 0.00281143\n",
      "Training state:  False\n",
      "---- Done in  201.8683979511261  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1000 Average loss: 0.00323027\n",
      "====> Epoch: 2000 Average loss: 0.00309351\n",
      "====> Epoch: 3000 Average loss: 0.00292347\n",
      "====> Epoch: 4000 Average loss: 0.00281765\n",
      "====> Epoch: 5000 Average loss: 0.00269726\n",
      "====> Epoch: 6000 Average loss: 0.00265869\n",
      "====> Epoch: 7000 Average loss: 0.00262306\n",
      "====> Epoch: 8000 Average loss: 0.00255308\n",
      "====> Epoch: 9000 Average loss: 0.00252183\n",
      "====> Epoch: 10000 Average loss: 0.00252009\n",
      "Training state:  False\n",
      "---- Done in  196.0692789554596  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00322431\n",
      "====> Epoch: 2000 Average loss: 0.00272926\n",
      "====> Epoch: 3000 Average loss: 0.00250364\n",
      "====> Epoch: 4000 Average loss: 0.00243513\n",
      "====> Epoch: 5000 Average loss: 0.00241009\n",
      "====> Epoch: 6000 Average loss: 0.00233923\n",
      "====> Epoch: 7000 Average loss: 0.00231074\n",
      "====> Epoch: 8000 Average loss: 0.00227529\n",
      "====> Epoch: 9000 Average loss: 0.00227535\n",
      "====> Epoch: 10000 Average loss: 0.00223168\n",
      "Training state:  False\n",
      "---- Done in  203.5274040699005  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00314436\n",
      "====> Epoch: 2000 Average loss: 0.00277691\n",
      "====> Epoch: 3000 Average loss: 0.00261265\n",
      "====> Epoch: 4000 Average loss: 0.00252233\n",
      "====> Epoch: 5000 Average loss: 0.00246825\n",
      "====> Epoch: 6000 Average loss: 0.00242118\n",
      "====> Epoch: 7000 Average loss: 0.00235924\n",
      "====> Epoch: 8000 Average loss: 0.00234922\n",
      "====> Epoch: 9000 Average loss: 0.00234305\n",
      "====> Epoch: 10000 Average loss: 0.00232523\n",
      "Training state:  False\n",
      "---- Done in  193.6792230606079  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00269902\n",
      "====> Epoch: 2000 Average loss: 0.00229587\n",
      "====> Epoch: 3000 Average loss: 0.00198235\n",
      "====> Epoch: 4000 Average loss: 0.00185526\n",
      "====> Epoch: 5000 Average loss: 0.00180938\n",
      "====> Epoch: 6000 Average loss: 0.00173572\n",
      "====> Epoch: 7000 Average loss: 0.00169420\n",
      "====> Epoch: 8000 Average loss: 0.00170317\n",
      "====> Epoch: 9000 Average loss: 0.00171618\n",
      "====> Epoch: 10000 Average loss: 0.00167795\n",
      "Training state:  False\n",
      "---- Done in  203.2312409877777  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00311217\n",
      "====> Epoch: 2000 Average loss: 0.00241221\n",
      "====> Epoch: 3000 Average loss: 0.00220583\n",
      "====> Epoch: 4000 Average loss: 0.00212753\n",
      "====> Epoch: 5000 Average loss: 0.00207025\n",
      "====> Epoch: 6000 Average loss: 0.00206262\n",
      "====> Epoch: 7000 Average loss: 0.00202602\n",
      "====> Epoch: 8000 Average loss: 0.00200150\n",
      "====> Epoch: 9000 Average loss: 0.00194676\n",
      "====> Epoch: 10000 Average loss: 0.00198397\n",
      "Training state:  False\n",
      "---- Done in  202.86622786521912  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00353518\n",
      "====> Epoch: 2000 Average loss: 0.00307830\n",
      "====> Epoch: 3000 Average loss: 0.00287184\n",
      "====> Epoch: 4000 Average loss: 0.00271080\n",
      "====> Epoch: 5000 Average loss: 0.00261430\n",
      "====> Epoch: 6000 Average loss: 0.00257423\n",
      "====> Epoch: 7000 Average loss: 0.00250740\n",
      "====> Epoch: 8000 Average loss: 0.00249201\n",
      "====> Epoch: 9000 Average loss: 0.00245172\n",
      "====> Epoch: 10000 Average loss: 0.00248979\n",
      "Training state:  False\n",
      "---- Done in  198.1005835533142  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 28\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00273745\n",
      "====> Epoch: 2000 Average loss: 0.00227523\n",
      "====> Epoch: 3000 Average loss: 0.00210334\n",
      "====> Epoch: 4000 Average loss: 0.00202603\n",
      "====> Epoch: 5000 Average loss: 0.00200554\n",
      "====> Epoch: 6000 Average loss: 0.00197174\n",
      "====> Epoch: 7000 Average loss: 0.00197374\n",
      "====> Epoch: 8000 Average loss: 0.00197905\n",
      "====> Epoch: 9000 Average loss: 0.00194734\n",
      "====> Epoch: 10000 Average loss: 0.00194899\n",
      "Training state:  False\n",
      "---- Done in  193.06395053863525  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00303402\n",
      "====> Epoch: 2000 Average loss: 0.00271711\n",
      "====> Epoch: 3000 Average loss: 0.00254225\n",
      "====> Epoch: 4000 Average loss: 0.00242100\n",
      "====> Epoch: 5000 Average loss: 0.00235137\n",
      "====> Epoch: 6000 Average loss: 0.00232103\n",
      "====> Epoch: 7000 Average loss: 0.00227448\n",
      "====> Epoch: 8000 Average loss: 0.00221016\n",
      "====> Epoch: 9000 Average loss: 0.00217062\n",
      "====> Epoch: 10000 Average loss: 0.00213798\n",
      "Training state:  False\n",
      "---- Done in  195.7309856414795  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00317537\n",
      "====> Epoch: 2000 Average loss: 0.00287734\n",
      "====> Epoch: 3000 Average loss: 0.00267890\n",
      "====> Epoch: 4000 Average loss: 0.00251202\n",
      "====> Epoch: 5000 Average loss: 0.00246068\n",
      "====> Epoch: 6000 Average loss: 0.00247647\n",
      "====> Epoch: 7000 Average loss: 0.00240891\n",
      "====> Epoch: 8000 Average loss: 0.00233259\n",
      "====> Epoch: 9000 Average loss: 0.00235765\n",
      "====> Epoch: 10000 Average loss: 0.00227722\n",
      "Training state:  False\n",
      "---- Done in  200.62464380264282  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00281573\n",
      "====> Epoch: 2000 Average loss: 0.00225787\n",
      "====> Epoch: 3000 Average loss: 0.00201924\n",
      "====> Epoch: 4000 Average loss: 0.00198281\n",
      "====> Epoch: 5000 Average loss: 0.00195378\n",
      "====> Epoch: 6000 Average loss: 0.00191162\n",
      "====> Epoch: 7000 Average loss: 0.00188585\n",
      "====> Epoch: 8000 Average loss: 0.00187136\n",
      "====> Epoch: 9000 Average loss: 0.00179138\n",
      "====> Epoch: 10000 Average loss: 0.00172980\n",
      "Training state:  False\n",
      "---- Done in  202.7257137298584  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00325304\n",
      "====> Epoch: 2000 Average loss: 0.00284870\n",
      "====> Epoch: 3000 Average loss: 0.00262621\n",
      "====> Epoch: 4000 Average loss: 0.00248431\n",
      "====> Epoch: 5000 Average loss: 0.00236939\n",
      "====> Epoch: 6000 Average loss: 0.00230117\n",
      "====> Epoch: 7000 Average loss: 0.00224416\n",
      "====> Epoch: 8000 Average loss: 0.00222373\n",
      "====> Epoch: 9000 Average loss: 0.00231402\n",
      "====> Epoch: 10000 Average loss: 0.00212994\n",
      "Training state:  False\n",
      "---- Done in  203.30992460250854  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00303956\n",
      "====> Epoch: 2000 Average loss: 0.00279697\n",
      "====> Epoch: 3000 Average loss: 0.00266896\n",
      "====> Epoch: 4000 Average loss: 0.00258757\n",
      "====> Epoch: 5000 Average loss: 0.00248898\n",
      "====> Epoch: 6000 Average loss: 0.00241994\n",
      "====> Epoch: 7000 Average loss: 0.00238945\n",
      "====> Epoch: 8000 Average loss: 0.00231493\n",
      "====> Epoch: 9000 Average loss: 0.00228468\n",
      "====> Epoch: 10000 Average loss: 0.00228307\n",
      "Training state:  False\n",
      "---- Done in  200.8026773929596  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00264647\n",
      "====> Epoch: 2000 Average loss: 0.00213073\n",
      "====> Epoch: 3000 Average loss: 0.00190338\n",
      "====> Epoch: 4000 Average loss: 0.00187758\n",
      "====> Epoch: 5000 Average loss: 0.00177731\n",
      "====> Epoch: 6000 Average loss: 0.00170839\n",
      "====> Epoch: 7000 Average loss: 0.00170391\n",
      "====> Epoch: 8000 Average loss: 0.00169150\n",
      "====> Epoch: 9000 Average loss: 0.00171126\n",
      "====> Epoch: 10000 Average loss: 0.00170873\n",
      "Training state:  False\n",
      "---- Done in  198.89272594451904  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 29\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00293728\n",
      "====> Epoch: 2000 Average loss: 0.00240084\n",
      "====> Epoch: 3000 Average loss: 0.00224230\n",
      "====> Epoch: 4000 Average loss: 0.00213746\n",
      "====> Epoch: 5000 Average loss: 0.00197718\n",
      "====> Epoch: 6000 Average loss: 0.00184672\n",
      "====> Epoch: 7000 Average loss: 0.00183236\n",
      "====> Epoch: 8000 Average loss: 0.00178076\n",
      "====> Epoch: 9000 Average loss: 0.00181107\n",
      "====> Epoch: 10000 Average loss: 0.00176001\n",
      "Training state:  False\n",
      "---- Done in  194.24773573875427  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00315186\n",
      "====> Epoch: 2000 Average loss: 0.00272725\n",
      "====> Epoch: 3000 Average loss: 0.00245847\n",
      "====> Epoch: 4000 Average loss: 0.00232838\n",
      "====> Epoch: 5000 Average loss: 0.00213040\n",
      "====> Epoch: 6000 Average loss: 0.00206986\n",
      "====> Epoch: 7000 Average loss: 0.00197290\n",
      "====> Epoch: 8000 Average loss: 0.00198860\n",
      "====> Epoch: 9000 Average loss: 0.00196270\n",
      "====> Epoch: 10000 Average loss: 0.00193041\n",
      "Training state:  False\n",
      "---- Done in  195.95629477500916  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1000 Average loss: 0.00307291\n",
      "====> Epoch: 2000 Average loss: 0.00255749\n",
      "====> Epoch: 3000 Average loss: 0.00243355\n",
      "====> Epoch: 4000 Average loss: 0.00230022\n",
      "====> Epoch: 5000 Average loss: 0.00226357\n",
      "====> Epoch: 6000 Average loss: 0.00219988\n",
      "====> Epoch: 7000 Average loss: 0.00212450\n",
      "====> Epoch: 8000 Average loss: 0.00206406\n",
      "====> Epoch: 9000 Average loss: 0.00204018\n",
      "====> Epoch: 10000 Average loss: 0.00218212\n",
      "Training state:  False\n",
      "---- Done in  196.34409093856812  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00266906\n",
      "====> Epoch: 2000 Average loss: 0.00242558\n",
      "====> Epoch: 3000 Average loss: 0.00228332\n",
      "====> Epoch: 4000 Average loss: 0.00217508\n",
      "====> Epoch: 5000 Average loss: 0.00216319\n",
      "====> Epoch: 6000 Average loss: 0.00213803\n",
      "====> Epoch: 7000 Average loss: 0.00210938\n",
      "====> Epoch: 8000 Average loss: 0.00207522\n",
      "====> Epoch: 9000 Average loss: 0.00205739\n",
      "====> Epoch: 10000 Average loss: 0.00207796\n",
      "Training state:  False\n",
      "---- Done in  191.44718766212463  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00290585\n",
      "====> Epoch: 2000 Average loss: 0.00219646\n",
      "====> Epoch: 3000 Average loss: 0.00207316\n",
      "====> Epoch: 4000 Average loss: 0.00196293\n",
      "====> Epoch: 5000 Average loss: 0.00194427\n",
      "====> Epoch: 6000 Average loss: 0.00184218\n",
      "====> Epoch: 7000 Average loss: 0.00180921\n",
      "====> Epoch: 8000 Average loss: 0.00183229\n",
      "====> Epoch: 9000 Average loss: 0.00180254\n",
      "====> Epoch: 10000 Average loss: 0.00178751\n",
      "Training state:  False\n",
      "---- Done in  198.15031242370605  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00306824\n",
      "====> Epoch: 2000 Average loss: 0.00264765\n",
      "====> Epoch: 3000 Average loss: 0.00244835\n",
      "====> Epoch: 4000 Average loss: 0.00228491\n",
      "====> Epoch: 5000 Average loss: 0.00220714\n",
      "====> Epoch: 6000 Average loss: 0.00217225\n",
      "====> Epoch: 7000 Average loss: 0.00226883\n",
      "====> Epoch: 8000 Average loss: 0.00213181\n",
      "====> Epoch: 9000 Average loss: 0.00212258\n",
      "====> Epoch: 10000 Average loss: 0.00205766\n",
      "Training state:  False\n",
      "---- Done in  192.55776000022888  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00289643\n",
      "====> Epoch: 2000 Average loss: 0.00237239\n",
      "====> Epoch: 3000 Average loss: 0.00215039\n",
      "====> Epoch: 4000 Average loss: 0.00208930\n",
      "====> Epoch: 5000 Average loss: 0.00193541\n",
      "====> Epoch: 6000 Average loss: 0.00189967\n",
      "====> Epoch: 7000 Average loss: 0.00191144\n",
      "====> Epoch: 8000 Average loss: 0.00185160\n",
      "====> Epoch: 9000 Average loss: 0.00190484\n",
      "====> Epoch: 10000 Average loss: 0.00181038\n",
      "Training state:  False\n",
      "---- Done in  196.79980897903442  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 30\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00304620\n",
      "====> Epoch: 2000 Average loss: 0.00252220\n",
      "====> Epoch: 3000 Average loss: 0.00219195\n",
      "====> Epoch: 4000 Average loss: 0.00207373\n",
      "====> Epoch: 5000 Average loss: 0.00200683\n",
      "====> Epoch: 6000 Average loss: 0.00197187\n",
      "====> Epoch: 7000 Average loss: 0.00190033\n",
      "====> Epoch: 8000 Average loss: 0.00189319\n",
      "====> Epoch: 9000 Average loss: 0.00193705\n",
      "====> Epoch: 10000 Average loss: 0.00182231\n",
      "Training state:  False\n",
      "---- Done in  198.04059481620789  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00288518\n",
      "====> Epoch: 2000 Average loss: 0.00242919\n",
      "====> Epoch: 3000 Average loss: 0.00220550\n",
      "====> Epoch: 4000 Average loss: 0.00207505\n",
      "====> Epoch: 5000 Average loss: 0.00199124\n",
      "====> Epoch: 6000 Average loss: 0.00195098\n",
      "====> Epoch: 7000 Average loss: 0.00187762\n",
      "====> Epoch: 8000 Average loss: 0.00189458\n",
      "====> Epoch: 9000 Average loss: 0.00185247\n",
      "====> Epoch: 10000 Average loss: 0.00207846\n",
      "Training state:  False\n",
      "---- Done in  201.49572563171387  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00299626\n",
      "====> Epoch: 2000 Average loss: 0.00264326\n",
      "====> Epoch: 3000 Average loss: 0.00250188\n",
      "====> Epoch: 4000 Average loss: 0.00246864\n",
      "====> Epoch: 5000 Average loss: 0.00241323\n",
      "====> Epoch: 6000 Average loss: 0.00231920\n",
      "====> Epoch: 7000 Average loss: 0.00226553\n",
      "====> Epoch: 8000 Average loss: 0.00222583\n",
      "====> Epoch: 9000 Average loss: 0.00215991\n",
      "====> Epoch: 10000 Average loss: 0.00209667\n",
      "Training state:  False\n",
      "---- Done in  200.32251286506653  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00320101\n",
      "====> Epoch: 2000 Average loss: 0.00262082\n",
      "====> Epoch: 3000 Average loss: 0.00242874\n",
      "====> Epoch: 4000 Average loss: 0.00233415\n",
      "====> Epoch: 5000 Average loss: 0.00223204\n",
      "====> Epoch: 6000 Average loss: 0.00220840\n",
      "====> Epoch: 7000 Average loss: 0.00215050\n",
      "====> Epoch: 8000 Average loss: 0.00218623\n",
      "====> Epoch: 9000 Average loss: 0.00215787\n",
      "====> Epoch: 10000 Average loss: 0.00211886\n",
      "Training state:  False\n",
      "---- Done in  200.2401099205017  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00311457\n",
      "====> Epoch: 2000 Average loss: 0.00257647\n",
      "====> Epoch: 3000 Average loss: 0.00236799\n",
      "====> Epoch: 4000 Average loss: 0.00232635\n",
      "====> Epoch: 5000 Average loss: 0.00228115\n",
      "====> Epoch: 6000 Average loss: 0.00224765\n",
      "====> Epoch: 7000 Average loss: 0.00219783\n",
      "====> Epoch: 8000 Average loss: 0.00213690\n",
      "====> Epoch: 9000 Average loss: 0.00212396\n",
      "====> Epoch: 10000 Average loss: 0.00205017\n",
      "Training state:  False\n",
      "---- Done in  199.59972262382507  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00249934\n",
      "====> Epoch: 2000 Average loss: 0.00220033\n",
      "====> Epoch: 3000 Average loss: 0.00210192\n",
      "====> Epoch: 4000 Average loss: 0.00193109\n",
      "====> Epoch: 5000 Average loss: 0.00186166\n",
      "====> Epoch: 6000 Average loss: 0.00179443\n",
      "====> Epoch: 7000 Average loss: 0.00181295\n",
      "====> Epoch: 8000 Average loss: 0.00176553\n",
      "====> Epoch: 9000 Average loss: 0.00175877\n",
      "====> Epoch: 10000 Average loss: 0.00175393\n",
      "Training state:  False\n",
      "---- Done in  199.09978127479553  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00230490\n",
      "====> Epoch: 2000 Average loss: 0.00211963\n",
      "====> Epoch: 3000 Average loss: 0.00196043\n",
      "====> Epoch: 4000 Average loss: 0.00188211\n",
      "====> Epoch: 5000 Average loss: 0.00181273\n",
      "====> Epoch: 6000 Average loss: 0.00173068\n",
      "====> Epoch: 7000 Average loss: 0.00171106\n",
      "====> Epoch: 8000 Average loss: 0.00168062\n",
      "====> Epoch: 9000 Average loss: 0.00165398\n",
      "====> Epoch: 10000 Average loss: 0.00158072\n",
      "Training state:  False\n",
      "---- Done in  198.67888641357422  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 31\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00258353\n"
     ]
    }
   ],
   "source": [
    "assignment_model_names = ['A_add_lin', 'B_add_mild_nlin', 'C_add_mod_nlin', 'D_mild_nadd_lin',\n",
    "                     'E_mild_nadd_mild_nlin', 'F_mod_nadd_lin', 'G_mod_nadd_mod_nlin']\n",
    "\n",
    "for dataset_number in range(25, 100):\n",
    "    print(\"Starting run for Dataset {}\".format(dataset_number))\n",
    "    \n",
    "    for model_name in assignment_model_names:\n",
    "        print(\"-- Running for model name: \", model_name)\n",
    "        \n",
    "        start = time()\n",
    "\n",
    "        dataset = CovariateDataset(\"n_{}_model_{}_v_{}_covar_data\", [1000, model_name, dataset_number])\n",
    "\n",
    "        trained_model, original_data, binary_mu_out, normal_mu_out, mu_latent, logvar_latent = \\\n",
    "            train_model(ModifiedVAE, dataset, dataset_number,verbose=True)\n",
    "\n",
    "        encode_data(trained_model, dataset)\n",
    "\n",
    "        print(\"---- Done in \", time() - start, \" seconds\\n\")\n",
    "                \n",
    "    print(\"================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1000/10000], loss:0.4875\n",
      "epoch [2000/10000], loss:0.4456\n",
      "epoch [3000/10000], loss:0.4104\n",
      "epoch [4000/10000], loss:0.3195\n",
      "epoch [5000/10000], loss:0.3145\n",
      "epoch [6000/10000], loss:0.3127\n",
      "epoch [7000/10000], loss:0.3118\n",
      "epoch [8000/10000], loss:0.3107\n",
      "epoch [9000/10000], loss:0.3096\n",
      "epoch [10000/10000], loss:0.3084\n",
      "Final loss: loss:0.3084\n",
      "epoch [1000/10000], loss:0.5869\n",
      "epoch [2000/10000], loss:0.5415\n",
      "epoch [3000/10000], loss:0.5173\n",
      "epoch [4000/10000], loss:0.4197\n",
      "epoch [5000/10000], loss:0.4216\n",
      "epoch [6000/10000], loss:0.4136\n",
      "epoch [7000/10000], loss:0.4132\n",
      "epoch [8000/10000], loss:0.4127\n",
      "epoch [9000/10000], loss:0.4123\n",
      "epoch [10000/10000], loss:0.4118\n",
      "Final loss: loss:0.4118\n"
     ]
    }
   ],
   "source": [
    "models_to_rerun = [('A_add_lin', 12, 'sparsity'), ('G_mod_nadd_mod_nlin', 40, 'sparsity')]\n",
    "\n",
    "for model_name, dataset_number, loss_type in models_to_rerun:\n",
    "    dataset = CovariateDataset(\"n_{}_model_{}_v_{}_covar_data\", [1000, model_name, dataset_number])\n",
    "    trained_model, final_loss = train_model(\n",
    "                                        autoencoder,\n",
    "                                        dataset,\n",
    "                                        loss=loss_type,\n",
    "                                        verbose=True)\n",
    "    encode_data(trained_model, dataset, loss=loss_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
