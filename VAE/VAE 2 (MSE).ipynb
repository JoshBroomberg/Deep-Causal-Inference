{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = \"../Data/Raw/\"\n",
    "PROCESSED_DATA_DIR = \"../Data/Processed/VAE/\"\n",
    "\n",
    "class CovariateDataset(Dataset):\n",
    "    def __init__(self, file_name_pattern, file_name_args):\n",
    "        self.file_name = file_name_pattern.format(*file_name_args)\n",
    "        self.data = np.loadtxt(RAW_DATA_DIR + self.file_name + \".csv\", delimiter=\",\")[:, 1:] # remove bias\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index].astype(float), 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def save_processed_data(self, data):\n",
    "        name = PROCESSED_DATA_DIR + self.file_name+\".csv\"\n",
    "        np.savetxt(name, data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Based on an example from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "# Extended to place a different prior on binary vs normal vars\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "ZDIMS = 4 # latent dimensions\n",
    "INTERMEDIATE_DIMS = 32\n",
    "FEATURES = 10\n",
    "DIAG_VAR = True\n",
    "\n",
    "BINARY = [0, 2, 5, 7, 8]\n",
    "NORMAL = [1, 3, 5, 6, 9]\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # ENCODER LAYERS\n",
    "        self.dense1 = nn.Linear(FEATURES, INTERMEDIATE_DIMS)\n",
    "        self.dense2_1 = nn.Linear(INTERMEDIATE_DIMS, ZDIMS)  # mu layer\n",
    "        self.dense2_2 = nn.Linear(INTERMEDIATE_DIMS, ZDIMS)  # logvariance layer\n",
    "        \n",
    "        # this last layer bottlenecks through ZDIMS connections\n",
    "\n",
    "        # DECODER LAYERS\n",
    "        self.dense3 = nn.Linear(ZDIMS, INTERMEDIATE_DIMS)\n",
    "        self.dense4 = nn.Linear(INTERMEDIATE_DIMS, FEATURES)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.dense1(x))\n",
    "        return self.dense2_1(h1), self.dense2_2(h1) #mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # If we sampled directly from the latent distribution\n",
    "            # we wouldn't be able to backprop the results because\n",
    "            # there is no clear grad on the distribution\n",
    "\n",
    "            # This reparam samples from a unit gaussian and then scales\n",
    "            # by the latent parameters giving a defined route to backprop.\n",
    "\n",
    "            std = logvar.mul(0.5).exp_() \n",
    "\n",
    "            # Sample from a unit gaussian with dimensions matching\n",
    "            # the latent space.\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "\n",
    "            return eps.mul(std).add_(mu) # rescale and return\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.dense3(z))\n",
    "        mu_out = self.dense4(h3)# Deleted: self.sigmoid(self.dense4(h3))\n",
    "        \n",
    "        return mu_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, FEATURES))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        mu_out = self.decode(z)\n",
    "        return mu_out, mu, logvar\n",
    "\n",
    "def loss_function(recon_batch_mu, batch_x, mu_latent, logvar_latent):\n",
    "    \n",
    "    # MSE: how good is the reconstruction in terms of\n",
    "    mse_loss = nn.MSELoss(size_average=False)\n",
    "    recon_loss = mse_loss(recon_batch_mu, batch_x)\n",
    "    \n",
    "    recon_loss /= batch_x.size()[0]\n",
    "    \n",
    "    # KLD is Kullbackâ€“Leibler divergence. Regularize VAE by\n",
    "    # penalizing divergence from the prior\n",
    "\n",
    "    # See Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    KLD = -0.5 * torch.sum(1 + logvar_latent - mu_latent.pow(2) - logvar_latent.exp())\n",
    "    # Normalise by same number of elements as in reconstruction\n",
    "    KLD /= batch_x.size()[0] * FEATURES\n",
    "    \n",
    "#     print(\"RL\", recon_loss)\n",
    "#     print(\"KLD\", KLD)\n",
    "    return recon_loss + KLD\n",
    "\n",
    "def train(model, optimizer, epoch, data_loader, log_results=False):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):\n",
    "        data = Variable(data)\n",
    "        data = data.float()\n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon_data, mu_latent, logvar_latent = model(data)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = loss_function(recon_data, data, mu_latent, logvar_latent)\n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        # Find the gradient and descend\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if log_results:\n",
    "        print('====> Epoch: {} Average loss: {:.8f}'.format(\n",
    "              epoch, train_loss / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Based on an example from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "# Extended to place a different prior on binary vs normal vars\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "ZDIMS = 4 # latent dimensions\n",
    "INTERMEDIATE_DIMS = 32\n",
    "FEATURES = 10\n",
    "DIAG_VAR = True\n",
    "\n",
    "BINARY = [0, 2, 5, 7, 8]\n",
    "NORMAL = [1, 3, 5, 6, 9]\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "class ModifiedVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedVAE, self).__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # ENCODER LAYERS\n",
    "        self.dense1 = nn.Linear(FEATURES, INTERMEDIATE_DIMS)\n",
    "        self.dense2_1 = nn.Linear(INTERMEDIATE_DIMS, ZDIMS)  # mu layer\n",
    "        self.dense2_2 = nn.Linear(INTERMEDIATE_DIMS, ZDIMS)  # logvariance layer\n",
    "        \n",
    "        # this last layer bottlenecks through ZDIMS connections\n",
    "\n",
    "        # DECODER LAYERS\n",
    "        self.dense3 = nn.Linear(ZDIMS, INTERMEDIATE_DIMS)\n",
    "        self.dense4 = nn.Linear(INTERMEDIATE_DIMS, len(BINARY))\n",
    "        self.dense5 = nn.Linear(INTERMEDIATE_DIMS, len(NORMAL))\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.dense1(x))\n",
    "        return self.dense2_1(h1), self.dense2_2(h1) #mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # If we sampled directly from the latent distribution\n",
    "            # we wouldn't be able to backprop the results because\n",
    "            # there is no clear grad on the distribution\n",
    "\n",
    "            # This reparam samples from a unit gaussian and then scales\n",
    "            # by the latent parameters giving a defined route to backprop.\n",
    "\n",
    "            std = logvar.mul(0.5).exp_() \n",
    "\n",
    "            # Sample from a unit gaussian with dimensions matching\n",
    "            # the latent space.\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "\n",
    "            return eps.mul(std).add_(mu) # rescale and return\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.dense3(z))\n",
    "        binary_mu_out = self.sigmoid(self.dense4(h3))\n",
    "        normal_mu_out = self.dense5(h3)\n",
    "        \n",
    "        return binary_mu_out, normal_mu_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_mu, latent_logvar = self.encode(x.view(-1, FEATURES))\n",
    "        z = self.reparameterize(latent_mu, latent_logvar)\n",
    "        binary_mu_out, normal_mu_out = self.decode(z)\n",
    "        return binary_mu_out, normal_mu_out, latent_mu, latent_logvar\n",
    "\n",
    "def loss_function(recon_binary_mu, recon_normal_mu, batch_x, mu_latent, logvar_latent):\n",
    "    \n",
    "    # MSE: how good is the reconstruction in terms of\n",
    "    mse_loss = nn.MSELoss(size_average=False)\n",
    "    normal_recon_loss = mse_loss(recon_normal_mu, batch_x[:, NORMAL])\n",
    "    normal_recon_loss /= (batch_x.size()[0])\n",
    "    \n",
    "    # Cross Entropy:\n",
    "    BCE = F.binary_cross_entropy(recon_binary_mu, batch_x[:, BINARY], size_average=False)\n",
    "    BCE /= (batch_x.size()[0])\n",
    "    \n",
    "    # KLD is Kullbackâ€“Leibler divergence. Regularize VAE by\n",
    "    # penalizing divergence from the prior\n",
    "\n",
    "    # See Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    KLD = -0.5 * torch.sum(1 + logvar_latent - mu_latent.pow(2) - logvar_latent.exp())\n",
    "    # Normalise by same number of elements as in reconstruction\n",
    "    KLD /= batch_x.size()[0] * FEATURES\n",
    "    \n",
    "#     print(\"RL\", normal_recon_loss.data.cpu().numpy()[0])\n",
    "#     print(\"BCE\", BCE.data.cpu().numpy()[0])\n",
    "#     print(\"KLD\", KLD.data.cpu().numpy()[0])\n",
    "    \n",
    "    return normal_recon_loss + BCE + KLD\n",
    "\n",
    "def train(model, optimizer, epoch, data_loader, log_results=False):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):\n",
    "        data = Variable(data)\n",
    "        data = data.float()\n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        binary_mu_out, normal_mu_out, mu_latent, logvar_latent = model(data)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = loss_function(binary_mu_out, normal_mu_out, data, mu_latent, logvar_latent)\n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        # Find the gradient and descend\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if log_results:\n",
    "        print('====> Epoch: {} Average loss: {:.8f}'.format(\n",
    "              epoch, train_loss / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Process Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model_class, dataset, dataset_number, verbose=True):\n",
    "    model = model_class()\n",
    "    if CUDA:\n",
    "        model = model.cuda()\n",
    "\n",
    "    num_epochs = 10000\n",
    "    batch_size = 1000\n",
    "    learning_rate = 1e-2\n",
    "    lr_sched = False\n",
    "         \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [int(num_epochs/4), int(3*num_epochs/4)], gamma=0.1)\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        checkpoint_interval = int(num_epochs/10)\n",
    "        \n",
    "        if lr_sched:\n",
    "            scheduler.step()\n",
    "\n",
    "        log = False\n",
    "        if epoch%checkpoint_interval == 0:\n",
    "            log = True\n",
    "            \n",
    "        train(model, optimizer, epoch, data_loader, log_results=log)\n",
    "    \n",
    "\n",
    "    torch.save(model.state_dict(), \"../Models/VAE_{}.pth\".format(dataset_number))\n",
    "    \n",
    "    # Show reconstruction\n",
    "    model.eval()\n",
    "    print(\"Training state: \", model.training)\n",
    "    \n",
    "    original_data,_ = next(iter(data_loader))\n",
    "    original_data = Variable(original_data)\n",
    "    original_data = original_data.float()\n",
    "    if CUDA:\n",
    "        original_data = original_data.cuda()\n",
    "        \n",
    "    binary_mu_out, normal_mu_out, mu_latent, logvar_latent = model(original_data)\n",
    "    \n",
    "    return model, original_data, binary_mu_out, normal_mu_out, mu_latent, logvar_latent\n",
    "\n",
    "def encode_data(model, dataset):\n",
    "    all_data = torch.from_numpy(dataset.data)\n",
    "    all_data = Variable(all_data)\n",
    "    all_data = all_data.float()\n",
    "    \n",
    "    if CUDA:\n",
    "        all_data = all_data.cuda()\n",
    "\n",
    "    latent_mu, latent_var = model.encode(all_data)\n",
    "    \n",
    "    if CUDA:\n",
    "        latent_mu = latent_mu.cpu()\n",
    "        latent_var = latent_var.cpu()\n",
    "        \n",
    "    data = np.hstack([latent_mu.data.numpy(), latent_var.data.numpy()])\n",
    "    dataset.save_processed_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 Average loss: 0.00720191\n",
      "====> Epoch: 20 Average loss: 0.00628152\n",
      "====> Epoch: 30 Average loss: 0.00559646\n",
      "====> Epoch: 40 Average loss: 0.00534063\n",
      "====> Epoch: 50 Average loss: 0.00487277\n",
      "====> Epoch: 60 Average loss: 0.00463829\n",
      "====> Epoch: 70 Average loss: 0.00453654\n",
      "====> Epoch: 80 Average loss: 0.00445161\n",
      "====> Epoch: 90 Average loss: 0.00436299\n",
      "====> Epoch: 100 Average loss: 0.00429253\n",
      "Training state:  False\n"
     ]
    }
   ],
   "source": [
    "dataset = CovariateDataset(\"n_{}_model_{}_v_{}_covar_data\", [1000, \"A_add_lin\", 1])\n",
    "trained_model, original_data, binary_mu_out, normal_mu_out, mu_latent, logvar_latent = \\\n",
    "    train_model(ModifiedVAE, dataset, 1,verbose=True)\n",
    "\n",
    "encode_data(trained_model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal: [1.0, 1.0, 1.0, -1.32, 0.02, 1.0, -0.35, 0.0, 1.0, -0.93]\n",
      "Mu out: [1.0, 1.15, 1.0, -1.01, 0.0, 0.98, -0.18, 0.12, 1.0, -1.18]\n",
      "Mu Latent: [1.1, -0.14, 1.1, -0.44]\n",
      "Std latent: [0.06, 0.03, 0.06, 0.07]\n",
      "\n",
      "Orginal: [1.0, -1.1, 0.0, -0.12, -0.33, 0.0, -2.12, 0.0, 0.0, -1.77]\n",
      "Mu out: [0.36, -1.32, 0.02, 0.53, 0.0, 0.01, -2.24, 0.0, 0.0, -1.25]\n",
      "Mu Latent: [-1.4, -0.21, 1.64, 1.06]\n",
      "Std latent: [0.09, 0.03, 0.07, 0.1]\n",
      "\n",
      "Orginal: [0.0, -2.11, 0.0, 0.21, 0.58, 0.0, 0.38, 0.0, 0.0, 1.91]\n",
      "Mu out: [0.02, -1.66, 0.01, 0.41, 50774.44, 0.08, 0.46, 0.0, 0.0, 1.06]\n",
      "Mu Latent: [-1.34, 0.32, 0.84, 1.66]\n",
      "Std latent: [0.06, 0.03, 0.04, 0.07]\n",
      "\n",
      "Orginal: [1.0, 1.29, 0.0, 0.74, 0.57, 1.0, -1.03, 1.0, 0.0, -1.44]\n",
      "Mu out: [1.0, 1.09, 0.0, 0.78, 0.0, 0.97, -0.86, 1.0, 0.0, -1.25]\n",
      "Mu Latent: [0.34, -2.01, 0.03, -1.48]\n",
      "Std latent: [0.05, 0.19, 0.03, 0.16]\n",
      "\n",
      "Orginal: [1.0, -1.29, 1.0, 0.23, 1.92, 1.0, 1.66, 0.0, 1.0, 1.65]\n",
      "Mu out: [0.98, -1.53, 1.0, 0.19, 0.0, 0.97, 1.58, 0.0, 1.0, 1.81]\n",
      "Mu Latent: [-1.46, 0.14, -0.42, -0.0]\n",
      "Std latent: [0.05, 0.04, 0.05, 0.04]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mu_out = torch.Tensor(1000, 10)\n",
    "\n",
    "for index in BINARY:\n",
    "    mu_out[:, index] = binary_mu_out[:, BINARY.index(index)].data.cpu()\n",
    "    \n",
    "for index in NORMAL:\n",
    "    mu_out[:, index] = normal_mu_out[:, NORMAL.index(index)].data.cpu()\n",
    "    \n",
    "for i in np.random.choice(list(range(1000)), size=5, ):\n",
    "    print(\"Orginal:\", list(np.round(original_data[i].data.cpu().numpy(), 2)))\n",
    "    print(\"Mu out:\", list(np.round(mu_out[i].numpy(), 2)))\n",
    "    print(\"Mu Latent:\", list(np.round(mu_latent[i].data.cpu().numpy(), 2)))\n",
    "    print(\"Std latent:\", list(np.round(logvar_latent[i].mul(0.5).exp().data.cpu().numpy(), 2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run for Dataset 25\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00266409\n",
      "====> Epoch: 2000 Average loss: 0.00196114\n",
      "====> Epoch: 3000 Average loss: 0.00178170\n",
      "====> Epoch: 4000 Average loss: 0.00167729\n",
      "====> Epoch: 5000 Average loss: 0.00164007\n",
      "====> Epoch: 6000 Average loss: 0.00160345\n",
      "====> Epoch: 7000 Average loss: 0.00160483\n",
      "====> Epoch: 8000 Average loss: 0.00157590\n",
      "====> Epoch: 9000 Average loss: 0.00156361\n",
      "====> Epoch: 10000 Average loss: 0.00153984\n",
      "Training state:  False\n",
      "---- Done in  176.25343918800354  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00287171\n",
      "====> Epoch: 2000 Average loss: 0.00261413\n",
      "====> Epoch: 3000 Average loss: 0.00252240\n",
      "====> Epoch: 4000 Average loss: 0.00245967\n",
      "====> Epoch: 5000 Average loss: 0.00229948\n",
      "====> Epoch: 6000 Average loss: 0.00223907\n",
      "====> Epoch: 7000 Average loss: 0.00218793\n",
      "====> Epoch: 8000 Average loss: 0.00215036\n",
      "====> Epoch: 9000 Average loss: 0.00208439\n",
      "====> Epoch: 10000 Average loss: 0.00206078\n",
      "Training state:  False\n",
      "---- Done in  74.69603300094604  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00293841\n",
      "====> Epoch: 2000 Average loss: 0.00262392\n",
      "====> Epoch: 3000 Average loss: 0.00252926\n",
      "====> Epoch: 4000 Average loss: 0.00246943\n",
      "====> Epoch: 5000 Average loss: 0.00246275\n",
      "====> Epoch: 6000 Average loss: 0.00242101\n",
      "====> Epoch: 7000 Average loss: 0.00241969\n",
      "====> Epoch: 8000 Average loss: 0.00235830\n",
      "====> Epoch: 9000 Average loss: 0.00236172\n",
      "====> Epoch: 10000 Average loss: 0.00235159\n",
      "Training state:  False\n",
      "---- Done in  78.27859854698181  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00279731\n",
      "====> Epoch: 2000 Average loss: 0.00247886\n",
      "====> Epoch: 3000 Average loss: 0.00225676\n",
      "====> Epoch: 4000 Average loss: 0.00221210\n",
      "====> Epoch: 5000 Average loss: 0.00214874\n",
      "====> Epoch: 6000 Average loss: 0.00212382\n",
      "====> Epoch: 7000 Average loss: 0.00208509\n",
      "====> Epoch: 8000 Average loss: 0.00210634\n",
      "====> Epoch: 9000 Average loss: 0.00206024\n",
      "====> Epoch: 10000 Average loss: 0.00202610\n",
      "Training state:  False\n",
      "---- Done in  191.11520981788635  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00308690\n",
      "====> Epoch: 2000 Average loss: 0.00253061\n",
      "====> Epoch: 3000 Average loss: 0.00230788\n",
      "====> Epoch: 4000 Average loss: 0.00212237\n",
      "====> Epoch: 5000 Average loss: 0.00206604\n",
      "====> Epoch: 6000 Average loss: 0.00199679\n",
      "====> Epoch: 7000 Average loss: 0.00193642\n",
      "====> Epoch: 8000 Average loss: 0.00196578\n",
      "====> Epoch: 9000 Average loss: 0.00191855\n",
      "====> Epoch: 10000 Average loss: 0.00191875\n",
      "Training state:  False\n",
      "---- Done in  195.36989974975586  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00279557\n",
      "====> Epoch: 2000 Average loss: 0.00230144\n",
      "====> Epoch: 3000 Average loss: 0.00223044\n",
      "====> Epoch: 4000 Average loss: 0.00214506\n",
      "====> Epoch: 5000 Average loss: 0.00211508\n",
      "====> Epoch: 6000 Average loss: 0.00214439\n",
      "====> Epoch: 7000 Average loss: 0.00205169\n",
      "====> Epoch: 8000 Average loss: 0.00205855\n",
      "====> Epoch: 9000 Average loss: 0.00205490\n",
      "====> Epoch: 10000 Average loss: 0.00208706\n",
      "Training state:  False\n",
      "---- Done in  201.0463948249817  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00298079\n",
      "====> Epoch: 2000 Average loss: 0.00251990\n",
      "====> Epoch: 3000 Average loss: 0.00241812\n",
      "====> Epoch: 4000 Average loss: 0.00233152\n",
      "====> Epoch: 5000 Average loss: 0.00224661\n",
      "====> Epoch: 6000 Average loss: 0.00219768\n",
      "====> Epoch: 7000 Average loss: 0.00218199\n",
      "====> Epoch: 8000 Average loss: 0.00217054\n",
      "====> Epoch: 9000 Average loss: 0.00211495\n",
      "====> Epoch: 10000 Average loss: 0.00212725\n",
      "Training state:  False\n",
      "---- Done in  202.66983699798584  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 26\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00292405\n",
      "====> Epoch: 2000 Average loss: 0.00251186\n",
      "====> Epoch: 3000 Average loss: 0.00234182\n",
      "====> Epoch: 4000 Average loss: 0.00216094\n",
      "====> Epoch: 5000 Average loss: 0.00201866\n",
      "====> Epoch: 6000 Average loss: 0.00200883\n",
      "====> Epoch: 7000 Average loss: 0.00192565\n",
      "====> Epoch: 8000 Average loss: 0.00190621\n",
      "====> Epoch: 9000 Average loss: 0.00188509\n",
      "====> Epoch: 10000 Average loss: 0.00186186\n",
      "Training state:  False\n",
      "---- Done in  202.29720950126648  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00311484\n",
      "====> Epoch: 2000 Average loss: 0.00276368\n",
      "====> Epoch: 3000 Average loss: 0.00262976\n",
      "====> Epoch: 4000 Average loss: 0.00253165\n",
      "====> Epoch: 5000 Average loss: 0.00247527\n",
      "====> Epoch: 6000 Average loss: 0.00241158\n",
      "====> Epoch: 7000 Average loss: 0.00240508\n",
      "====> Epoch: 8000 Average loss: 0.00241118\n",
      "====> Epoch: 9000 Average loss: 0.00233859\n",
      "====> Epoch: 10000 Average loss: 0.00232654\n",
      "Training state:  False\n",
      "---- Done in  195.91568231582642  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00294650\n",
      "====> Epoch: 2000 Average loss: 0.00245739\n",
      "====> Epoch: 3000 Average loss: 0.00207508\n",
      "====> Epoch: 4000 Average loss: 0.00202429\n",
      "====> Epoch: 5000 Average loss: 0.00193784\n",
      "====> Epoch: 6000 Average loss: 0.00189760\n",
      "====> Epoch: 7000 Average loss: 0.00187236\n",
      "====> Epoch: 8000 Average loss: 0.00190144\n",
      "====> Epoch: 9000 Average loss: 0.00183488\n",
      "====> Epoch: 10000 Average loss: 0.00184221\n",
      "Training state:  False\n",
      "---- Done in  204.11820602416992  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00265714\n",
      "====> Epoch: 2000 Average loss: 0.00238165\n",
      "====> Epoch: 3000 Average loss: 0.00213855\n",
      "====> Epoch: 4000 Average loss: 0.00199778\n",
      "====> Epoch: 5000 Average loss: 0.00193366\n",
      "====> Epoch: 6000 Average loss: 0.00189520\n",
      "====> Epoch: 7000 Average loss: 0.00185746\n",
      "====> Epoch: 8000 Average loss: 0.00181165\n",
      "====> Epoch: 9000 Average loss: 0.00174586\n",
      "====> Epoch: 10000 Average loss: 0.00171001\n",
      "Training state:  False\n",
      "---- Done in  204.30704188346863  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00237457\n",
      "====> Epoch: 2000 Average loss: 0.00199957\n",
      "====> Epoch: 3000 Average loss: 0.00171812\n",
      "====> Epoch: 4000 Average loss: 0.00168578\n",
      "====> Epoch: 5000 Average loss: 0.00161883\n",
      "====> Epoch: 6000 Average loss: 0.00159041\n",
      "====> Epoch: 7000 Average loss: 0.00156696\n",
      "====> Epoch: 8000 Average loss: 0.00156293\n",
      "====> Epoch: 9000 Average loss: 0.00152597\n",
      "====> Epoch: 10000 Average loss: 0.00153669\n",
      "Training state:  False\n",
      "---- Done in  202.7212884426117  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00302763\n",
      "====> Epoch: 2000 Average loss: 0.00258540\n",
      "====> Epoch: 3000 Average loss: 0.00238257\n",
      "====> Epoch: 4000 Average loss: 0.00222108\n",
      "====> Epoch: 5000 Average loss: 0.00217371\n",
      "====> Epoch: 6000 Average loss: 0.00209257\n",
      "====> Epoch: 7000 Average loss: 0.00213731\n",
      "====> Epoch: 8000 Average loss: 0.00205861\n",
      "====> Epoch: 9000 Average loss: 0.00201807\n",
      "====> Epoch: 10000 Average loss: 0.00203647\n",
      "Training state:  False\n",
      "---- Done in  197.91174244880676  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00260662\n",
      "====> Epoch: 2000 Average loss: 0.00202745\n",
      "====> Epoch: 3000 Average loss: 0.00166127\n",
      "====> Epoch: 4000 Average loss: 0.00158036\n",
      "====> Epoch: 5000 Average loss: 0.00152654\n",
      "====> Epoch: 6000 Average loss: 0.00153450\n",
      "====> Epoch: 7000 Average loss: 0.00150852\n",
      "====> Epoch: 8000 Average loss: 0.00149309\n",
      "====> Epoch: 9000 Average loss: 0.00148301\n",
      "====> Epoch: 10000 Average loss: 0.00150517\n",
      "Training state:  False\n",
      "---- Done in  196.0406575202942  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 27\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00314192\n",
      "====> Epoch: 2000 Average loss: 0.00302219\n",
      "====> Epoch: 3000 Average loss: 0.00296659\n",
      "====> Epoch: 4000 Average loss: 0.00286314\n",
      "====> Epoch: 5000 Average loss: 0.00284862\n",
      "====> Epoch: 6000 Average loss: 0.00284169\n",
      "====> Epoch: 7000 Average loss: 0.00283039\n",
      "====> Epoch: 8000 Average loss: 0.00277882\n",
      "====> Epoch: 9000 Average loss: 0.00281918\n",
      "====> Epoch: 10000 Average loss: 0.00281143\n",
      "Training state:  False\n",
      "---- Done in  201.8683979511261  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1000 Average loss: 0.00323027\n",
      "====> Epoch: 2000 Average loss: 0.00309351\n",
      "====> Epoch: 3000 Average loss: 0.00292347\n",
      "====> Epoch: 4000 Average loss: 0.00281765\n",
      "====> Epoch: 5000 Average loss: 0.00269726\n",
      "====> Epoch: 6000 Average loss: 0.00265869\n",
      "====> Epoch: 7000 Average loss: 0.00262306\n",
      "====> Epoch: 8000 Average loss: 0.00255308\n",
      "====> Epoch: 9000 Average loss: 0.00252183\n",
      "====> Epoch: 10000 Average loss: 0.00252009\n",
      "Training state:  False\n",
      "---- Done in  196.0692789554596  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00322431\n",
      "====> Epoch: 2000 Average loss: 0.00272926\n",
      "====> Epoch: 3000 Average loss: 0.00250364\n",
      "====> Epoch: 4000 Average loss: 0.00243513\n",
      "====> Epoch: 5000 Average loss: 0.00241009\n",
      "====> Epoch: 6000 Average loss: 0.00233923\n",
      "====> Epoch: 7000 Average loss: 0.00231074\n",
      "====> Epoch: 8000 Average loss: 0.00227529\n",
      "====> Epoch: 9000 Average loss: 0.00227535\n",
      "====> Epoch: 10000 Average loss: 0.00223168\n",
      "Training state:  False\n",
      "---- Done in  203.5274040699005  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00314436\n",
      "====> Epoch: 2000 Average loss: 0.00277691\n",
      "====> Epoch: 3000 Average loss: 0.00261265\n",
      "====> Epoch: 4000 Average loss: 0.00252233\n",
      "====> Epoch: 5000 Average loss: 0.00246825\n",
      "====> Epoch: 6000 Average loss: 0.00242118\n",
      "====> Epoch: 7000 Average loss: 0.00235924\n",
      "====> Epoch: 8000 Average loss: 0.00234922\n",
      "====> Epoch: 9000 Average loss: 0.00234305\n",
      "====> Epoch: 10000 Average loss: 0.00232523\n",
      "Training state:  False\n",
      "---- Done in  193.6792230606079  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00269902\n",
      "====> Epoch: 2000 Average loss: 0.00229587\n",
      "====> Epoch: 3000 Average loss: 0.00198235\n",
      "====> Epoch: 4000 Average loss: 0.00185526\n",
      "====> Epoch: 5000 Average loss: 0.00180938\n",
      "====> Epoch: 6000 Average loss: 0.00173572\n",
      "====> Epoch: 7000 Average loss: 0.00169420\n",
      "====> Epoch: 8000 Average loss: 0.00170317\n",
      "====> Epoch: 9000 Average loss: 0.00171618\n",
      "====> Epoch: 10000 Average loss: 0.00167795\n",
      "Training state:  False\n",
      "---- Done in  203.2312409877777  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00311217\n",
      "====> Epoch: 2000 Average loss: 0.00241221\n",
      "====> Epoch: 3000 Average loss: 0.00220583\n",
      "====> Epoch: 4000 Average loss: 0.00212753\n",
      "====> Epoch: 5000 Average loss: 0.00207025\n",
      "====> Epoch: 6000 Average loss: 0.00206262\n",
      "====> Epoch: 7000 Average loss: 0.00202602\n",
      "====> Epoch: 8000 Average loss: 0.00200150\n",
      "====> Epoch: 9000 Average loss: 0.00194676\n",
      "====> Epoch: 10000 Average loss: 0.00198397\n",
      "Training state:  False\n",
      "---- Done in  202.86622786521912  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00353518\n",
      "====> Epoch: 2000 Average loss: 0.00307830\n",
      "====> Epoch: 3000 Average loss: 0.00287184\n",
      "====> Epoch: 4000 Average loss: 0.00271080\n",
      "====> Epoch: 5000 Average loss: 0.00261430\n",
      "====> Epoch: 6000 Average loss: 0.00257423\n",
      "====> Epoch: 7000 Average loss: 0.00250740\n",
      "====> Epoch: 8000 Average loss: 0.00249201\n",
      "====> Epoch: 9000 Average loss: 0.00245172\n",
      "====> Epoch: 10000 Average loss: 0.00248979\n",
      "Training state:  False\n",
      "---- Done in  198.1005835533142  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 28\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00273745\n",
      "====> Epoch: 2000 Average loss: 0.00227523\n",
      "====> Epoch: 3000 Average loss: 0.00210334\n",
      "====> Epoch: 4000 Average loss: 0.00202603\n",
      "====> Epoch: 5000 Average loss: 0.00200554\n",
      "====> Epoch: 6000 Average loss: 0.00197174\n",
      "====> Epoch: 7000 Average loss: 0.00197374\n",
      "====> Epoch: 8000 Average loss: 0.00197905\n",
      "====> Epoch: 9000 Average loss: 0.00194734\n",
      "====> Epoch: 10000 Average loss: 0.00194899\n",
      "Training state:  False\n",
      "---- Done in  193.06395053863525  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00303402\n",
      "====> Epoch: 2000 Average loss: 0.00271711\n",
      "====> Epoch: 3000 Average loss: 0.00254225\n",
      "====> Epoch: 4000 Average loss: 0.00242100\n",
      "====> Epoch: 5000 Average loss: 0.00235137\n",
      "====> Epoch: 6000 Average loss: 0.00232103\n",
      "====> Epoch: 7000 Average loss: 0.00227448\n",
      "====> Epoch: 8000 Average loss: 0.00221016\n",
      "====> Epoch: 9000 Average loss: 0.00217062\n",
      "====> Epoch: 10000 Average loss: 0.00213798\n",
      "Training state:  False\n",
      "---- Done in  195.7309856414795  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00317537\n",
      "====> Epoch: 2000 Average loss: 0.00287734\n",
      "====> Epoch: 3000 Average loss: 0.00267890\n",
      "====> Epoch: 4000 Average loss: 0.00251202\n",
      "====> Epoch: 5000 Average loss: 0.00246068\n",
      "====> Epoch: 6000 Average loss: 0.00247647\n",
      "====> Epoch: 7000 Average loss: 0.00240891\n",
      "====> Epoch: 8000 Average loss: 0.00233259\n",
      "====> Epoch: 9000 Average loss: 0.00235765\n",
      "====> Epoch: 10000 Average loss: 0.00227722\n",
      "Training state:  False\n",
      "---- Done in  200.62464380264282  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00281573\n",
      "====> Epoch: 2000 Average loss: 0.00225787\n",
      "====> Epoch: 3000 Average loss: 0.00201924\n",
      "====> Epoch: 4000 Average loss: 0.00198281\n",
      "====> Epoch: 5000 Average loss: 0.00195378\n",
      "====> Epoch: 6000 Average loss: 0.00191162\n",
      "====> Epoch: 7000 Average loss: 0.00188585\n",
      "====> Epoch: 8000 Average loss: 0.00187136\n",
      "====> Epoch: 9000 Average loss: 0.00179138\n",
      "====> Epoch: 10000 Average loss: 0.00172980\n",
      "Training state:  False\n",
      "---- Done in  202.7257137298584  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00325304\n",
      "====> Epoch: 2000 Average loss: 0.00284870\n",
      "====> Epoch: 3000 Average loss: 0.00262621\n",
      "====> Epoch: 4000 Average loss: 0.00248431\n",
      "====> Epoch: 5000 Average loss: 0.00236939\n",
      "====> Epoch: 6000 Average loss: 0.00230117\n",
      "====> Epoch: 7000 Average loss: 0.00224416\n",
      "====> Epoch: 8000 Average loss: 0.00222373\n",
      "====> Epoch: 9000 Average loss: 0.00231402\n",
      "====> Epoch: 10000 Average loss: 0.00212994\n",
      "Training state:  False\n",
      "---- Done in  203.30992460250854  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00303956\n",
      "====> Epoch: 2000 Average loss: 0.00279697\n",
      "====> Epoch: 3000 Average loss: 0.00266896\n",
      "====> Epoch: 4000 Average loss: 0.00258757\n",
      "====> Epoch: 5000 Average loss: 0.00248898\n",
      "====> Epoch: 6000 Average loss: 0.00241994\n",
      "====> Epoch: 7000 Average loss: 0.00238945\n",
      "====> Epoch: 8000 Average loss: 0.00231493\n",
      "====> Epoch: 9000 Average loss: 0.00228468\n",
      "====> Epoch: 10000 Average loss: 0.00228307\n",
      "Training state:  False\n",
      "---- Done in  200.8026773929596  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00264647\n",
      "====> Epoch: 2000 Average loss: 0.00213073\n",
      "====> Epoch: 3000 Average loss: 0.00190338\n",
      "====> Epoch: 4000 Average loss: 0.00187758\n",
      "====> Epoch: 5000 Average loss: 0.00177731\n",
      "====> Epoch: 6000 Average loss: 0.00170839\n",
      "====> Epoch: 7000 Average loss: 0.00170391\n",
      "====> Epoch: 8000 Average loss: 0.00169150\n",
      "====> Epoch: 9000 Average loss: 0.00171126\n",
      "====> Epoch: 10000 Average loss: 0.00170873\n",
      "Training state:  False\n",
      "---- Done in  198.89272594451904  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 29\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00293728\n",
      "====> Epoch: 2000 Average loss: 0.00240084\n",
      "====> Epoch: 3000 Average loss: 0.00224230\n",
      "====> Epoch: 4000 Average loss: 0.00213746\n",
      "====> Epoch: 5000 Average loss: 0.00197718\n",
      "====> Epoch: 6000 Average loss: 0.00184672\n",
      "====> Epoch: 7000 Average loss: 0.00183236\n",
      "====> Epoch: 8000 Average loss: 0.00178076\n",
      "====> Epoch: 9000 Average loss: 0.00181107\n",
      "====> Epoch: 10000 Average loss: 0.00176001\n",
      "Training state:  False\n",
      "---- Done in  194.24773573875427  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00315186\n",
      "====> Epoch: 2000 Average loss: 0.00272725\n",
      "====> Epoch: 3000 Average loss: 0.00245847\n",
      "====> Epoch: 4000 Average loss: 0.00232838\n",
      "====> Epoch: 5000 Average loss: 0.00213040\n",
      "====> Epoch: 6000 Average loss: 0.00206986\n",
      "====> Epoch: 7000 Average loss: 0.00197290\n",
      "====> Epoch: 8000 Average loss: 0.00198860\n",
      "====> Epoch: 9000 Average loss: 0.00196270\n",
      "====> Epoch: 10000 Average loss: 0.00193041\n",
      "Training state:  False\n",
      "---- Done in  195.95629477500916  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1000 Average loss: 0.00307291\n",
      "====> Epoch: 2000 Average loss: 0.00255749\n",
      "====> Epoch: 3000 Average loss: 0.00243355\n",
      "====> Epoch: 4000 Average loss: 0.00230022\n",
      "====> Epoch: 5000 Average loss: 0.00226357\n",
      "====> Epoch: 6000 Average loss: 0.00219988\n",
      "====> Epoch: 7000 Average loss: 0.00212450\n",
      "====> Epoch: 8000 Average loss: 0.00206406\n",
      "====> Epoch: 9000 Average loss: 0.00204018\n",
      "====> Epoch: 10000 Average loss: 0.00218212\n",
      "Training state:  False\n",
      "---- Done in  196.34409093856812  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00266906\n",
      "====> Epoch: 2000 Average loss: 0.00242558\n",
      "====> Epoch: 3000 Average loss: 0.00228332\n",
      "====> Epoch: 4000 Average loss: 0.00217508\n",
      "====> Epoch: 5000 Average loss: 0.00216319\n",
      "====> Epoch: 6000 Average loss: 0.00213803\n",
      "====> Epoch: 7000 Average loss: 0.00210938\n",
      "====> Epoch: 8000 Average loss: 0.00207522\n",
      "====> Epoch: 9000 Average loss: 0.00205739\n",
      "====> Epoch: 10000 Average loss: 0.00207796\n",
      "Training state:  False\n",
      "---- Done in  191.44718766212463  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00290585\n",
      "====> Epoch: 2000 Average loss: 0.00219646\n",
      "====> Epoch: 3000 Average loss: 0.00207316\n",
      "====> Epoch: 4000 Average loss: 0.00196293\n",
      "====> Epoch: 5000 Average loss: 0.00194427\n",
      "====> Epoch: 6000 Average loss: 0.00184218\n",
      "====> Epoch: 7000 Average loss: 0.00180921\n",
      "====> Epoch: 8000 Average loss: 0.00183229\n",
      "====> Epoch: 9000 Average loss: 0.00180254\n",
      "====> Epoch: 10000 Average loss: 0.00178751\n",
      "Training state:  False\n",
      "---- Done in  198.15031242370605  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00306824\n",
      "====> Epoch: 2000 Average loss: 0.00264765\n",
      "====> Epoch: 3000 Average loss: 0.00244835\n",
      "====> Epoch: 4000 Average loss: 0.00228491\n",
      "====> Epoch: 5000 Average loss: 0.00220714\n",
      "====> Epoch: 6000 Average loss: 0.00217225\n",
      "====> Epoch: 7000 Average loss: 0.00226883\n",
      "====> Epoch: 8000 Average loss: 0.00213181\n",
      "====> Epoch: 9000 Average loss: 0.00212258\n",
      "====> Epoch: 10000 Average loss: 0.00205766\n",
      "Training state:  False\n",
      "---- Done in  192.55776000022888  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00289643\n",
      "====> Epoch: 2000 Average loss: 0.00237239\n",
      "====> Epoch: 3000 Average loss: 0.00215039\n",
      "====> Epoch: 4000 Average loss: 0.00208930\n",
      "====> Epoch: 5000 Average loss: 0.00193541\n",
      "====> Epoch: 6000 Average loss: 0.00189967\n",
      "====> Epoch: 7000 Average loss: 0.00191144\n",
      "====> Epoch: 8000 Average loss: 0.00185160\n",
      "====> Epoch: 9000 Average loss: 0.00190484\n",
      "====> Epoch: 10000 Average loss: 0.00181038\n",
      "Training state:  False\n",
      "---- Done in  196.79980897903442  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 30\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00304620\n",
      "====> Epoch: 2000 Average loss: 0.00252220\n",
      "====> Epoch: 3000 Average loss: 0.00219195\n",
      "====> Epoch: 4000 Average loss: 0.00207373\n",
      "====> Epoch: 5000 Average loss: 0.00200683\n",
      "====> Epoch: 6000 Average loss: 0.00197187\n",
      "====> Epoch: 7000 Average loss: 0.00190033\n",
      "====> Epoch: 8000 Average loss: 0.00189319\n",
      "====> Epoch: 9000 Average loss: 0.00193705\n",
      "====> Epoch: 10000 Average loss: 0.00182231\n",
      "Training state:  False\n",
      "---- Done in  198.04059481620789  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00288518\n",
      "====> Epoch: 2000 Average loss: 0.00242919\n",
      "====> Epoch: 3000 Average loss: 0.00220550\n",
      "====> Epoch: 4000 Average loss: 0.00207505\n",
      "====> Epoch: 5000 Average loss: 0.00199124\n",
      "====> Epoch: 6000 Average loss: 0.00195098\n",
      "====> Epoch: 7000 Average loss: 0.00187762\n",
      "====> Epoch: 8000 Average loss: 0.00189458\n",
      "====> Epoch: 9000 Average loss: 0.00185247\n",
      "====> Epoch: 10000 Average loss: 0.00207846\n",
      "Training state:  False\n",
      "---- Done in  201.49572563171387  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00299626\n",
      "====> Epoch: 2000 Average loss: 0.00264326\n",
      "====> Epoch: 3000 Average loss: 0.00250188\n",
      "====> Epoch: 4000 Average loss: 0.00246864\n",
      "====> Epoch: 5000 Average loss: 0.00241323\n",
      "====> Epoch: 6000 Average loss: 0.00231920\n",
      "====> Epoch: 7000 Average loss: 0.00226553\n",
      "====> Epoch: 8000 Average loss: 0.00222583\n",
      "====> Epoch: 9000 Average loss: 0.00215991\n",
      "====> Epoch: 10000 Average loss: 0.00209667\n",
      "Training state:  False\n",
      "---- Done in  200.32251286506653  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00320101\n",
      "====> Epoch: 2000 Average loss: 0.00262082\n",
      "====> Epoch: 3000 Average loss: 0.00242874\n",
      "====> Epoch: 4000 Average loss: 0.00233415\n",
      "====> Epoch: 5000 Average loss: 0.00223204\n",
      "====> Epoch: 6000 Average loss: 0.00220840\n",
      "====> Epoch: 7000 Average loss: 0.00215050\n",
      "====> Epoch: 8000 Average loss: 0.00218623\n",
      "====> Epoch: 9000 Average loss: 0.00215787\n",
      "====> Epoch: 10000 Average loss: 0.00211886\n",
      "Training state:  False\n",
      "---- Done in  200.2401099205017  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00311457\n",
      "====> Epoch: 2000 Average loss: 0.00257647\n",
      "====> Epoch: 3000 Average loss: 0.00236799\n",
      "====> Epoch: 4000 Average loss: 0.00232635\n",
      "====> Epoch: 5000 Average loss: 0.00228115\n",
      "====> Epoch: 6000 Average loss: 0.00224765\n",
      "====> Epoch: 7000 Average loss: 0.00219783\n",
      "====> Epoch: 8000 Average loss: 0.00213690\n",
      "====> Epoch: 9000 Average loss: 0.00212396\n",
      "====> Epoch: 10000 Average loss: 0.00205017\n",
      "Training state:  False\n",
      "---- Done in  199.59972262382507  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00249934\n",
      "====> Epoch: 2000 Average loss: 0.00220033\n",
      "====> Epoch: 3000 Average loss: 0.00210192\n",
      "====> Epoch: 4000 Average loss: 0.00193109\n",
      "====> Epoch: 5000 Average loss: 0.00186166\n",
      "====> Epoch: 6000 Average loss: 0.00179443\n",
      "====> Epoch: 7000 Average loss: 0.00181295\n",
      "====> Epoch: 8000 Average loss: 0.00176553\n",
      "====> Epoch: 9000 Average loss: 0.00175877\n",
      "====> Epoch: 10000 Average loss: 0.00175393\n",
      "Training state:  False\n",
      "---- Done in  199.09978127479553  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00230490\n",
      "====> Epoch: 2000 Average loss: 0.00211963\n",
      "====> Epoch: 3000 Average loss: 0.00196043\n",
      "====> Epoch: 4000 Average loss: 0.00188211\n",
      "====> Epoch: 5000 Average loss: 0.00181273\n",
      "====> Epoch: 6000 Average loss: 0.00173068\n",
      "====> Epoch: 7000 Average loss: 0.00171106\n",
      "====> Epoch: 8000 Average loss: 0.00168062\n",
      "====> Epoch: 9000 Average loss: 0.00165398\n",
      "====> Epoch: 10000 Average loss: 0.00158072\n",
      "Training state:  False\n",
      "---- Done in  198.67888641357422  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 31\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00258353\n",
      "====> Epoch: 2000 Average loss: 0.00224114\n",
      "====> Epoch: 3000 Average loss: 0.00207007\n",
      "====> Epoch: 4000 Average loss: 0.00205438\n",
      "====> Epoch: 5000 Average loss: 0.00198707\n",
      "====> Epoch: 6000 Average loss: 0.00194971\n",
      "====> Epoch: 7000 Average loss: 0.00195524\n",
      "====> Epoch: 8000 Average loss: 0.00197710\n",
      "====> Epoch: 9000 Average loss: 0.00193940\n",
      "====> Epoch: 10000 Average loss: 0.00192622\n",
      "Training state:  False\n",
      "---- Done in  196.33320450782776  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00311106\n",
      "====> Epoch: 2000 Average loss: 0.00217655\n",
      "====> Epoch: 3000 Average loss: 0.00198584\n",
      "====> Epoch: 4000 Average loss: 0.00188652\n",
      "====> Epoch: 5000 Average loss: 0.00186147\n",
      "====> Epoch: 6000 Average loss: 0.00182655\n",
      "====> Epoch: 7000 Average loss: 0.00181383\n",
      "====> Epoch: 8000 Average loss: 0.00178141\n",
      "====> Epoch: 9000 Average loss: 0.00175949\n",
      "====> Epoch: 10000 Average loss: 0.00172448\n",
      "Training state:  False\n",
      "---- Done in  194.14973759651184  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00264942\n",
      "====> Epoch: 2000 Average loss: 0.00224714\n",
      "====> Epoch: 3000 Average loss: 0.00195723\n",
      "====> Epoch: 4000 Average loss: 0.00170571\n",
      "====> Epoch: 5000 Average loss: 0.00163834\n",
      "====> Epoch: 6000 Average loss: 0.00158575\n",
      "====> Epoch: 7000 Average loss: 0.00154375\n",
      "====> Epoch: 8000 Average loss: 0.00154434\n",
      "====> Epoch: 9000 Average loss: 0.00155597\n",
      "====> Epoch: 10000 Average loss: 0.00151819\n",
      "Training state:  False\n",
      "---- Done in  199.3411853313446  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1000 Average loss: 0.00290695\n",
      "====> Epoch: 2000 Average loss: 0.00250016\n",
      "====> Epoch: 3000 Average loss: 0.00225207\n",
      "====> Epoch: 4000 Average loss: 0.00212198\n",
      "====> Epoch: 5000 Average loss: 0.00204247\n",
      "====> Epoch: 6000 Average loss: 0.00204969\n",
      "====> Epoch: 7000 Average loss: 0.00205565\n",
      "====> Epoch: 8000 Average loss: 0.00196224\n",
      "====> Epoch: 9000 Average loss: 0.00191219\n",
      "====> Epoch: 10000 Average loss: 0.00202915\n",
      "Training state:  False\n",
      "---- Done in  196.89593815803528  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00309452\n",
      "====> Epoch: 2000 Average loss: 0.00263653\n",
      "====> Epoch: 3000 Average loss: 0.00245619\n",
      "====> Epoch: 4000 Average loss: 0.00234162\n",
      "====> Epoch: 5000 Average loss: 0.00226132\n",
      "====> Epoch: 6000 Average loss: 0.00222269\n",
      "====> Epoch: 7000 Average loss: 0.00222422\n",
      "====> Epoch: 8000 Average loss: 0.00214449\n",
      "====> Epoch: 9000 Average loss: 0.00215431\n",
      "====> Epoch: 10000 Average loss: 0.00210280\n",
      "Training state:  False\n",
      "---- Done in  197.34741282463074  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00302492\n",
      "====> Epoch: 2000 Average loss: 0.00229561\n",
      "====> Epoch: 3000 Average loss: 0.00204186\n",
      "====> Epoch: 4000 Average loss: 0.00193861\n",
      "====> Epoch: 5000 Average loss: 0.00191271\n",
      "====> Epoch: 6000 Average loss: 0.00191391\n",
      "====> Epoch: 7000 Average loss: 0.00188738\n",
      "====> Epoch: 8000 Average loss: 0.00187288\n",
      "====> Epoch: 9000 Average loss: 0.00191414\n",
      "====> Epoch: 10000 Average loss: 0.00186804\n",
      "Training state:  False\n",
      "---- Done in  200.65219593048096  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00303363\n",
      "====> Epoch: 2000 Average loss: 0.00263967\n",
      "====> Epoch: 3000 Average loss: 0.00253957\n",
      "====> Epoch: 4000 Average loss: 0.00237745\n",
      "====> Epoch: 5000 Average loss: 0.00229487\n",
      "====> Epoch: 6000 Average loss: 0.00230433\n",
      "====> Epoch: 7000 Average loss: 0.00224198\n",
      "====> Epoch: 8000 Average loss: 0.00220369\n",
      "====> Epoch: 9000 Average loss: 0.00214494\n",
      "====> Epoch: 10000 Average loss: 0.00211507\n",
      "Training state:  False\n",
      "---- Done in  197.0737669467926  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 32\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00282043\n",
      "====> Epoch: 2000 Average loss: 0.00252539\n",
      "====> Epoch: 3000 Average loss: 0.00244212\n",
      "====> Epoch: 4000 Average loss: 0.00229350\n",
      "====> Epoch: 5000 Average loss: 0.00219806\n",
      "====> Epoch: 6000 Average loss: 0.00210858\n",
      "====> Epoch: 7000 Average loss: 0.00209683\n",
      "====> Epoch: 8000 Average loss: 0.00206277\n",
      "====> Epoch: 9000 Average loss: 0.00202387\n",
      "====> Epoch: 10000 Average loss: 0.00201422\n",
      "Training state:  False\n",
      "---- Done in  201.86731433868408  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00305314\n",
      "====> Epoch: 2000 Average loss: 0.00250211\n",
      "====> Epoch: 3000 Average loss: 0.00218549\n",
      "====> Epoch: 4000 Average loss: 0.00209253\n",
      "====> Epoch: 5000 Average loss: 0.00203712\n",
      "====> Epoch: 6000 Average loss: 0.00199651\n",
      "====> Epoch: 7000 Average loss: 0.00198683\n",
      "====> Epoch: 8000 Average loss: 0.00197233\n",
      "====> Epoch: 9000 Average loss: 0.00196658\n",
      "====> Epoch: 10000 Average loss: 0.00195755\n",
      "Training state:  False\n",
      "---- Done in  200.97025513648987  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00313458\n",
      "====> Epoch: 2000 Average loss: 0.00278363\n",
      "====> Epoch: 3000 Average loss: 0.00268774\n",
      "====> Epoch: 4000 Average loss: 0.00266726\n",
      "====> Epoch: 5000 Average loss: 0.00265323\n",
      "====> Epoch: 6000 Average loss: 0.00259940\n",
      "====> Epoch: 7000 Average loss: 0.00258703\n",
      "====> Epoch: 8000 Average loss: 0.00256234\n",
      "====> Epoch: 9000 Average loss: 0.00254206\n",
      "====> Epoch: 10000 Average loss: 0.00253658\n",
      "Training state:  False\n",
      "---- Done in  198.35764241218567  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00239222\n",
      "====> Epoch: 2000 Average loss: 0.00202644\n",
      "====> Epoch: 3000 Average loss: 0.00192157\n",
      "====> Epoch: 4000 Average loss: 0.00185186\n",
      "====> Epoch: 5000 Average loss: 0.00181853\n",
      "====> Epoch: 6000 Average loss: 0.00183856\n",
      "====> Epoch: 7000 Average loss: 0.00177645\n",
      "====> Epoch: 8000 Average loss: 0.00177489\n",
      "====> Epoch: 9000 Average loss: 0.00181681\n",
      "====> Epoch: 10000 Average loss: 0.00175463\n",
      "Training state:  False\n",
      "---- Done in  200.48764562606812  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00293364\n",
      "====> Epoch: 2000 Average loss: 0.00258860\n",
      "====> Epoch: 3000 Average loss: 0.00244246\n",
      "====> Epoch: 4000 Average loss: 0.00232957\n",
      "====> Epoch: 5000 Average loss: 0.00225992\n",
      "====> Epoch: 6000 Average loss: 0.00217483\n",
      "====> Epoch: 7000 Average loss: 0.00212623\n",
      "====> Epoch: 8000 Average loss: 0.00204013\n",
      "====> Epoch: 9000 Average loss: 0.00195932\n",
      "====> Epoch: 10000 Average loss: 0.00191289\n",
      "Training state:  False\n",
      "---- Done in  206.88996171951294  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00243588\n",
      "====> Epoch: 2000 Average loss: 0.00177067\n",
      "====> Epoch: 3000 Average loss: 0.00164267\n",
      "====> Epoch: 4000 Average loss: 0.00159809\n",
      "====> Epoch: 5000 Average loss: 0.00155243\n",
      "====> Epoch: 6000 Average loss: 0.00152404\n",
      "====> Epoch: 7000 Average loss: 0.00149483\n",
      "====> Epoch: 8000 Average loss: 0.00151920\n",
      "====> Epoch: 9000 Average loss: 0.00151206\n",
      "====> Epoch: 10000 Average loss: 0.00150035\n",
      "Training state:  False\n",
      "---- Done in  197.45283961296082  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00290958\n",
      "====> Epoch: 2000 Average loss: 0.00244918\n",
      "====> Epoch: 3000 Average loss: 0.00229744\n",
      "====> Epoch: 4000 Average loss: 0.00223830\n",
      "====> Epoch: 5000 Average loss: 0.00228761\n",
      "====> Epoch: 6000 Average loss: 0.00221787\n",
      "====> Epoch: 7000 Average loss: 0.00216703\n",
      "====> Epoch: 8000 Average loss: 0.00216421\n",
      "====> Epoch: 9000 Average loss: 0.00213789\n",
      "====> Epoch: 10000 Average loss: 0.00216806\n",
      "Training state:  False\n",
      "---- Done in  194.49908709526062  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 33\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00317167\n",
      "====> Epoch: 2000 Average loss: 0.00284578\n",
      "====> Epoch: 3000 Average loss: 0.00253466\n",
      "====> Epoch: 4000 Average loss: 0.00236436\n",
      "====> Epoch: 5000 Average loss: 0.00252319\n",
      "====> Epoch: 6000 Average loss: 0.00222283\n",
      "====> Epoch: 7000 Average loss: 0.00220758\n",
      "====> Epoch: 8000 Average loss: 0.00217515\n",
      "====> Epoch: 9000 Average loss: 0.00215627\n",
      "====> Epoch: 10000 Average loss: 0.00222239\n",
      "Training state:  False\n",
      "---- Done in  197.29977345466614  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00309382\n",
      "====> Epoch: 2000 Average loss: 0.00287655\n",
      "====> Epoch: 3000 Average loss: 0.00271455\n",
      "====> Epoch: 4000 Average loss: 0.00251475\n",
      "====> Epoch: 5000 Average loss: 0.00238688\n",
      "====> Epoch: 6000 Average loss: 0.00228795\n",
      "====> Epoch: 7000 Average loss: 0.00224742\n",
      "====> Epoch: 8000 Average loss: 0.00221534\n",
      "====> Epoch: 9000 Average loss: 0.00219071\n",
      "====> Epoch: 10000 Average loss: 0.00220714\n",
      "Training state:  False\n",
      "---- Done in  202.13459253311157  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00292971\n",
      "====> Epoch: 2000 Average loss: 0.00229521\n",
      "====> Epoch: 3000 Average loss: 0.00211660\n",
      "====> Epoch: 4000 Average loss: 0.00192393\n",
      "====> Epoch: 5000 Average loss: 0.00181911\n",
      "====> Epoch: 6000 Average loss: 0.00180239\n",
      "====> Epoch: 7000 Average loss: 0.00178358\n",
      "====> Epoch: 8000 Average loss: 0.00172957\n",
      "====> Epoch: 9000 Average loss: 0.00174085\n",
      "====> Epoch: 10000 Average loss: 0.00172604\n",
      "Training state:  False\n",
      "---- Done in  199.51570844650269  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00317336\n",
      "====> Epoch: 2000 Average loss: 0.00282771\n",
      "====> Epoch: 3000 Average loss: 0.00273820\n",
      "====> Epoch: 4000 Average loss: 0.00266287\n",
      "====> Epoch: 5000 Average loss: 0.00262911\n",
      "====> Epoch: 6000 Average loss: 0.00259059\n",
      "====> Epoch: 7000 Average loss: 0.00254118\n",
      "====> Epoch: 8000 Average loss: 0.00257400\n",
      "====> Epoch: 9000 Average loss: 0.00252809\n",
      "====> Epoch: 10000 Average loss: 0.00248455\n",
      "Training state:  False\n",
      "---- Done in  196.74910283088684  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1000 Average loss: 0.00320760\n",
      "====> Epoch: 2000 Average loss: 0.00300347\n",
      "====> Epoch: 3000 Average loss: 0.00293442\n",
      "====> Epoch: 4000 Average loss: 0.00289257\n",
      "====> Epoch: 5000 Average loss: 0.00282485\n",
      "====> Epoch: 6000 Average loss: 0.00280795\n",
      "====> Epoch: 7000 Average loss: 0.00276472\n",
      "====> Epoch: 8000 Average loss: 0.00276294\n",
      "====> Epoch: 9000 Average loss: 0.00274696\n",
      "====> Epoch: 10000 Average loss: 0.00272379\n",
      "Training state:  False\n",
      "---- Done in  201.66581845283508  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00296338\n",
      "====> Epoch: 2000 Average loss: 0.00255604\n",
      "====> Epoch: 3000 Average loss: 0.00233718\n",
      "====> Epoch: 4000 Average loss: 0.00215611\n",
      "====> Epoch: 5000 Average loss: 0.00208270\n",
      "====> Epoch: 6000 Average loss: 0.00204307\n",
      "====> Epoch: 7000 Average loss: 0.00203010\n",
      "====> Epoch: 8000 Average loss: 0.00200109\n",
      "====> Epoch: 9000 Average loss: 0.00197961\n",
      "====> Epoch: 10000 Average loss: 0.00195741\n",
      "Training state:  False\n",
      "---- Done in  203.13535499572754  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00268785\n",
      "====> Epoch: 2000 Average loss: 0.00222356\n",
      "====> Epoch: 3000 Average loss: 0.00208576\n",
      "====> Epoch: 4000 Average loss: 0.00204342\n",
      "====> Epoch: 5000 Average loss: 0.00198336\n",
      "====> Epoch: 6000 Average loss: 0.00195809\n",
      "====> Epoch: 7000 Average loss: 0.00191283\n",
      "====> Epoch: 8000 Average loss: 0.00190002\n",
      "====> Epoch: 9000 Average loss: 0.00182529\n",
      "====> Epoch: 10000 Average loss: 0.00173918\n",
      "Training state:  False\n",
      "---- Done in  198.61812329292297  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 34\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00310026\n",
      "====> Epoch: 2000 Average loss: 0.00246853\n",
      "====> Epoch: 3000 Average loss: 0.00206749\n",
      "====> Epoch: 4000 Average loss: 0.00189342\n",
      "====> Epoch: 5000 Average loss: 0.00185360\n",
      "====> Epoch: 6000 Average loss: 0.00180413\n",
      "====> Epoch: 7000 Average loss: 0.00177154\n",
      "====> Epoch: 8000 Average loss: 0.00175903\n",
      "====> Epoch: 9000 Average loss: 0.00179138\n",
      "====> Epoch: 10000 Average loss: 0.00173813\n",
      "Training state:  False\n",
      "---- Done in  200.6282296180725  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00340867\n",
      "====> Epoch: 2000 Average loss: 0.00294744\n",
      "====> Epoch: 3000 Average loss: 0.00275852\n",
      "====> Epoch: 4000 Average loss: 0.00267109\n",
      "====> Epoch: 5000 Average loss: 0.00260273\n",
      "====> Epoch: 6000 Average loss: 0.00256291\n",
      "====> Epoch: 7000 Average loss: 0.00252284\n",
      "====> Epoch: 8000 Average loss: 0.00253091\n",
      "====> Epoch: 9000 Average loss: 0.00232529\n",
      "====> Epoch: 10000 Average loss: 0.00221160\n",
      "Training state:  False\n",
      "---- Done in  196.87133049964905  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00321012\n",
      "====> Epoch: 2000 Average loss: 0.00266956\n",
      "====> Epoch: 3000 Average loss: 0.00251176\n",
      "====> Epoch: 4000 Average loss: 0.00238335\n",
      "====> Epoch: 5000 Average loss: 0.00228036\n",
      "====> Epoch: 6000 Average loss: 0.00216720\n",
      "====> Epoch: 7000 Average loss: 0.00215471\n",
      "====> Epoch: 8000 Average loss: 0.00213792\n",
      "====> Epoch: 9000 Average loss: 0.00207331\n",
      "====> Epoch: 10000 Average loss: 0.00208746\n",
      "Training state:  False\n",
      "---- Done in  200.8806450366974  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00317954\n",
      "====> Epoch: 2000 Average loss: 0.00290672\n",
      "====> Epoch: 3000 Average loss: 0.00277615\n",
      "====> Epoch: 4000 Average loss: 0.00268051\n",
      "====> Epoch: 5000 Average loss: 0.00265286\n",
      "====> Epoch: 6000 Average loss: 0.00263217\n",
      "====> Epoch: 7000 Average loss: 0.00256652\n",
      "====> Epoch: 8000 Average loss: 0.00255654\n",
      "====> Epoch: 9000 Average loss: 0.00257132\n",
      "====> Epoch: 10000 Average loss: 0.00255422\n",
      "Training state:  False\n",
      "---- Done in  198.86363172531128  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00335132\n",
      "====> Epoch: 2000 Average loss: 0.00287212\n",
      "====> Epoch: 3000 Average loss: 0.00264614\n",
      "====> Epoch: 4000 Average loss: 0.00247622\n",
      "====> Epoch: 5000 Average loss: 0.00243974\n",
      "====> Epoch: 6000 Average loss: 0.00235647\n",
      "====> Epoch: 7000 Average loss: 0.00228664\n",
      "====> Epoch: 8000 Average loss: 0.00227824\n",
      "====> Epoch: 9000 Average loss: 0.00228393\n",
      "====> Epoch: 10000 Average loss: 0.00224475\n",
      "Training state:  False\n",
      "---- Done in  200.735826253891  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00276366\n",
      "====> Epoch: 2000 Average loss: 0.00246767\n",
      "====> Epoch: 3000 Average loss: 0.00231402\n",
      "====> Epoch: 4000 Average loss: 0.00219841\n",
      "====> Epoch: 5000 Average loss: 0.00207983\n",
      "====> Epoch: 6000 Average loss: 0.00199204\n",
      "====> Epoch: 7000 Average loss: 0.00191532\n",
      "====> Epoch: 8000 Average loss: 0.00189939\n",
      "====> Epoch: 9000 Average loss: 0.00187472\n",
      "====> Epoch: 10000 Average loss: 0.00183180\n",
      "Training state:  False\n",
      "---- Done in  203.2013120651245  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00275374\n",
      "====> Epoch: 2000 Average loss: 0.00211627\n",
      "====> Epoch: 3000 Average loss: 0.00197235\n",
      "====> Epoch: 4000 Average loss: 0.00190489\n",
      "====> Epoch: 5000 Average loss: 0.00184358\n",
      "====> Epoch: 6000 Average loss: 0.00183100\n",
      "====> Epoch: 7000 Average loss: 0.00181771\n",
      "====> Epoch: 8000 Average loss: 0.00179509\n",
      "====> Epoch: 9000 Average loss: 0.00178536\n",
      "====> Epoch: 10000 Average loss: 0.00178662\n",
      "Training state:  False\n",
      "---- Done in  201.15706968307495  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 35\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00303410\n",
      "====> Epoch: 2000 Average loss: 0.00268384\n",
      "====> Epoch: 3000 Average loss: 0.00249822\n",
      "====> Epoch: 4000 Average loss: 0.00232905\n",
      "====> Epoch: 5000 Average loss: 0.00221595\n",
      "====> Epoch: 6000 Average loss: 0.00217900\n",
      "====> Epoch: 7000 Average loss: 0.00214539\n",
      "====> Epoch: 8000 Average loss: 0.00213302\n",
      "====> Epoch: 9000 Average loss: 0.00213336\n",
      "====> Epoch: 10000 Average loss: 0.00207175\n",
      "Training state:  False\n",
      "---- Done in  196.90601658821106  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00282703\n",
      "====> Epoch: 2000 Average loss: 0.00230384\n",
      "====> Epoch: 3000 Average loss: 0.00216741\n",
      "====> Epoch: 4000 Average loss: 0.00205314\n",
      "====> Epoch: 5000 Average loss: 0.00202441\n",
      "====> Epoch: 6000 Average loss: 0.00199341\n",
      "====> Epoch: 7000 Average loss: 0.00196892\n",
      "====> Epoch: 8000 Average loss: 0.00193279\n",
      "====> Epoch: 9000 Average loss: 0.00188848\n",
      "====> Epoch: 10000 Average loss: 0.00185328\n",
      "Training state:  False\n",
      "---- Done in  154.8645782470703  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00235190\n",
      "====> Epoch: 2000 Average loss: 0.00208822\n",
      "====> Epoch: 3000 Average loss: 0.00200406\n",
      "====> Epoch: 4000 Average loss: 0.00195999\n",
      "====> Epoch: 5000 Average loss: 0.00190807\n",
      "====> Epoch: 6000 Average loss: 0.00189508\n",
      "====> Epoch: 7000 Average loss: 0.00188263\n",
      "====> Epoch: 8000 Average loss: 0.00189897\n",
      "====> Epoch: 9000 Average loss: 0.00185661\n",
      "====> Epoch: 10000 Average loss: 0.00185576\n",
      "Training state:  False\n",
      "---- Done in  82.91498899459839  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00319418\n",
      "====> Epoch: 2000 Average loss: 0.00271352\n",
      "====> Epoch: 3000 Average loss: 0.00258681\n",
      "====> Epoch: 4000 Average loss: 0.00244418\n",
      "====> Epoch: 5000 Average loss: 0.00238203\n",
      "====> Epoch: 6000 Average loss: 0.00234445\n",
      "====> Epoch: 7000 Average loss: 0.00233296\n",
      "====> Epoch: 8000 Average loss: 0.00231092\n",
      "====> Epoch: 9000 Average loss: 0.00224967\n",
      "====> Epoch: 10000 Average loss: 0.00229711\n",
      "Training state:  False\n",
      "---- Done in  74.22776699066162  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00356573\n",
      "====> Epoch: 2000 Average loss: 0.00316243\n",
      "====> Epoch: 3000 Average loss: 0.00294154\n",
      "====> Epoch: 4000 Average loss: 0.00275169\n",
      "====> Epoch: 5000 Average loss: 0.00266936\n",
      "====> Epoch: 6000 Average loss: 0.00258942\n",
      "====> Epoch: 7000 Average loss: 0.00263457\n",
      "====> Epoch: 8000 Average loss: 0.00251599\n",
      "====> Epoch: 9000 Average loss: 0.00254409\n",
      "====> Epoch: 10000 Average loss: 0.00249040\n",
      "Training state:  False\n",
      "---- Done in  74.69135046005249  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1000 Average loss: 0.00318040\n",
      "====> Epoch: 2000 Average loss: 0.00262858\n",
      "====> Epoch: 3000 Average loss: 0.00241616\n",
      "====> Epoch: 4000 Average loss: 0.00229410\n",
      "====> Epoch: 5000 Average loss: 0.00218376\n",
      "====> Epoch: 6000 Average loss: 0.00213353\n",
      "====> Epoch: 7000 Average loss: 0.00211917\n",
      "====> Epoch: 8000 Average loss: 0.00212440\n",
      "====> Epoch: 9000 Average loss: 0.00207420\n",
      "====> Epoch: 10000 Average loss: 0.00206628\n",
      "Training state:  False\n",
      "---- Done in  73.98984956741333  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00335048\n",
      "====> Epoch: 2000 Average loss: 0.00290431\n",
      "====> Epoch: 3000 Average loss: 0.00274984\n",
      "====> Epoch: 4000 Average loss: 0.00262470\n",
      "====> Epoch: 5000 Average loss: 0.00262677\n",
      "====> Epoch: 6000 Average loss: 0.00258755\n",
      "====> Epoch: 7000 Average loss: 0.00252228\n",
      "====> Epoch: 8000 Average loss: 0.00246899\n",
      "====> Epoch: 9000 Average loss: 0.00249785\n",
      "====> Epoch: 10000 Average loss: 0.00244970\n",
      "Training state:  False\n",
      "---- Done in  73.84392070770264  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 36\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00297177\n",
      "====> Epoch: 2000 Average loss: 0.00272568\n",
      "====> Epoch: 3000 Average loss: 0.00258444\n",
      "====> Epoch: 4000 Average loss: 0.00245071\n",
      "====> Epoch: 5000 Average loss: 0.00233097\n",
      "====> Epoch: 6000 Average loss: 0.00226121\n",
      "====> Epoch: 7000 Average loss: 0.00209268\n",
      "====> Epoch: 8000 Average loss: 0.00208720\n",
      "====> Epoch: 9000 Average loss: 0.00197871\n",
      "====> Epoch: 10000 Average loss: 0.00188017\n",
      "Training state:  False\n",
      "---- Done in  76.87672209739685  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00315682\n",
      "====> Epoch: 2000 Average loss: 0.00261239\n",
      "====> Epoch: 3000 Average loss: 0.00241016\n",
      "====> Epoch: 4000 Average loss: 0.00230105\n",
      "====> Epoch: 5000 Average loss: 0.00227305\n",
      "====> Epoch: 6000 Average loss: 0.00220738\n",
      "====> Epoch: 7000 Average loss: 0.00218390\n",
      "====> Epoch: 8000 Average loss: 0.00217294\n",
      "====> Epoch: 9000 Average loss: 0.00212051\n",
      "====> Epoch: 10000 Average loss: 0.00215501\n",
      "Training state:  False\n",
      "---- Done in  74.36161923408508  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00274960\n",
      "====> Epoch: 2000 Average loss: 0.00222490\n",
      "====> Epoch: 3000 Average loss: 0.00209682\n",
      "====> Epoch: 4000 Average loss: 0.00203454\n",
      "====> Epoch: 5000 Average loss: 0.00204575\n",
      "====> Epoch: 6000 Average loss: 0.00191881\n",
      "====> Epoch: 7000 Average loss: 0.00186753\n",
      "====> Epoch: 8000 Average loss: 0.00182700\n",
      "====> Epoch: 9000 Average loss: 0.00175278\n",
      "====> Epoch: 10000 Average loss: 0.00173916\n",
      "Training state:  False\n",
      "---- Done in  73.94685792922974  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00271346\n",
      "====> Epoch: 2000 Average loss: 0.00222082\n",
      "====> Epoch: 3000 Average loss: 0.00215233\n",
      "====> Epoch: 4000 Average loss: 0.00202523\n",
      "====> Epoch: 5000 Average loss: 0.00200934\n",
      "====> Epoch: 6000 Average loss: 0.00186710\n",
      "====> Epoch: 7000 Average loss: 0.00180789\n",
      "====> Epoch: 8000 Average loss: 0.00179826\n",
      "====> Epoch: 9000 Average loss: 0.00178736\n",
      "====> Epoch: 10000 Average loss: 0.00180073\n",
      "Training state:  False\n",
      "---- Done in  77.78702855110168  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00348977\n",
      "====> Epoch: 2000 Average loss: 0.00316554\n",
      "====> Epoch: 3000 Average loss: 0.00307676\n",
      "====> Epoch: 4000 Average loss: 0.00304708\n",
      "====> Epoch: 5000 Average loss: 0.00301995\n",
      "====> Epoch: 6000 Average loss: 0.00298993\n",
      "====> Epoch: 7000 Average loss: 0.00299501\n",
      "====> Epoch: 8000 Average loss: 0.00295902\n",
      "====> Epoch: 9000 Average loss: 0.00294560\n",
      "====> Epoch: 10000 Average loss: 0.00291549\n",
      "Training state:  False\n",
      "---- Done in  79.22229671478271  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00264868\n",
      "====> Epoch: 2000 Average loss: 0.00246456\n",
      "====> Epoch: 3000 Average loss: 0.00225160\n",
      "====> Epoch: 4000 Average loss: 0.00214006\n",
      "====> Epoch: 5000 Average loss: 0.00208482\n",
      "====> Epoch: 6000 Average loss: 0.00205589\n",
      "====> Epoch: 7000 Average loss: 0.00200914\n",
      "====> Epoch: 8000 Average loss: 0.00202310\n",
      "====> Epoch: 9000 Average loss: 0.00196628\n",
      "====> Epoch: 10000 Average loss: 0.00192275\n",
      "Training state:  False\n",
      "---- Done in  78.94512605667114  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00286102\n",
      "====> Epoch: 2000 Average loss: 0.00240430\n",
      "====> Epoch: 3000 Average loss: 0.00219925\n",
      "====> Epoch: 4000 Average loss: 0.00216194\n",
      "====> Epoch: 5000 Average loss: 0.00210384\n",
      "====> Epoch: 6000 Average loss: 0.00206554\n",
      "====> Epoch: 7000 Average loss: 0.00205086\n",
      "====> Epoch: 8000 Average loss: 0.00201255\n",
      "====> Epoch: 9000 Average loss: 0.00200807\n",
      "====> Epoch: 10000 Average loss: 0.00200899\n",
      "Training state:  False\n",
      "---- Done in  79.28578925132751  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 37\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00278030\n",
      "====> Epoch: 2000 Average loss: 0.00248803\n",
      "====> Epoch: 3000 Average loss: 0.00243261\n",
      "====> Epoch: 4000 Average loss: 0.00228146\n",
      "====> Epoch: 5000 Average loss: 0.00217757\n",
      "====> Epoch: 6000 Average loss: 0.00207402\n",
      "====> Epoch: 7000 Average loss: 0.00209564\n",
      "====> Epoch: 8000 Average loss: 0.00206607\n",
      "====> Epoch: 9000 Average loss: 0.00199955\n",
      "====> Epoch: 10000 Average loss: 0.00194514\n",
      "Training state:  False\n",
      "---- Done in  78.61347222328186  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00270917\n",
      "====> Epoch: 2000 Average loss: 0.00211001\n",
      "====> Epoch: 3000 Average loss: 0.00199532\n",
      "====> Epoch: 4000 Average loss: 0.00198162\n",
      "====> Epoch: 5000 Average loss: 0.00188923\n",
      "====> Epoch: 6000 Average loss: 0.00191491\n",
      "====> Epoch: 7000 Average loss: 0.00188377\n",
      "====> Epoch: 8000 Average loss: 0.00184696\n",
      "====> Epoch: 9000 Average loss: 0.00183306\n",
      "====> Epoch: 10000 Average loss: 0.00183544\n",
      "Training state:  False\n",
      "---- Done in  79.09906888008118  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00302423\n",
      "====> Epoch: 2000 Average loss: 0.00254580\n",
      "====> Epoch: 3000 Average loss: 0.00225138\n",
      "====> Epoch: 4000 Average loss: 0.00212899\n",
      "====> Epoch: 5000 Average loss: 0.00207625\n",
      "====> Epoch: 6000 Average loss: 0.00206855\n",
      "====> Epoch: 7000 Average loss: 0.00205243\n",
      "====> Epoch: 8000 Average loss: 0.00202519\n",
      "====> Epoch: 9000 Average loss: 0.00200452\n",
      "====> Epoch: 10000 Average loss: 0.00194518\n",
      "Training state:  False\n",
      "---- Done in  78.89315557479858  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00309517\n",
      "====> Epoch: 2000 Average loss: 0.00289467\n",
      "====> Epoch: 3000 Average loss: 0.00277806\n",
      "====> Epoch: 4000 Average loss: 0.00264579\n",
      "====> Epoch: 5000 Average loss: 0.00252011\n",
      "====> Epoch: 6000 Average loss: 0.00243982\n",
      "====> Epoch: 7000 Average loss: 0.00241504\n",
      "====> Epoch: 8000 Average loss: 0.00229152\n",
      "====> Epoch: 9000 Average loss: 0.00227591\n",
      "====> Epoch: 10000 Average loss: 0.00226280\n",
      "Training state:  False\n",
      "---- Done in  79.08177781105042  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00315606\n",
      "====> Epoch: 2000 Average loss: 0.00281685\n",
      "====> Epoch: 3000 Average loss: 0.00266823\n",
      "====> Epoch: 4000 Average loss: 0.00265242\n",
      "====> Epoch: 5000 Average loss: 0.00253541\n",
      "====> Epoch: 6000 Average loss: 0.00255174\n",
      "====> Epoch: 7000 Average loss: 0.00250877\n",
      "====> Epoch: 8000 Average loss: 0.00247597\n",
      "====> Epoch: 9000 Average loss: 0.00246499\n",
      "====> Epoch: 10000 Average loss: 0.00246473\n",
      "Training state:  False\n",
      "---- Done in  78.71834635734558  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00278073\n",
      "====> Epoch: 2000 Average loss: 0.00255939\n",
      "====> Epoch: 3000 Average loss: 0.00244201\n",
      "====> Epoch: 4000 Average loss: 0.00238377\n",
      "====> Epoch: 5000 Average loss: 0.00230495\n",
      "====> Epoch: 6000 Average loss: 0.00229869\n",
      "====> Epoch: 7000 Average loss: 0.00224684\n",
      "====> Epoch: 8000 Average loss: 0.00224182\n",
      "====> Epoch: 9000 Average loss: 0.00219290\n",
      "====> Epoch: 10000 Average loss: 0.00218925\n",
      "Training state:  False\n",
      "---- Done in  77.59905934333801  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1000 Average loss: 0.00280251\n",
      "====> Epoch: 2000 Average loss: 0.00227345\n",
      "====> Epoch: 3000 Average loss: 0.00208656\n",
      "====> Epoch: 4000 Average loss: 0.00196919\n",
      "====> Epoch: 5000 Average loss: 0.00188322\n",
      "====> Epoch: 6000 Average loss: 0.00183691\n",
      "====> Epoch: 7000 Average loss: 0.00172702\n",
      "====> Epoch: 8000 Average loss: 0.00173177\n",
      "====> Epoch: 9000 Average loss: 0.00169215\n",
      "====> Epoch: 10000 Average loss: 0.00169399\n",
      "Training state:  False\n",
      "---- Done in  74.02070784568787  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 38\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00272125\n",
      "====> Epoch: 2000 Average loss: 0.00243689\n",
      "====> Epoch: 3000 Average loss: 0.00230135\n",
      "====> Epoch: 4000 Average loss: 0.00220091\n",
      "====> Epoch: 5000 Average loss: 0.00211183\n",
      "====> Epoch: 6000 Average loss: 0.00212040\n",
      "====> Epoch: 7000 Average loss: 0.00209703\n",
      "====> Epoch: 8000 Average loss: 0.00207248\n",
      "====> Epoch: 9000 Average loss: 0.00208200\n",
      "====> Epoch: 10000 Average loss: 0.00205249\n",
      "Training state:  False\n",
      "---- Done in  74.02502536773682  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00276846\n",
      "====> Epoch: 2000 Average loss: 0.00232658\n",
      "====> Epoch: 3000 Average loss: 0.00213876\n",
      "====> Epoch: 4000 Average loss: 0.00192539\n",
      "====> Epoch: 5000 Average loss: 0.00177184\n",
      "====> Epoch: 6000 Average loss: 0.00173524\n",
      "====> Epoch: 7000 Average loss: 0.00171155\n",
      "====> Epoch: 8000 Average loss: 0.00165747\n",
      "====> Epoch: 9000 Average loss: 0.00171441\n",
      "====> Epoch: 10000 Average loss: 0.00162088\n",
      "Training state:  False\n",
      "---- Done in  74.26495552062988  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00275812\n",
      "====> Epoch: 2000 Average loss: 0.00248320\n",
      "====> Epoch: 3000 Average loss: 0.00236341\n",
      "====> Epoch: 4000 Average loss: 0.00225742\n",
      "====> Epoch: 5000 Average loss: 0.00217513\n",
      "====> Epoch: 6000 Average loss: 0.00214969\n",
      "====> Epoch: 7000 Average loss: 0.00206137\n",
      "====> Epoch: 8000 Average loss: 0.00204571\n",
      "====> Epoch: 9000 Average loss: 0.00217927\n",
      "====> Epoch: 10000 Average loss: 0.00199847\n",
      "Training state:  False\n",
      "---- Done in  74.26795053482056  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00286806\n",
      "====> Epoch: 2000 Average loss: 0.00224892\n",
      "====> Epoch: 3000 Average loss: 0.00201939\n",
      "====> Epoch: 4000 Average loss: 0.00192316\n",
      "====> Epoch: 5000 Average loss: 0.00194338\n",
      "====> Epoch: 6000 Average loss: 0.00183879\n",
      "====> Epoch: 7000 Average loss: 0.00188555\n",
      "====> Epoch: 8000 Average loss: 0.00181593\n",
      "====> Epoch: 9000 Average loss: 0.00178314\n",
      "====> Epoch: 10000 Average loss: 0.00181305\n",
      "Training state:  False\n",
      "---- Done in  74.30579447746277  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00279837\n",
      "====> Epoch: 2000 Average loss: 0.00235588\n",
      "====> Epoch: 3000 Average loss: 0.00208581\n",
      "====> Epoch: 4000 Average loss: 0.00197478\n",
      "====> Epoch: 5000 Average loss: 0.00192689\n",
      "====> Epoch: 6000 Average loss: 0.00190099\n",
      "====> Epoch: 7000 Average loss: 0.00188404\n",
      "====> Epoch: 8000 Average loss: 0.00185270\n",
      "====> Epoch: 9000 Average loss: 0.00185767\n",
      "====> Epoch: 10000 Average loss: 0.00187247\n",
      "Training state:  False\n",
      "---- Done in  74.2364068031311  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00324817\n",
      "====> Epoch: 2000 Average loss: 0.00272403\n",
      "====> Epoch: 3000 Average loss: 0.00261021\n",
      "====> Epoch: 4000 Average loss: 0.00254679\n",
      "====> Epoch: 5000 Average loss: 0.00248955\n",
      "====> Epoch: 6000 Average loss: 0.00245989\n",
      "====> Epoch: 7000 Average loss: 0.00243313\n",
      "====> Epoch: 8000 Average loss: 0.00233294\n",
      "====> Epoch: 9000 Average loss: 0.00232209\n",
      "====> Epoch: 10000 Average loss: 0.00231204\n",
      "Training state:  False\n",
      "---- Done in  73.97701716423035  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00249603\n",
      "====> Epoch: 2000 Average loss: 0.00205726\n",
      "====> Epoch: 3000 Average loss: 0.00193089\n",
      "====> Epoch: 4000 Average loss: 0.00176891\n",
      "====> Epoch: 5000 Average loss: 0.00166967\n",
      "====> Epoch: 6000 Average loss: 0.00162819\n",
      "====> Epoch: 7000 Average loss: 0.00160233\n",
      "====> Epoch: 8000 Average loss: 0.00155807\n",
      "====> Epoch: 9000 Average loss: 0.00154011\n",
      "====> Epoch: 10000 Average loss: 0.00149362\n",
      "Training state:  False\n",
      "---- Done in  74.06678557395935  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 39\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00318905\n",
      "====> Epoch: 2000 Average loss: 0.00279757\n",
      "====> Epoch: 3000 Average loss: 0.00271822\n",
      "====> Epoch: 4000 Average loss: 0.00261892\n",
      "====> Epoch: 5000 Average loss: 0.00248666\n",
      "====> Epoch: 6000 Average loss: 0.00241694\n",
      "====> Epoch: 7000 Average loss: 0.00234868\n",
      "====> Epoch: 8000 Average loss: 0.00230393\n",
      "====> Epoch: 9000 Average loss: 0.00232376\n",
      "====> Epoch: 10000 Average loss: 0.00219430\n",
      "Training state:  False\n",
      "---- Done in  73.93176174163818  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00258472\n",
      "====> Epoch: 2000 Average loss: 0.00201876\n",
      "====> Epoch: 3000 Average loss: 0.00187207\n",
      "====> Epoch: 4000 Average loss: 0.00180144\n",
      "====> Epoch: 5000 Average loss: 0.00176179\n",
      "====> Epoch: 6000 Average loss: 0.00175121\n",
      "====> Epoch: 7000 Average loss: 0.00174629\n",
      "====> Epoch: 8000 Average loss: 0.00169848\n",
      "====> Epoch: 9000 Average loss: 0.00171844\n",
      "====> Epoch: 10000 Average loss: 0.00166221\n",
      "Training state:  False\n",
      "---- Done in  73.71849799156189  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00291551\n",
      "====> Epoch: 2000 Average loss: 0.00243703\n",
      "====> Epoch: 3000 Average loss: 0.00227892\n",
      "====> Epoch: 4000 Average loss: 0.00217929\n",
      "====> Epoch: 5000 Average loss: 0.00214884\n",
      "====> Epoch: 6000 Average loss: 0.00210534\n",
      "====> Epoch: 7000 Average loss: 0.00208050\n",
      "====> Epoch: 8000 Average loss: 0.00212796\n",
      "====> Epoch: 9000 Average loss: 0.00204898\n",
      "====> Epoch: 10000 Average loss: 0.00202939\n",
      "Training state:  False\n",
      "---- Done in  74.20317459106445  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00286931\n",
      "====> Epoch: 2000 Average loss: 0.00250689\n",
      "====> Epoch: 3000 Average loss: 0.00238117\n",
      "====> Epoch: 4000 Average loss: 0.00234168\n",
      "====> Epoch: 5000 Average loss: 0.00229709\n",
      "====> Epoch: 6000 Average loss: 0.00227954\n",
      "====> Epoch: 7000 Average loss: 0.00224739\n",
      "====> Epoch: 8000 Average loss: 0.00220027\n",
      "====> Epoch: 9000 Average loss: 0.00221780\n",
      "====> Epoch: 10000 Average loss: 0.00219418\n",
      "Training state:  False\n",
      "---- Done in  74.35306000709534  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00331919\n",
      "====> Epoch: 2000 Average loss: 0.00283770\n",
      "====> Epoch: 3000 Average loss: 0.00260811\n",
      "====> Epoch: 4000 Average loss: 0.00241166\n",
      "====> Epoch: 5000 Average loss: 0.00231693\n",
      "====> Epoch: 6000 Average loss: 0.00217184\n",
      "====> Epoch: 7000 Average loss: 0.00216816\n",
      "====> Epoch: 8000 Average loss: 0.00214654\n",
      "====> Epoch: 9000 Average loss: 0.00212264\n",
      "====> Epoch: 10000 Average loss: 0.00201576\n",
      "Training state:  False\n",
      "---- Done in  74.04680967330933  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00299616\n",
      "====> Epoch: 2000 Average loss: 0.00266700\n",
      "====> Epoch: 3000 Average loss: 0.00242172\n",
      "====> Epoch: 4000 Average loss: 0.00226449\n",
      "====> Epoch: 5000 Average loss: 0.00221816\n",
      "====> Epoch: 6000 Average loss: 0.00209872\n",
      "====> Epoch: 7000 Average loss: 0.00203647\n",
      "====> Epoch: 8000 Average loss: 0.00203007\n",
      "====> Epoch: 9000 Average loss: 0.00200694\n",
      "====> Epoch: 10000 Average loss: 0.00197120\n",
      "Training state:  False\n",
      "---- Done in  74.86108160018921  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00320900\n",
      "====> Epoch: 2000 Average loss: 0.00292876\n",
      "====> Epoch: 3000 Average loss: 0.00284987\n",
      "====> Epoch: 4000 Average loss: 0.00277235\n",
      "====> Epoch: 5000 Average loss: 0.00268874\n",
      "====> Epoch: 6000 Average loss: 0.00267035\n",
      "====> Epoch: 7000 Average loss: 0.00265024\n",
      "====> Epoch: 8000 Average loss: 0.00261619\n",
      "====> Epoch: 9000 Average loss: 0.00261224\n",
      "====> Epoch: 10000 Average loss: 0.00262876\n",
      "Training state:  False\n",
      "---- Done in  74.66803932189941  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 40\n",
      "-- Running for model name:  A_add_lin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1000 Average loss: 0.00258415\n",
      "====> Epoch: 2000 Average loss: 0.00237599\n",
      "====> Epoch: 3000 Average loss: 0.00227690\n",
      "====> Epoch: 4000 Average loss: 0.00223454\n",
      "====> Epoch: 5000 Average loss: 0.00216997\n",
      "====> Epoch: 6000 Average loss: 0.00213062\n",
      "====> Epoch: 7000 Average loss: 0.00213726\n",
      "====> Epoch: 8000 Average loss: 0.00207064\n",
      "====> Epoch: 9000 Average loss: 0.00207092\n",
      "====> Epoch: 10000 Average loss: 0.00204794\n",
      "Training state:  False\n",
      "---- Done in  74.73376226425171  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00280479\n",
      "====> Epoch: 2000 Average loss: 0.00212738\n",
      "====> Epoch: 3000 Average loss: 0.00195379\n",
      "====> Epoch: 4000 Average loss: 0.00191203\n",
      "====> Epoch: 5000 Average loss: 0.00182756\n",
      "====> Epoch: 6000 Average loss: 0.00184186\n",
      "====> Epoch: 7000 Average loss: 0.00183093\n",
      "====> Epoch: 8000 Average loss: 0.00176563\n",
      "====> Epoch: 9000 Average loss: 0.00173695\n",
      "====> Epoch: 10000 Average loss: 0.00173456\n",
      "Training state:  False\n",
      "---- Done in  74.77895879745483  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00294466\n",
      "====> Epoch: 2000 Average loss: 0.00260471\n",
      "====> Epoch: 3000 Average loss: 0.00248871\n",
      "====> Epoch: 4000 Average loss: 0.00239525\n",
      "====> Epoch: 5000 Average loss: 0.00234267\n",
      "====> Epoch: 6000 Average loss: 0.00234191\n",
      "====> Epoch: 7000 Average loss: 0.00233178\n",
      "====> Epoch: 8000 Average loss: 0.00229001\n",
      "====> Epoch: 9000 Average loss: 0.00230816\n",
      "====> Epoch: 10000 Average loss: 0.00229195\n",
      "Training state:  False\n",
      "---- Done in  74.7159469127655  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00265647\n",
      "====> Epoch: 2000 Average loss: 0.00225036\n",
      "====> Epoch: 3000 Average loss: 0.00197048\n",
      "====> Epoch: 4000 Average loss: 0.00193241\n",
      "====> Epoch: 5000 Average loss: 0.00188683\n",
      "====> Epoch: 6000 Average loss: 0.00184054\n",
      "====> Epoch: 7000 Average loss: 0.00181231\n",
      "====> Epoch: 8000 Average loss: 0.00178064\n",
      "====> Epoch: 9000 Average loss: 0.00178674\n",
      "====> Epoch: 10000 Average loss: 0.00177324\n",
      "Training state:  False\n",
      "---- Done in  74.6738646030426  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00252495\n",
      "====> Epoch: 2000 Average loss: 0.00227996\n",
      "====> Epoch: 3000 Average loss: 0.00212409\n",
      "====> Epoch: 4000 Average loss: 0.00208992\n",
      "====> Epoch: 5000 Average loss: 0.00197316\n",
      "====> Epoch: 6000 Average loss: 0.00194746\n",
      "====> Epoch: 7000 Average loss: 0.00191179\n",
      "====> Epoch: 8000 Average loss: 0.00188227\n",
      "====> Epoch: 9000 Average loss: 0.00186303\n",
      "====> Epoch: 10000 Average loss: 0.00186287\n",
      "Training state:  False\n",
      "---- Done in  74.92388343811035  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00306215\n",
      "====> Epoch: 2000 Average loss: 0.00248586\n",
      "====> Epoch: 3000 Average loss: 0.00224161\n",
      "====> Epoch: 4000 Average loss: 0.00211234\n",
      "====> Epoch: 5000 Average loss: 0.00207368\n",
      "====> Epoch: 6000 Average loss: 0.00205812\n",
      "====> Epoch: 7000 Average loss: 0.00199418\n",
      "====> Epoch: 8000 Average loss: 0.00202551\n",
      "====> Epoch: 9000 Average loss: 0.00197294\n",
      "====> Epoch: 10000 Average loss: 0.00199208\n",
      "Training state:  False\n",
      "---- Done in  74.69960188865662  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00279000\n",
      "====> Epoch: 2000 Average loss: 0.00253856\n",
      "====> Epoch: 3000 Average loss: 0.00244370\n",
      "====> Epoch: 4000 Average loss: 0.00234121\n",
      "====> Epoch: 5000 Average loss: 0.00230150\n",
      "====> Epoch: 6000 Average loss: 0.00220065\n",
      "====> Epoch: 7000 Average loss: 0.00213609\n",
      "====> Epoch: 8000 Average loss: 0.00203726\n",
      "====> Epoch: 9000 Average loss: 0.00211328\n",
      "====> Epoch: 10000 Average loss: 0.00199358\n",
      "Training state:  False\n",
      "---- Done in  74.8918182849884  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 41\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00279749\n",
      "====> Epoch: 2000 Average loss: 0.00215180\n",
      "====> Epoch: 3000 Average loss: 0.00187013\n",
      "====> Epoch: 4000 Average loss: 0.00177791\n",
      "====> Epoch: 5000 Average loss: 0.00173000\n",
      "====> Epoch: 6000 Average loss: 0.00168194\n",
      "====> Epoch: 7000 Average loss: 0.00166205\n",
      "====> Epoch: 8000 Average loss: 0.00162088\n",
      "====> Epoch: 9000 Average loss: 0.00161697\n",
      "====> Epoch: 10000 Average loss: 0.00164826\n",
      "Training state:  False\n",
      "---- Done in  74.93154573440552  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00323458\n",
      "====> Epoch: 2000 Average loss: 0.00281353\n",
      "====> Epoch: 3000 Average loss: 0.00257789\n",
      "====> Epoch: 4000 Average loss: 0.00236791\n",
      "====> Epoch: 5000 Average loss: 0.00225073\n",
      "====> Epoch: 6000 Average loss: 0.00219992\n",
      "====> Epoch: 7000 Average loss: 0.00217064\n",
      "====> Epoch: 8000 Average loss: 0.00211613\n",
      "====> Epoch: 9000 Average loss: 0.00207979\n",
      "====> Epoch: 10000 Average loss: 0.00210596\n",
      "Training state:  False\n",
      "---- Done in  74.6989574432373  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00295425\n",
      "====> Epoch: 2000 Average loss: 0.00258437\n",
      "====> Epoch: 3000 Average loss: 0.00238539\n",
      "====> Epoch: 4000 Average loss: 0.00231026\n",
      "====> Epoch: 5000 Average loss: 0.00228558\n",
      "====> Epoch: 6000 Average loss: 0.00224581\n",
      "====> Epoch: 7000 Average loss: 0.00221044\n",
      "====> Epoch: 8000 Average loss: 0.00225454\n",
      "====> Epoch: 9000 Average loss: 0.00220986\n",
      "====> Epoch: 10000 Average loss: 0.00219230\n",
      "Training state:  False\n",
      "---- Done in  74.83089590072632  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00277705\n",
      "====> Epoch: 2000 Average loss: 0.00234215\n",
      "====> Epoch: 3000 Average loss: 0.00218669\n",
      "====> Epoch: 4000 Average loss: 0.00211793\n",
      "====> Epoch: 5000 Average loss: 0.00203363\n",
      "====> Epoch: 6000 Average loss: 0.00197985\n",
      "====> Epoch: 7000 Average loss: 0.00193130\n",
      "====> Epoch: 8000 Average loss: 0.00197114\n",
      "====> Epoch: 9000 Average loss: 0.00191112\n",
      "====> Epoch: 10000 Average loss: 0.00193647\n",
      "Training state:  False\n",
      "---- Done in  74.88588213920593  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00303780\n",
      "====> Epoch: 2000 Average loss: 0.00268561\n",
      "====> Epoch: 3000 Average loss: 0.00249065\n",
      "====> Epoch: 4000 Average loss: 0.00243138\n",
      "====> Epoch: 5000 Average loss: 0.00239963\n",
      "====> Epoch: 6000 Average loss: 0.00236726\n",
      "====> Epoch: 7000 Average loss: 0.00229257\n",
      "====> Epoch: 8000 Average loss: 0.00225374\n",
      "====> Epoch: 9000 Average loss: 0.00223438\n",
      "====> Epoch: 10000 Average loss: 0.00219933\n",
      "Training state:  False\n",
      "---- Done in  74.71252059936523  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00294905\n",
      "====> Epoch: 2000 Average loss: 0.00235860\n",
      "====> Epoch: 3000 Average loss: 0.00208190\n",
      "====> Epoch: 4000 Average loss: 0.00190301\n",
      "====> Epoch: 5000 Average loss: 0.00183955\n",
      "====> Epoch: 6000 Average loss: 0.00181875\n",
      "====> Epoch: 7000 Average loss: 0.00177848\n",
      "====> Epoch: 8000 Average loss: 0.00171563\n",
      "====> Epoch: 9000 Average loss: 0.00166986\n",
      "====> Epoch: 10000 Average loss: 0.00167380\n",
      "Training state:  False\n",
      "---- Done in  74.68803215026855  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00254843\n",
      "====> Epoch: 2000 Average loss: 0.00226975\n",
      "====> Epoch: 3000 Average loss: 0.00215828\n",
      "====> Epoch: 4000 Average loss: 0.00203517\n",
      "====> Epoch: 5000 Average loss: 0.00192813\n",
      "====> Epoch: 6000 Average loss: 0.00181628\n",
      "====> Epoch: 7000 Average loss: 0.00173377\n",
      "====> Epoch: 8000 Average loss: 0.00167740\n",
      "====> Epoch: 9000 Average loss: 0.00168736\n",
      "====> Epoch: 10000 Average loss: 0.00165217\n",
      "Training state:  False\n",
      "---- Done in  74.58570432662964  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 42\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00270702\n",
      "====> Epoch: 2000 Average loss: 0.00235985\n",
      "====> Epoch: 3000 Average loss: 0.00221644\n",
      "====> Epoch: 4000 Average loss: 0.00209197\n",
      "====> Epoch: 5000 Average loss: 0.00200766\n",
      "====> Epoch: 6000 Average loss: 0.00195899\n",
      "====> Epoch: 7000 Average loss: 0.00186262\n",
      "====> Epoch: 8000 Average loss: 0.00180987\n",
      "====> Epoch: 9000 Average loss: 0.00171772\n",
      "====> Epoch: 10000 Average loss: 0.00168962\n",
      "Training state:  False\n",
      "---- Done in  74.89835166931152  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1000 Average loss: 0.00260686\n",
      "====> Epoch: 2000 Average loss: 0.00183079\n",
      "====> Epoch: 3000 Average loss: 0.00165314\n",
      "====> Epoch: 4000 Average loss: 0.00161135\n",
      "====> Epoch: 5000 Average loss: 0.00152941\n",
      "====> Epoch: 6000 Average loss: 0.00148115\n",
      "====> Epoch: 7000 Average loss: 0.00145412\n",
      "====> Epoch: 8000 Average loss: 0.00145476\n",
      "====> Epoch: 9000 Average loss: 0.00142571\n",
      "====> Epoch: 10000 Average loss: 0.00142326\n",
      "Training state:  False\n",
      "---- Done in  74.65212225914001  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00261131\n",
      "====> Epoch: 2000 Average loss: 0.00205116\n",
      "====> Epoch: 3000 Average loss: 0.00196821\n",
      "====> Epoch: 4000 Average loss: 0.00190560\n",
      "====> Epoch: 5000 Average loss: 0.00186414\n",
      "====> Epoch: 6000 Average loss: 0.00180738\n",
      "====> Epoch: 7000 Average loss: 0.00175656\n",
      "====> Epoch: 8000 Average loss: 0.00172740\n",
      "====> Epoch: 9000 Average loss: 0.00169531\n",
      "====> Epoch: 10000 Average loss: 0.00168177\n",
      "Training state:  False\n",
      "---- Done in  74.7586395740509  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00277608\n",
      "====> Epoch: 2000 Average loss: 0.00224293\n",
      "====> Epoch: 3000 Average loss: 0.00200511\n",
      "====> Epoch: 4000 Average loss: 0.00181426\n",
      "====> Epoch: 5000 Average loss: 0.00172766\n",
      "====> Epoch: 6000 Average loss: 0.00178299\n",
      "====> Epoch: 7000 Average loss: 0.00163268\n",
      "====> Epoch: 8000 Average loss: 0.00161085\n",
      "====> Epoch: 9000 Average loss: 0.00159678\n",
      "====> Epoch: 10000 Average loss: 0.00159239\n",
      "Training state:  False\n",
      "---- Done in  74.86939287185669  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00287880\n",
      "====> Epoch: 2000 Average loss: 0.00253093\n",
      "====> Epoch: 3000 Average loss: 0.00242273\n",
      "====> Epoch: 4000 Average loss: 0.00237127\n",
      "====> Epoch: 5000 Average loss: 0.00231164\n",
      "====> Epoch: 6000 Average loss: 0.00225974\n",
      "====> Epoch: 7000 Average loss: 0.00238700\n",
      "====> Epoch: 8000 Average loss: 0.00228827\n",
      "====> Epoch: 9000 Average loss: 0.00222655\n",
      "====> Epoch: 10000 Average loss: 0.00223177\n",
      "Training state:  False\n",
      "---- Done in  74.7301676273346  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00314578\n",
      "====> Epoch: 2000 Average loss: 0.00279060\n",
      "====> Epoch: 3000 Average loss: 0.00265161\n",
      "====> Epoch: 4000 Average loss: 0.00255108\n",
      "====> Epoch: 5000 Average loss: 0.00243763\n",
      "====> Epoch: 6000 Average loss: 0.00237174\n",
      "====> Epoch: 7000 Average loss: 0.00232988\n",
      "====> Epoch: 8000 Average loss: 0.00230354\n",
      "====> Epoch: 9000 Average loss: 0.00223114\n",
      "====> Epoch: 10000 Average loss: 0.00223455\n",
      "Training state:  False\n",
      "---- Done in  74.68755149841309  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00277631\n",
      "====> Epoch: 2000 Average loss: 0.00237564\n",
      "====> Epoch: 3000 Average loss: 0.00221372\n",
      "====> Epoch: 4000 Average loss: 0.00208857\n",
      "====> Epoch: 5000 Average loss: 0.00208737\n",
      "====> Epoch: 6000 Average loss: 0.00200990\n",
      "====> Epoch: 7000 Average loss: 0.00197875\n",
      "====> Epoch: 8000 Average loss: 0.00187942\n",
      "====> Epoch: 9000 Average loss: 0.00182050\n",
      "====> Epoch: 10000 Average loss: 0.00185244\n",
      "Training state:  False\n",
      "---- Done in  74.72257685661316  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 43\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00273716\n",
      "====> Epoch: 2000 Average loss: 0.00249129\n",
      "====> Epoch: 3000 Average loss: 0.00220878\n",
      "====> Epoch: 4000 Average loss: 0.00205542\n",
      "====> Epoch: 5000 Average loss: 0.00200469\n",
      "====> Epoch: 6000 Average loss: 0.00196260\n",
      "====> Epoch: 7000 Average loss: 0.00192778\n",
      "====> Epoch: 8000 Average loss: 0.00187764\n",
      "====> Epoch: 9000 Average loss: 0.00187520\n",
      "====> Epoch: 10000 Average loss: 0.00188235\n",
      "Training state:  False\n",
      "---- Done in  74.75946521759033  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00272961\n",
      "====> Epoch: 2000 Average loss: 0.00238466\n",
      "====> Epoch: 3000 Average loss: 0.00218324\n",
      "====> Epoch: 4000 Average loss: 0.00206338\n",
      "====> Epoch: 5000 Average loss: 0.00196375\n",
      "====> Epoch: 6000 Average loss: 0.00193646\n",
      "====> Epoch: 7000 Average loss: 0.00190813\n",
      "====> Epoch: 8000 Average loss: 0.00189679\n",
      "====> Epoch: 9000 Average loss: 0.00197997\n",
      "====> Epoch: 10000 Average loss: 0.00190058\n",
      "Training state:  False\n",
      "---- Done in  74.78785800933838  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00334967\n",
      "====> Epoch: 2000 Average loss: 0.00285054\n",
      "====> Epoch: 3000 Average loss: 0.00264822\n",
      "====> Epoch: 4000 Average loss: 0.00258965\n",
      "====> Epoch: 5000 Average loss: 0.00257816\n",
      "====> Epoch: 6000 Average loss: 0.00255555\n",
      "====> Epoch: 7000 Average loss: 0.00250320\n",
      "====> Epoch: 8000 Average loss: 0.00248074\n",
      "====> Epoch: 9000 Average loss: 0.00248363\n",
      "====> Epoch: 10000 Average loss: 0.00245191\n",
      "Training state:  False\n",
      "---- Done in  74.72625708580017  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00227851\n",
      "====> Epoch: 2000 Average loss: 0.00202339\n",
      "====> Epoch: 3000 Average loss: 0.00192488\n",
      "====> Epoch: 4000 Average loss: 0.00185510\n",
      "====> Epoch: 5000 Average loss: 0.00179812\n",
      "====> Epoch: 6000 Average loss: 0.00179574\n",
      "====> Epoch: 7000 Average loss: 0.00178123\n",
      "====> Epoch: 8000 Average loss: 0.00178899\n",
      "====> Epoch: 9000 Average loss: 0.00178289\n",
      "====> Epoch: 10000 Average loss: 0.00174923\n",
      "Training state:  False\n",
      "---- Done in  74.81403112411499  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00308815\n",
      "====> Epoch: 2000 Average loss: 0.00286238\n",
      "====> Epoch: 3000 Average loss: 0.00246241\n",
      "====> Epoch: 4000 Average loss: 0.00205266\n",
      "====> Epoch: 5000 Average loss: 0.00200490\n",
      "====> Epoch: 6000 Average loss: 0.00192796\n",
      "====> Epoch: 7000 Average loss: 0.00191400\n",
      "====> Epoch: 8000 Average loss: 0.00187398\n",
      "====> Epoch: 9000 Average loss: 0.00188252\n",
      "====> Epoch: 10000 Average loss: 0.00183819\n",
      "Training state:  False\n",
      "---- Done in  74.63762784004211  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00318557\n",
      "====> Epoch: 2000 Average loss: 0.00281685\n",
      "====> Epoch: 3000 Average loss: 0.00267894\n",
      "====> Epoch: 4000 Average loss: 0.00253125\n",
      "====> Epoch: 5000 Average loss: 0.00248642\n",
      "====> Epoch: 6000 Average loss: 0.00245714\n",
      "====> Epoch: 7000 Average loss: 0.00241404\n",
      "====> Epoch: 8000 Average loss: 0.00237578\n",
      "====> Epoch: 9000 Average loss: 0.00236789\n",
      "====> Epoch: 10000 Average loss: 0.00226226\n",
      "Training state:  False\n",
      "---- Done in  76.85780787467957  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n"
     ]
    }
   ],
   "source": [
    "assignment_model_names = ['A_add_lin', 'B_add_mild_nlin', 'C_add_mod_nlin', 'D_mild_nadd_lin',\n",
    "                     'E_mild_nadd_mild_nlin', 'F_mod_nadd_lin', 'G_mod_nadd_mod_nlin']\n",
    "\n",
    "for dataset_number in range(25, 100):\n",
    "    print(\"Starting run for Dataset {}\".format(dataset_number))\n",
    "    \n",
    "    for model_name in assignment_model_names:\n",
    "        print(\"-- Running for model name: \", model_name)\n",
    "        \n",
    "        start = time()\n",
    "\n",
    "        dataset = CovariateDataset(\"n_{}_model_{}_v_{}_covar_data\", [1000, model_name, dataset_number])\n",
    "\n",
    "        trained_model, original_data, binary_mu_out, normal_mu_out, mu_latent, logvar_latent = \\\n",
    "            train_model(ModifiedVAE, dataset, dataset_number,verbose=True)\n",
    "\n",
    "        encode_data(trained_model, dataset)\n",
    "\n",
    "        print(\"---- Done in \", time() - start, \" seconds\\n\")\n",
    "                \n",
    "    print(\"================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1000/10000], loss:0.4875\n",
      "epoch [2000/10000], loss:0.4456\n",
      "epoch [3000/10000], loss:0.4104\n",
      "epoch [4000/10000], loss:0.3195\n",
      "epoch [5000/10000], loss:0.3145\n",
      "epoch [6000/10000], loss:0.3127\n",
      "epoch [7000/10000], loss:0.3118\n",
      "epoch [8000/10000], loss:0.3107\n",
      "epoch [9000/10000], loss:0.3096\n",
      "epoch [10000/10000], loss:0.3084\n",
      "Final loss: loss:0.3084\n",
      "epoch [1000/10000], loss:0.5869\n",
      "epoch [2000/10000], loss:0.5415\n",
      "epoch [3000/10000], loss:0.5173\n",
      "epoch [4000/10000], loss:0.4197\n",
      "epoch [5000/10000], loss:0.4216\n",
      "epoch [6000/10000], loss:0.4136\n",
      "epoch [7000/10000], loss:0.4132\n",
      "epoch [8000/10000], loss:0.4127\n",
      "epoch [9000/10000], loss:0.4123\n",
      "epoch [10000/10000], loss:0.4118\n",
      "Final loss: loss:0.4118\n"
     ]
    }
   ],
   "source": [
    "models_to_rerun = [('A_add_lin', 12, 'sparsity'), ('G_mod_nadd_mod_nlin', 40, 'sparsity')]\n",
    "\n",
    "for model_name, dataset_number, loss_type in models_to_rerun:\n",
    "    dataset = CovariateDataset(\"n_{}_model_{}_v_{}_covar_data\", [1000, model_name, dataset_number])\n",
    "    trained_model, final_loss = train_model(\n",
    "                                        autoencoder,\n",
    "                                        dataset,\n",
    "                                        loss=loss_type,\n",
    "                                        verbose=True)\n",
    "    encode_data(trained_model, dataset, loss=loss_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
