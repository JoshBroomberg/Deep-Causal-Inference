{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = \"../Data/Raw/\"\n",
    "PROCESSED_DATA_DIR = \"../Data/Processed/VAE/\"\n",
    "\n",
    "class CovariateDataset(Dataset):\n",
    "    def __init__(self, file_name_pattern, file_name_args):\n",
    "        self.file_name = file_name_pattern.format(*file_name_args)\n",
    "        self.data = np.loadtxt(RAW_DATA_DIR + self.file_name + \".csv\", delimiter=\",\")[:, 1:] # remove bias\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index].astype(float), 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def save_processed_data(self, data):\n",
    "        name = PROCESSED_DATA_DIR + self.file_name+\".csv\"\n",
    "        np.savetxt(name, data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Based on an example from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "# Extended to place a different prior on binary vs normal vars\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "ZDIMS = 4 # latent dimensions\n",
    "INTERMEDIATE_DIMS = 32\n",
    "FEATURES = 10\n",
    "DIAG_VAR = True\n",
    "\n",
    "BINARY = [0, 2, 5, 7, 8]\n",
    "NORMAL = [1, 3, 5, 6, 9]\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # ENCODER LAYERS\n",
    "        self.dense1 = nn.Linear(FEATURES, INTERMEDIATE_DIMS)\n",
    "        self.dense2_1 = nn.Linear(INTERMEDIATE_DIMS, ZDIMS)  # mu layer\n",
    "        self.dense2_2 = nn.Linear(INTERMEDIATE_DIMS, ZDIMS)  # logvariance layer\n",
    "        \n",
    "        # this last layer bottlenecks through ZDIMS connections\n",
    "\n",
    "        # DECODER LAYERS\n",
    "        self.dense3 = nn.Linear(ZDIMS, INTERMEDIATE_DIMS)\n",
    "        self.dense4 = nn.Linear(INTERMEDIATE_DIMS, FEATURES)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.dense1(x))\n",
    "        return self.dense2_1(h1), self.dense2_2(h1) #mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # If we sampled directly from the latent distribution\n",
    "            # we wouldn't be able to backprop the results because\n",
    "            # there is no clear grad on the distribution\n",
    "\n",
    "            # This reparam samples from a unit gaussian and then scales\n",
    "            # by the latent parameters giving a defined route to backprop.\n",
    "\n",
    "            std = logvar.mul(0.5).exp_() \n",
    "\n",
    "            # Sample from a unit gaussian with dimensions matching\n",
    "            # the latent space.\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "\n",
    "            return eps.mul(std).add_(mu) # rescale and return\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.dense3(z))\n",
    "        mu_out = self.dense4(h3)# Deleted: self.sigmoid(self.dense4(h3))\n",
    "        \n",
    "        return mu_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, FEATURES))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        mu_out = self.decode(z)\n",
    "        return mu_out, mu, logvar\n",
    "\n",
    "def loss_function(recon_batch_mu, batch_x, mu_latent, logvar_latent):\n",
    "    \n",
    "    # MSE: how good is the reconstruction in terms of\n",
    "    mse_loss = nn.MSELoss(size_average=False)\n",
    "    recon_loss = mse_loss(recon_batch_mu, batch_x)\n",
    "    \n",
    "    recon_loss /= batch_x.size()[0]\n",
    "    \n",
    "    # KLD is Kullbackâ€“Leibler divergence. Regularize VAE by\n",
    "    # penalizing divergence from the prior\n",
    "\n",
    "    # See Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    KLD = -0.5 * torch.sum(1 + logvar_latent - mu_latent.pow(2) - logvar_latent.exp())\n",
    "    # Normalise by same number of elements as in reconstruction\n",
    "    KLD /= batch_x.size()[0] * FEATURES\n",
    "    \n",
    "#     print(\"RL\", recon_loss)\n",
    "#     print(\"KLD\", KLD)\n",
    "    return recon_loss + KLD\n",
    "\n",
    "def train(model, optimizer, epoch, data_loader, log_results=False):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):\n",
    "        data = Variable(data)\n",
    "        data = data.float()\n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon_data, mu_latent, logvar_latent = model(data)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = loss_function(recon_data, data, mu_latent, logvar_latent)\n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        # Find the gradient and descend\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if log_results:\n",
    "        print('====> Epoch: {} Average loss: {:.8f}'.format(\n",
    "              epoch, train_loss / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Based on an example from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "# Extended to place a different prior on binary vs normal vars\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "ZDIMS = 4 # latent dimensions\n",
    "INTERMEDIATE_DIMS = 32\n",
    "FEATURES = 10\n",
    "DIAG_VAR = True\n",
    "\n",
    "BINARY = [0, 2, 5, 7, 8]\n",
    "NORMAL = [1, 3, 5, 6, 9]\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "class ModifiedVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedVAE, self).__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # ENCODER LAYERS\n",
    "        self.dense1 = nn.Linear(FEATURES, INTERMEDIATE_DIMS)\n",
    "        self.dense2_1 = nn.Linear(INTERMEDIATE_DIMS, ZDIMS)  # mu layer\n",
    "        self.dense2_2 = nn.Linear(INTERMEDIATE_DIMS, ZDIMS)  # logvariance layer\n",
    "        \n",
    "        # this last layer bottlenecks through ZDIMS connections\n",
    "\n",
    "        # DECODER LAYERS\n",
    "        self.dense3 = nn.Linear(ZDIMS, INTERMEDIATE_DIMS)\n",
    "        self.dense4 = nn.Linear(INTERMEDIATE_DIMS, len(BINARY))\n",
    "        self.dense5 = nn.Linear(INTERMEDIATE_DIMS, len(NORMAL))\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.dense1(x))\n",
    "        return self.dense2_1(h1), self.dense2_2(h1) #mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # If we sampled directly from the latent distribution\n",
    "            # we wouldn't be able to backprop the results because\n",
    "            # there is no clear grad on the distribution\n",
    "\n",
    "            # This reparam samples from a unit gaussian and then scales\n",
    "            # by the latent parameters giving a defined route to backprop.\n",
    "\n",
    "            std = logvar.mul(0.5).exp_() \n",
    "\n",
    "            # Sample from a unit gaussian with dimensions matching\n",
    "            # the latent space.\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "\n",
    "            return eps.mul(std).add_(mu) # rescale and return\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.dense3(z))\n",
    "        binary_mu_out = self.sigmoid(self.dense4(h3))\n",
    "        normal_mu_out = self.dense5(h3)\n",
    "        \n",
    "        return binary_mu_out, normal_mu_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_mu, latent_logvar = self.encode(x.view(-1, FEATURES))\n",
    "        z = self.reparameterize(latent_mu, latent_logvar)\n",
    "        binary_mu_out, normal_mu_out = self.decode(z)\n",
    "        return binary_mu_out, normal_mu_out, latent_mu, latent_logvar\n",
    "\n",
    "def loss_function(recon_binary_mu, recon_normal_mu, batch_x, mu_latent, logvar_latent):\n",
    "    \n",
    "    # MSE: how good is the reconstruction in terms of\n",
    "    mse_loss = nn.MSELoss(size_average=False)\n",
    "    normal_recon_loss = mse_loss(recon_normal_mu, batch_x[:, NORMAL])\n",
    "    normal_recon_loss /= (batch_x.size()[0])\n",
    "    \n",
    "    # Cross Entropy:\n",
    "    BCE = F.binary_cross_entropy(recon_binary_mu, batch_x[:, BINARY], size_average=False)\n",
    "    BCE /= (batch_x.size()[0])\n",
    "    \n",
    "    # KLD is Kullbackâ€“Leibler divergence. Regularize VAE by\n",
    "    # penalizing divergence from the prior\n",
    "\n",
    "    # See Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    KLD = -0.5 * torch.sum(1 + logvar_latent - mu_latent.pow(2) - logvar_latent.exp())\n",
    "    # Normalise by same number of elements as in reconstruction\n",
    "    KLD /= batch_x.size()[0] * FEATURES\n",
    "    \n",
    "#     print(\"RL\", normal_recon_loss.data.cpu().numpy()[0])\n",
    "#     print(\"BCE\", BCE.data.cpu().numpy()[0])\n",
    "#     print(\"KLD\", KLD.data.cpu().numpy()[0])\n",
    "    \n",
    "    return normal_recon_loss + BCE + KLD\n",
    "\n",
    "def train(model, optimizer, epoch, data_loader, log_results=False):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):\n",
    "        data = Variable(data)\n",
    "        data = data.float()\n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        binary_mu_out, normal_mu_out, mu_latent, logvar_latent = model(data)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = loss_function(binary_mu_out, normal_mu_out, data, mu_latent, logvar_latent)\n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        # Find the gradient and descend\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if log_results:\n",
    "        print('====> Epoch: {} Average loss: {:.8f}'.format(\n",
    "              epoch, train_loss / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Process Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model_class, dataset, dataset_number, verbose=True):\n",
    "    model = model_class()\n",
    "    if CUDA:\n",
    "        model = model.cuda()\n",
    "\n",
    "    num_epochs = 10000\n",
    "    batch_size = 1000\n",
    "    learning_rate = 1e-2\n",
    "    lr_sched = False\n",
    "         \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [int(num_epochs/4), int(3*num_epochs/4)], gamma=0.1)\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        checkpoint_interval = int(num_epochs/10)\n",
    "        \n",
    "        if lr_sched:\n",
    "            scheduler.step()\n",
    "\n",
    "        log = False\n",
    "        if epoch%checkpoint_interval == 0:\n",
    "            log = True\n",
    "            \n",
    "        train(model, optimizer, epoch, data_loader, log_results=log)\n",
    "    \n",
    "\n",
    "    torch.save(model.state_dict(), \"../Models/VAE_{}.pth\".format(dataset_number))\n",
    "    \n",
    "    # Show reconstruction\n",
    "    model.eval()\n",
    "    print(\"Training state: \", model.training)\n",
    "    \n",
    "    original_data,_ = next(iter(data_loader))\n",
    "    original_data = Variable(original_data)\n",
    "    original_data = original_data.float()\n",
    "    if CUDA:\n",
    "        original_data = original_data.cuda()\n",
    "        \n",
    "    binary_mu_out, normal_mu_out, mu_latent, logvar_latent = model(original_data)\n",
    "    \n",
    "    return model, original_data, binary_mu_out, normal_mu_out, mu_latent, logvar_latent\n",
    "\n",
    "def encode_data(model, dataset):\n",
    "    all_data = torch.from_numpy(dataset.data)\n",
    "    all_data = Variable(all_data)\n",
    "    all_data = all_data.float()\n",
    "    \n",
    "    if CUDA:\n",
    "        all_data = all_data.cuda()\n",
    "\n",
    "    latent_mu, latent_var = model.encode(all_data)\n",
    "    \n",
    "    if CUDA:\n",
    "        latent_mu = latent_mu.cpu()\n",
    "        latent_var = latent_var.cpu()\n",
    "        \n",
    "    data = np.hstack([latent_mu.data.numpy(), latent_var.data.numpy()])\n",
    "    dataset.save_processed_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1000 Average loss: 0.00232590\n",
      "====> Epoch: 2000 Average loss: 0.00206668\n",
      "====> Epoch: 3000 Average loss: 0.00194457\n",
      "====> Epoch: 4000 Average loss: 0.00180364\n",
      "====> Epoch: 5000 Average loss: 0.00171928\n",
      "====> Epoch: 6000 Average loss: 0.00166028\n",
      "====> Epoch: 7000 Average loss: 0.00165625\n",
      "====> Epoch: 8000 Average loss: 0.00158786\n",
      "====> Epoch: 9000 Average loss: 0.00156656\n",
      "====> Epoch: 10000 Average loss: 0.00155415\n",
      "Training state:  False\n"
     ]
    }
   ],
   "source": [
    "dataset = CovariateDataset(\"n_{}_model_{}_v_{}_covar_data\", [1000, \"A_add_lin\", 92])\n",
    "trained_model, original_data, binary_mu_out, normal_mu_out, mu_latent, logvar_latent = \\\n",
    "    train_model(ModifiedVAE, dataset, 1,verbose=True)\n",
    "\n",
    "encode_data(trained_model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal: [1.0, 1.0, 1.0, -1.32, 0.02, 1.0, -0.35, 0.0, 1.0, -0.93]\n",
      "Mu out: [1.0, 1.15, 1.0, -1.01, 0.0, 0.98, -0.18, 0.12, 1.0, -1.18]\n",
      "Mu Latent: [1.1, -0.14, 1.1, -0.44]\n",
      "Std latent: [0.06, 0.03, 0.06, 0.07]\n",
      "\n",
      "Orginal: [1.0, -1.1, 0.0, -0.12, -0.33, 0.0, -2.12, 0.0, 0.0, -1.77]\n",
      "Mu out: [0.36, -1.32, 0.02, 0.53, 0.0, 0.01, -2.24, 0.0, 0.0, -1.25]\n",
      "Mu Latent: [-1.4, -0.21, 1.64, 1.06]\n",
      "Std latent: [0.09, 0.03, 0.07, 0.1]\n",
      "\n",
      "Orginal: [0.0, -2.11, 0.0, 0.21, 0.58, 0.0, 0.38, 0.0, 0.0, 1.91]\n",
      "Mu out: [0.02, -1.66, 0.01, 0.41, 50774.44, 0.08, 0.46, 0.0, 0.0, 1.06]\n",
      "Mu Latent: [-1.34, 0.32, 0.84, 1.66]\n",
      "Std latent: [0.06, 0.03, 0.04, 0.07]\n",
      "\n",
      "Orginal: [1.0, 1.29, 0.0, 0.74, 0.57, 1.0, -1.03, 1.0, 0.0, -1.44]\n",
      "Mu out: [1.0, 1.09, 0.0, 0.78, 0.0, 0.97, -0.86, 1.0, 0.0, -1.25]\n",
      "Mu Latent: [0.34, -2.01, 0.03, -1.48]\n",
      "Std latent: [0.05, 0.19, 0.03, 0.16]\n",
      "\n",
      "Orginal: [1.0, -1.29, 1.0, 0.23, 1.92, 1.0, 1.66, 0.0, 1.0, 1.65]\n",
      "Mu out: [0.98, -1.53, 1.0, 0.19, 0.0, 0.97, 1.58, 0.0, 1.0, 1.81]\n",
      "Mu Latent: [-1.46, 0.14, -0.42, -0.0]\n",
      "Std latent: [0.05, 0.04, 0.05, 0.04]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mu_out = torch.Tensor(1000, 10)\n",
    "\n",
    "for index in BINARY:\n",
    "    mu_out[:, index] = binary_mu_out[:, BINARY.index(index)].data.cpu()\n",
    "    \n",
    "for index in NORMAL:\n",
    "    mu_out[:, index] = normal_mu_out[:, NORMAL.index(index)].data.cpu()\n",
    "    \n",
    "for i in np.random.choice(list(range(1000)), size=5, ):\n",
    "    print(\"Orginal:\", list(np.round(original_data[i].data.cpu().numpy(), 2)))\n",
    "    print(\"Mu out:\", list(np.round(mu_out[i].numpy(), 2)))\n",
    "    print(\"Mu Latent:\", list(np.round(mu_latent[i].data.cpu().numpy(), 2)))\n",
    "    print(\"Std latent:\", list(np.round(logvar_latent[i].mul(0.5).exp().data.cpu().numpy(), 2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run for Dataset 90\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00268768\n",
      "====> Epoch: 2000 Average loss: 0.00237313\n",
      "====> Epoch: 3000 Average loss: 0.00224286\n",
      "====> Epoch: 4000 Average loss: 0.00217269\n",
      "====> Epoch: 5000 Average loss: 0.00208389\n",
      "====> Epoch: 6000 Average loss: 0.00200424\n",
      "====> Epoch: 7000 Average loss: 0.00197165\n",
      "====> Epoch: 8000 Average loss: 0.00194840\n",
      "====> Epoch: 9000 Average loss: 0.00185654\n",
      "====> Epoch: 10000 Average loss: 0.00185729\n",
      "Training state:  False\n",
      "---- Done in  84.95870208740234  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00287928\n",
      "====> Epoch: 2000 Average loss: 0.00241111\n",
      "====> Epoch: 3000 Average loss: 0.00213923\n",
      "====> Epoch: 4000 Average loss: 0.00215906\n",
      "====> Epoch: 5000 Average loss: 0.00196259\n",
      "====> Epoch: 6000 Average loss: 0.00191134\n",
      "====> Epoch: 7000 Average loss: 0.00185074\n",
      "====> Epoch: 8000 Average loss: 0.00185662\n",
      "====> Epoch: 9000 Average loss: 0.00182047\n",
      "====> Epoch: 10000 Average loss: 0.00186788\n",
      "Training state:  False\n",
      "---- Done in  82.72416687011719  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00308135\n",
      "====> Epoch: 2000 Average loss: 0.00275319\n",
      "====> Epoch: 3000 Average loss: 0.00257373\n",
      "====> Epoch: 4000 Average loss: 0.00262785\n",
      "====> Epoch: 5000 Average loss: 0.00233751\n",
      "====> Epoch: 6000 Average loss: 0.00230588\n",
      "====> Epoch: 7000 Average loss: 0.00230868\n",
      "====> Epoch: 8000 Average loss: 0.00227018\n",
      "====> Epoch: 9000 Average loss: 0.00225592\n",
      "====> Epoch: 10000 Average loss: 0.00220673\n",
      "Training state:  False\n",
      "---- Done in  83.96097993850708  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00285985\n",
      "====> Epoch: 2000 Average loss: 0.00218383\n",
      "====> Epoch: 3000 Average loss: 0.00207368\n",
      "====> Epoch: 4000 Average loss: 0.00200355\n",
      "====> Epoch: 5000 Average loss: 0.00192303\n",
      "====> Epoch: 6000 Average loss: 0.00190600\n",
      "====> Epoch: 7000 Average loss: 0.00188649\n",
      "====> Epoch: 8000 Average loss: 0.00186674\n",
      "====> Epoch: 9000 Average loss: 0.00185738\n",
      "====> Epoch: 10000 Average loss: 0.00183367\n",
      "Training state:  False\n",
      "---- Done in  83.41443800926208  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00312728\n",
      "====> Epoch: 2000 Average loss: 0.00277702\n",
      "====> Epoch: 3000 Average loss: 0.00262714\n",
      "====> Epoch: 4000 Average loss: 0.00253980\n",
      "====> Epoch: 5000 Average loss: 0.00246542\n",
      "====> Epoch: 6000 Average loss: 0.00237524\n",
      "====> Epoch: 7000 Average loss: 0.00231881\n",
      "====> Epoch: 8000 Average loss: 0.00232344\n",
      "====> Epoch: 9000 Average loss: 0.00232872\n",
      "====> Epoch: 10000 Average loss: 0.00227015\n",
      "Training state:  False\n",
      "---- Done in  84.29766011238098  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00268200\n",
      "====> Epoch: 2000 Average loss: 0.00206906\n",
      "====> Epoch: 3000 Average loss: 0.00176848\n",
      "====> Epoch: 4000 Average loss: 0.00166931\n",
      "====> Epoch: 5000 Average loss: 0.00161120\n",
      "====> Epoch: 6000 Average loss: 0.00160374\n",
      "====> Epoch: 7000 Average loss: 0.00159742\n",
      "====> Epoch: 8000 Average loss: 0.00160923\n",
      "====> Epoch: 9000 Average loss: 0.00156979\n",
      "====> Epoch: 10000 Average loss: 0.00158413\n",
      "Training state:  False\n",
      "---- Done in  85.69637894630432  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00284781\n",
      "====> Epoch: 2000 Average loss: 0.00250718\n",
      "====> Epoch: 3000 Average loss: 0.00229198\n",
      "====> Epoch: 4000 Average loss: 0.00214665\n",
      "====> Epoch: 5000 Average loss: 0.00206138\n",
      "====> Epoch: 6000 Average loss: 0.00201588\n",
      "====> Epoch: 7000 Average loss: 0.00192481\n",
      "====> Epoch: 8000 Average loss: 0.00192973\n",
      "====> Epoch: 9000 Average loss: 0.00194713\n",
      "====> Epoch: 10000 Average loss: 0.00193677\n",
      "Training state:  False\n",
      "---- Done in  85.49331212043762  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 91\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00300385\n",
      "====> Epoch: 2000 Average loss: 0.00248388\n",
      "====> Epoch: 3000 Average loss: 0.00221511\n",
      "====> Epoch: 4000 Average loss: 0.00204721\n",
      "====> Epoch: 5000 Average loss: 0.00198145\n",
      "====> Epoch: 6000 Average loss: 0.00198101\n",
      "====> Epoch: 7000 Average loss: 0.00191016\n",
      "====> Epoch: 8000 Average loss: 0.00192343\n",
      "====> Epoch: 9000 Average loss: 0.00199205\n",
      "====> Epoch: 10000 Average loss: 0.00194342\n",
      "Training state:  False\n",
      "---- Done in  86.4841411113739  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00301937\n",
      "====> Epoch: 2000 Average loss: 0.00270866\n",
      "====> Epoch: 3000 Average loss: 0.00263463\n",
      "====> Epoch: 4000 Average loss: 0.00257478\n",
      "====> Epoch: 5000 Average loss: 0.00256522\n",
      "====> Epoch: 6000 Average loss: 0.00251580\n",
      "====> Epoch: 7000 Average loss: 0.00244934\n",
      "====> Epoch: 8000 Average loss: 0.00225821\n",
      "====> Epoch: 9000 Average loss: 0.00215826\n",
      "====> Epoch: 10000 Average loss: 0.00210668\n",
      "Training state:  False\n",
      "---- Done in  84.74725914001465  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00314180\n",
      "====> Epoch: 2000 Average loss: 0.00276592\n",
      "====> Epoch: 3000 Average loss: 0.00254082\n",
      "====> Epoch: 4000 Average loss: 0.00227517\n",
      "====> Epoch: 5000 Average loss: 0.00217302\n",
      "====> Epoch: 6000 Average loss: 0.00210094\n",
      "====> Epoch: 7000 Average loss: 0.00205824\n",
      "====> Epoch: 8000 Average loss: 0.00202212\n",
      "====> Epoch: 9000 Average loss: 0.00198173\n",
      "====> Epoch: 10000 Average loss: 0.00195171\n",
      "Training state:  False\n",
      "---- Done in  84.78400993347168  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00315134\n",
      "====> Epoch: 2000 Average loss: 0.00289525\n",
      "====> Epoch: 3000 Average loss: 0.00268520\n",
      "====> Epoch: 4000 Average loss: 0.00248732\n",
      "====> Epoch: 5000 Average loss: 0.00234438\n",
      "====> Epoch: 6000 Average loss: 0.00228041\n",
      "====> Epoch: 7000 Average loss: 0.00227541\n",
      "====> Epoch: 8000 Average loss: 0.00225058\n",
      "====> Epoch: 9000 Average loss: 0.00221486\n",
      "====> Epoch: 10000 Average loss: 0.00217799\n",
      "Training state:  False\n",
      "---- Done in  84.1204936504364  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00305827\n",
      "====> Epoch: 2000 Average loss: 0.00276294\n",
      "====> Epoch: 3000 Average loss: 0.00268740\n",
      "====> Epoch: 4000 Average loss: 0.00261828\n",
      "====> Epoch: 5000 Average loss: 0.00255322\n",
      "====> Epoch: 6000 Average loss: 0.00255647\n",
      "====> Epoch: 7000 Average loss: 0.00249986\n",
      "====> Epoch: 8000 Average loss: 0.00251776\n",
      "====> Epoch: 9000 Average loss: 0.00252773\n",
      "====> Epoch: 10000 Average loss: 0.00247379\n",
      "Training state:  False\n",
      "---- Done in  85.09275007247925  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00281374\n",
      "====> Epoch: 2000 Average loss: 0.00255727\n",
      "====> Epoch: 3000 Average loss: 0.00242623\n",
      "====> Epoch: 4000 Average loss: 0.00237114\n",
      "====> Epoch: 5000 Average loss: 0.00233401\n",
      "====> Epoch: 6000 Average loss: 0.00227741\n",
      "====> Epoch: 7000 Average loss: 0.00227803\n",
      "====> Epoch: 8000 Average loss: 0.00225788\n",
      "====> Epoch: 9000 Average loss: 0.00224440\n",
      "====> Epoch: 10000 Average loss: 0.00223277\n",
      "Training state:  False\n",
      "---- Done in  85.4829728603363  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00253027\n",
      "====> Epoch: 2000 Average loss: 0.00201017\n",
      "====> Epoch: 3000 Average loss: 0.00171787\n",
      "====> Epoch: 4000 Average loss: 0.00159854\n",
      "====> Epoch: 5000 Average loss: 0.00158892\n",
      "====> Epoch: 6000 Average loss: 0.00154581\n",
      "====> Epoch: 7000 Average loss: 0.00155053\n",
      "====> Epoch: 8000 Average loss: 0.00153724\n",
      "====> Epoch: 9000 Average loss: 0.00150832\n",
      "====> Epoch: 10000 Average loss: 0.00149547\n",
      "Training state:  False\n",
      "---- Done in  84.70116019248962  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 92\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00269248\n",
      "====> Epoch: 2000 Average loss: 0.00231682\n",
      "====> Epoch: 3000 Average loss: 0.00214793\n",
      "====> Epoch: 4000 Average loss: 0.00208234\n",
      "====> Epoch: 5000 Average loss: 0.00201720\n",
      "====> Epoch: 6000 Average loss: 0.00198676\n",
      "====> Epoch: 7000 Average loss: 0.00195351\n",
      "====> Epoch: 8000 Average loss: 0.00193456\n",
      "====> Epoch: 9000 Average loss: 0.00184022\n",
      "====> Epoch: 10000 Average loss: 0.00184000\n",
      "Training state:  False\n",
      "---- Done in  84.71853399276733  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1000 Average loss: 0.00284964\n",
      "====> Epoch: 2000 Average loss: 0.00255731\n",
      "====> Epoch: 3000 Average loss: 0.00242743\n",
      "====> Epoch: 4000 Average loss: 0.00237298\n",
      "====> Epoch: 5000 Average loss: 0.00233932\n",
      "====> Epoch: 6000 Average loss: 0.00232236\n",
      "====> Epoch: 7000 Average loss: 0.00225559\n",
      "====> Epoch: 8000 Average loss: 0.00225023\n",
      "====> Epoch: 9000 Average loss: 0.00224519\n",
      "====> Epoch: 10000 Average loss: 0.00221983\n",
      "Training state:  False\n",
      "---- Done in  84.03732299804688  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00272160\n",
      "====> Epoch: 2000 Average loss: 0.00235737\n",
      "====> Epoch: 3000 Average loss: 0.00233571\n",
      "====> Epoch: 4000 Average loss: 0.00221473\n",
      "====> Epoch: 5000 Average loss: 0.00226114\n",
      "====> Epoch: 6000 Average loss: 0.00216919\n",
      "====> Epoch: 7000 Average loss: 0.00211925\n",
      "====> Epoch: 8000 Average loss: 0.00205293\n",
      "====> Epoch: 9000 Average loss: 0.00198881\n",
      "====> Epoch: 10000 Average loss: 0.00197461\n",
      "Training state:  False\n",
      "---- Done in  84.40620303153992  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00228161\n",
      "====> Epoch: 2000 Average loss: 0.00206574\n",
      "====> Epoch: 3000 Average loss: 0.00193017\n",
      "====> Epoch: 4000 Average loss: 0.00185030\n",
      "====> Epoch: 5000 Average loss: 0.00175998\n",
      "====> Epoch: 6000 Average loss: 0.00169887\n",
      "====> Epoch: 7000 Average loss: 0.00163047\n",
      "====> Epoch: 8000 Average loss: 0.00168461\n",
      "====> Epoch: 9000 Average loss: 0.00161832\n",
      "====> Epoch: 10000 Average loss: 0.00158678\n",
      "Training state:  False\n",
      "---- Done in  85.73783016204834  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00253641\n",
      "====> Epoch: 2000 Average loss: 0.00227023\n",
      "====> Epoch: 3000 Average loss: 0.00220682\n",
      "====> Epoch: 4000 Average loss: 0.00214008\n",
      "====> Epoch: 5000 Average loss: 0.00211216\n",
      "====> Epoch: 6000 Average loss: 0.00212430\n",
      "====> Epoch: 7000 Average loss: 0.00209022\n",
      "====> Epoch: 8000 Average loss: 0.00206942\n",
      "====> Epoch: 9000 Average loss: 0.00210609\n",
      "====> Epoch: 10000 Average loss: 0.00204976\n",
      "Training state:  False\n",
      "---- Done in  84.70711493492126  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00335619\n",
      "====> Epoch: 2000 Average loss: 0.00299815\n",
      "====> Epoch: 3000 Average loss: 0.00288668\n",
      "====> Epoch: 4000 Average loss: 0.00281740\n",
      "====> Epoch: 5000 Average loss: 0.00278942\n",
      "====> Epoch: 6000 Average loss: 0.00276693\n",
      "====> Epoch: 7000 Average loss: 0.00260003\n",
      "====> Epoch: 8000 Average loss: 0.00255976\n",
      "====> Epoch: 9000 Average loss: 0.00251381\n",
      "====> Epoch: 10000 Average loss: 0.00252299\n",
      "Training state:  False\n",
      "---- Done in  86.2931182384491  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00258785\n",
      "====> Epoch: 2000 Average loss: 0.00227245\n",
      "====> Epoch: 3000 Average loss: 0.00211474\n",
      "====> Epoch: 4000 Average loss: 0.00203214\n",
      "====> Epoch: 5000 Average loss: 0.00199222\n",
      "====> Epoch: 6000 Average loss: 0.00197331\n",
      "====> Epoch: 7000 Average loss: 0.00190974\n",
      "====> Epoch: 8000 Average loss: 0.00177210\n",
      "====> Epoch: 9000 Average loss: 0.00174310\n",
      "====> Epoch: 10000 Average loss: 0.00172087\n",
      "Training state:  False\n",
      "---- Done in  87.82305788993835  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 93\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00241509\n",
      "====> Epoch: 2000 Average loss: 0.00198066\n",
      "====> Epoch: 3000 Average loss: 0.00189634\n",
      "====> Epoch: 4000 Average loss: 0.00186786\n",
      "====> Epoch: 5000 Average loss: 0.00185510\n",
      "====> Epoch: 6000 Average loss: 0.00180494\n",
      "====> Epoch: 7000 Average loss: 0.00180080\n",
      "====> Epoch: 8000 Average loss: 0.00177660\n",
      "====> Epoch: 9000 Average loss: 0.00176619\n",
      "====> Epoch: 10000 Average loss: 0.00179133\n",
      "Training state:  False\n",
      "---- Done in  85.92957711219788  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00293057\n",
      "====> Epoch: 2000 Average loss: 0.00243845\n",
      "====> Epoch: 3000 Average loss: 0.00221675\n",
      "====> Epoch: 4000 Average loss: 0.00217267\n",
      "====> Epoch: 5000 Average loss: 0.00220660\n",
      "====> Epoch: 6000 Average loss: 0.00211687\n",
      "====> Epoch: 7000 Average loss: 0.00209077\n",
      "====> Epoch: 8000 Average loss: 0.00207962\n",
      "====> Epoch: 9000 Average loss: 0.00205167\n",
      "====> Epoch: 10000 Average loss: 0.00202816\n",
      "Training state:  False\n",
      "---- Done in  86.26453614234924  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00299316\n",
      "====> Epoch: 2000 Average loss: 0.00260510\n",
      "====> Epoch: 3000 Average loss: 0.00229668\n",
      "====> Epoch: 4000 Average loss: 0.00207701\n",
      "====> Epoch: 5000 Average loss: 0.00195822\n",
      "====> Epoch: 6000 Average loss: 0.00185358\n",
      "====> Epoch: 7000 Average loss: 0.00183882\n",
      "====> Epoch: 8000 Average loss: 0.00176884\n",
      "====> Epoch: 9000 Average loss: 0.00180415\n",
      "====> Epoch: 10000 Average loss: 0.00178910\n",
      "Training state:  False\n",
      "---- Done in  87.09334993362427  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00296817\n",
      "====> Epoch: 2000 Average loss: 0.00269417\n",
      "====> Epoch: 3000 Average loss: 0.00255633\n",
      "====> Epoch: 4000 Average loss: 0.00244268\n",
      "====> Epoch: 5000 Average loss: 0.00239488\n",
      "====> Epoch: 6000 Average loss: 0.00234605\n",
      "====> Epoch: 7000 Average loss: 0.00232241\n",
      "====> Epoch: 8000 Average loss: 0.00228301\n",
      "====> Epoch: 9000 Average loss: 0.00228046\n",
      "====> Epoch: 10000 Average loss: 0.00226970\n",
      "Training state:  False\n",
      "---- Done in  87.5416510105133  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00293021\n",
      "====> Epoch: 2000 Average loss: 0.00257351\n",
      "====> Epoch: 3000 Average loss: 0.00234486\n",
      "====> Epoch: 4000 Average loss: 0.00216332\n",
      "====> Epoch: 5000 Average loss: 0.00208973\n",
      "====> Epoch: 6000 Average loss: 0.00204196\n",
      "====> Epoch: 7000 Average loss: 0.00197871\n",
      "====> Epoch: 8000 Average loss: 0.00190117\n",
      "====> Epoch: 9000 Average loss: 0.00188790\n",
      "====> Epoch: 10000 Average loss: 0.00187680\n",
      "Training state:  False\n",
      "---- Done in  89.33804416656494  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00283959\n",
      "====> Epoch: 2000 Average loss: 0.00234237\n",
      "====> Epoch: 3000 Average loss: 0.00204978\n",
      "====> Epoch: 4000 Average loss: 0.00191120\n",
      "====> Epoch: 5000 Average loss: 0.00187595\n",
      "====> Epoch: 6000 Average loss: 0.00187367\n",
      "====> Epoch: 7000 Average loss: 0.00177712\n",
      "====> Epoch: 8000 Average loss: 0.00176543\n",
      "====> Epoch: 9000 Average loss: 0.00171524\n",
      "====> Epoch: 10000 Average loss: 0.00173417\n",
      "Training state:  False\n",
      "---- Done in  88.40667581558228  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00266188\n",
      "====> Epoch: 2000 Average loss: 0.00229372\n",
      "====> Epoch: 3000 Average loss: 0.00216714\n",
      "====> Epoch: 4000 Average loss: 0.00203758\n",
      "====> Epoch: 5000 Average loss: 0.00199635\n",
      "====> Epoch: 6000 Average loss: 0.00197589\n",
      "====> Epoch: 7000 Average loss: 0.00192103\n",
      "====> Epoch: 8000 Average loss: 0.00195405\n",
      "====> Epoch: 9000 Average loss: 0.00185344\n",
      "====> Epoch: 10000 Average loss: 0.00181960\n",
      "Training state:  False\n",
      "---- Done in  96.7903139591217  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 94\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00315951\n",
      "====> Epoch: 2000 Average loss: 0.00275763\n",
      "====> Epoch: 3000 Average loss: 0.00238977\n",
      "====> Epoch: 4000 Average loss: 0.00227343\n",
      "====> Epoch: 5000 Average loss: 0.00216393\n",
      "====> Epoch: 6000 Average loss: 0.00209797\n",
      "====> Epoch: 7000 Average loss: 0.00205893\n",
      "====> Epoch: 8000 Average loss: 0.00200784\n",
      "====> Epoch: 9000 Average loss: 0.00202733\n",
      "====> Epoch: 10000 Average loss: 0.00198650\n",
      "Training state:  False\n",
      "---- Done in  94.0506010055542  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00290862\n",
      "====> Epoch: 2000 Average loss: 0.00267193\n",
      "====> Epoch: 3000 Average loss: 0.00253168\n",
      "====> Epoch: 4000 Average loss: 0.00250772\n",
      "====> Epoch: 5000 Average loss: 0.00232742\n",
      "====> Epoch: 6000 Average loss: 0.00229582\n",
      "====> Epoch: 7000 Average loss: 0.00224571\n",
      "====> Epoch: 8000 Average loss: 0.00218750\n",
      "====> Epoch: 9000 Average loss: 0.00217559\n",
      "====> Epoch: 10000 Average loss: 0.00211737\n",
      "Training state:  False\n",
      "---- Done in  91.87096071243286  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1000 Average loss: 0.00317736\n",
      "====> Epoch: 2000 Average loss: 0.00298828\n",
      "====> Epoch: 3000 Average loss: 0.00294221\n",
      "====> Epoch: 4000 Average loss: 0.00291144\n",
      "====> Epoch: 5000 Average loss: 0.00288035\n",
      "====> Epoch: 6000 Average loss: 0.00285088\n",
      "====> Epoch: 7000 Average loss: 0.00281545\n",
      "====> Epoch: 8000 Average loss: 0.00285385\n",
      "====> Epoch: 9000 Average loss: 0.00280671\n",
      "====> Epoch: 10000 Average loss: 0.00279417\n",
      "Training state:  False\n",
      "---- Done in  93.21560001373291  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00285003\n",
      "====> Epoch: 2000 Average loss: 0.00242106\n",
      "====> Epoch: 3000 Average loss: 0.00230542\n",
      "====> Epoch: 4000 Average loss: 0.00213527\n",
      "====> Epoch: 5000 Average loss: 0.00204583\n",
      "====> Epoch: 6000 Average loss: 0.00200030\n",
      "====> Epoch: 7000 Average loss: 0.00194190\n",
      "====> Epoch: 8000 Average loss: 0.00189005\n",
      "====> Epoch: 9000 Average loss: 0.00185654\n",
      "====> Epoch: 10000 Average loss: 0.00184931\n",
      "Training state:  False\n",
      "---- Done in  90.97170996665955  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00315657\n",
      "====> Epoch: 2000 Average loss: 0.00276550\n",
      "====> Epoch: 3000 Average loss: 0.00238038\n",
      "====> Epoch: 4000 Average loss: 0.00223225\n",
      "====> Epoch: 5000 Average loss: 0.00214459\n",
      "====> Epoch: 6000 Average loss: 0.00211188\n",
      "====> Epoch: 7000 Average loss: 0.00204864\n",
      "====> Epoch: 8000 Average loss: 0.00197280\n",
      "====> Epoch: 9000 Average loss: 0.00195182\n",
      "====> Epoch: 10000 Average loss: 0.00193599\n",
      "Training state:  False\n",
      "---- Done in  90.97623705863953  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00287582\n",
      "====> Epoch: 2000 Average loss: 0.00235072\n",
      "====> Epoch: 3000 Average loss: 0.00223349\n",
      "====> Epoch: 4000 Average loss: 0.00215549\n",
      "====> Epoch: 5000 Average loss: 0.00213477\n",
      "====> Epoch: 6000 Average loss: 0.00211807\n",
      "====> Epoch: 7000 Average loss: 0.00206496\n",
      "====> Epoch: 8000 Average loss: 0.00200512\n",
      "====> Epoch: 9000 Average loss: 0.00204089\n",
      "====> Epoch: 10000 Average loss: 0.00199241\n",
      "Training state:  False\n",
      "---- Done in  91.67234086990356  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00329205\n",
      "====> Epoch: 2000 Average loss: 0.00257297\n",
      "====> Epoch: 3000 Average loss: 0.00232967\n",
      "====> Epoch: 4000 Average loss: 0.00228283\n",
      "====> Epoch: 5000 Average loss: 0.00227274\n",
      "====> Epoch: 6000 Average loss: 0.00222408\n",
      "====> Epoch: 7000 Average loss: 0.00221632\n",
      "====> Epoch: 8000 Average loss: 0.00220821\n",
      "====> Epoch: 9000 Average loss: 0.00217706\n",
      "====> Epoch: 10000 Average loss: 0.00218745\n",
      "Training state:  False\n",
      "---- Done in  90.53277802467346  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 95\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00273242\n",
      "====> Epoch: 2000 Average loss: 0.00244008\n",
      "====> Epoch: 3000 Average loss: 0.00233228\n",
      "====> Epoch: 4000 Average loss: 0.00229687\n",
      "====> Epoch: 5000 Average loss: 0.00223165\n",
      "====> Epoch: 6000 Average loss: 0.00221477\n",
      "====> Epoch: 7000 Average loss: 0.00219114\n",
      "====> Epoch: 8000 Average loss: 0.00211715\n",
      "====> Epoch: 9000 Average loss: 0.00213101\n",
      "====> Epoch: 10000 Average loss: 0.00210095\n",
      "Training state:  False\n",
      "---- Done in  89.01382875442505  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00309736\n",
      "====> Epoch: 2000 Average loss: 0.00286079\n",
      "====> Epoch: 3000 Average loss: 0.00277473\n",
      "====> Epoch: 4000 Average loss: 0.00266064\n",
      "====> Epoch: 5000 Average loss: 0.00252330\n",
      "====> Epoch: 6000 Average loss: 0.00249127\n",
      "====> Epoch: 7000 Average loss: 0.00244314\n",
      "====> Epoch: 8000 Average loss: 0.00233948\n",
      "====> Epoch: 9000 Average loss: 0.00229673\n",
      "====> Epoch: 10000 Average loss: 0.00229180\n",
      "Training state:  False\n",
      "---- Done in  95.2107720375061  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00338684\n",
      "====> Epoch: 2000 Average loss: 0.00284562\n",
      "====> Epoch: 3000 Average loss: 0.00263964\n",
      "====> Epoch: 4000 Average loss: 0.00250184\n",
      "====> Epoch: 5000 Average loss: 0.00238939\n",
      "====> Epoch: 6000 Average loss: 0.00240871\n",
      "====> Epoch: 7000 Average loss: 0.00226186\n",
      "====> Epoch: 8000 Average loss: 0.00221403\n",
      "====> Epoch: 9000 Average loss: 0.00217368\n",
      "====> Epoch: 10000 Average loss: 0.00212129\n",
      "Training state:  False\n",
      "---- Done in  89.20070791244507  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00297088\n",
      "====> Epoch: 2000 Average loss: 0.00240934\n",
      "====> Epoch: 3000 Average loss: 0.00230139\n",
      "====> Epoch: 4000 Average loss: 0.00225910\n",
      "====> Epoch: 5000 Average loss: 0.00221995\n",
      "====> Epoch: 6000 Average loss: 0.00221328\n",
      "====> Epoch: 7000 Average loss: 0.00210232\n",
      "====> Epoch: 8000 Average loss: 0.00200698\n",
      "====> Epoch: 9000 Average loss: 0.00197502\n",
      "====> Epoch: 10000 Average loss: 0.00197773\n",
      "Training state:  False\n",
      "---- Done in  89.77836179733276  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00298138\n",
      "====> Epoch: 2000 Average loss: 0.00228710\n",
      "====> Epoch: 3000 Average loss: 0.00219081\n",
      "====> Epoch: 4000 Average loss: 0.00204054\n",
      "====> Epoch: 5000 Average loss: 0.00201648\n",
      "====> Epoch: 6000 Average loss: 0.00195836\n",
      "====> Epoch: 7000 Average loss: 0.00197467\n",
      "====> Epoch: 8000 Average loss: 0.00197072\n",
      "====> Epoch: 9000 Average loss: 0.00199943\n",
      "====> Epoch: 10000 Average loss: 0.00190665\n",
      "Training state:  False\n",
      "---- Done in  95.50839328765869  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00303405\n",
      "====> Epoch: 2000 Average loss: 0.00255907\n",
      "====> Epoch: 3000 Average loss: 0.00232972\n",
      "====> Epoch: 4000 Average loss: 0.00221617\n",
      "====> Epoch: 5000 Average loss: 0.00214577\n",
      "====> Epoch: 6000 Average loss: 0.00216069\n",
      "====> Epoch: 7000 Average loss: 0.00214974\n",
      "====> Epoch: 8000 Average loss: 0.00210771\n",
      "====> Epoch: 9000 Average loss: 0.00208500\n",
      "====> Epoch: 10000 Average loss: 0.00201006\n",
      "Training state:  False\n",
      "---- Done in  89.16998481750488  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00306082\n",
      "====> Epoch: 2000 Average loss: 0.00262866\n",
      "====> Epoch: 3000 Average loss: 0.00250473\n",
      "====> Epoch: 4000 Average loss: 0.00239747\n",
      "====> Epoch: 5000 Average loss: 0.00235913\n",
      "====> Epoch: 6000 Average loss: 0.00229163\n",
      "====> Epoch: 7000 Average loss: 0.00225232\n",
      "====> Epoch: 8000 Average loss: 0.00220289\n",
      "====> Epoch: 9000 Average loss: 0.00217514\n",
      "====> Epoch: 10000 Average loss: 0.00217819\n",
      "Training state:  False\n",
      "---- Done in  90.21903729438782  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 96\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00303954\n",
      "====> Epoch: 2000 Average loss: 0.00268203\n",
      "====> Epoch: 3000 Average loss: 0.00249031\n",
      "====> Epoch: 4000 Average loss: 0.00236504\n",
      "====> Epoch: 5000 Average loss: 0.00225561\n",
      "====> Epoch: 6000 Average loss: 0.00217029\n",
      "====> Epoch: 7000 Average loss: 0.00215143\n",
      "====> Epoch: 8000 Average loss: 0.00214274\n",
      "====> Epoch: 9000 Average loss: 0.00211525\n",
      "====> Epoch: 10000 Average loss: 0.00205963\n",
      "Training state:  False\n",
      "---- Done in  90.2923572063446  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00244765\n",
      "====> Epoch: 2000 Average loss: 0.00221634\n",
      "====> Epoch: 3000 Average loss: 0.00215232\n",
      "====> Epoch: 4000 Average loss: 0.00210143\n",
      "====> Epoch: 5000 Average loss: 0.00202992\n",
      "====> Epoch: 6000 Average loss: 0.00198027\n",
      "====> Epoch: 7000 Average loss: 0.00199446\n",
      "====> Epoch: 8000 Average loss: 0.00196369\n",
      "====> Epoch: 9000 Average loss: 0.00196046\n",
      "====> Epoch: 10000 Average loss: 0.00194832\n",
      "Training state:  False\n",
      "---- Done in  91.18650817871094  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00313393\n",
      "====> Epoch: 2000 Average loss: 0.00275967\n",
      "====> Epoch: 3000 Average loss: 0.00251464\n",
      "====> Epoch: 4000 Average loss: 0.00239393\n",
      "====> Epoch: 5000 Average loss: 0.00232454\n",
      "====> Epoch: 6000 Average loss: 0.00229894\n",
      "====> Epoch: 7000 Average loss: 0.00224030\n",
      "====> Epoch: 8000 Average loss: 0.00223209\n",
      "====> Epoch: 9000 Average loss: 0.00228718\n",
      "====> Epoch: 10000 Average loss: 0.00223925\n",
      "Training state:  False\n",
      "---- Done in  89.89194416999817  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1000 Average loss: 0.00292306\n",
      "====> Epoch: 2000 Average loss: 0.00272235\n",
      "====> Epoch: 3000 Average loss: 0.00264855\n",
      "====> Epoch: 4000 Average loss: 0.00253912\n",
      "====> Epoch: 5000 Average loss: 0.00247762\n",
      "====> Epoch: 6000 Average loss: 0.00251480\n",
      "====> Epoch: 7000 Average loss: 0.00242523\n",
      "====> Epoch: 8000 Average loss: 0.00239897\n",
      "====> Epoch: 9000 Average loss: 0.00238141\n",
      "====> Epoch: 10000 Average loss: 0.00233883\n",
      "Training state:  False\n",
      "---- Done in  95.20706605911255  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00322684\n",
      "====> Epoch: 2000 Average loss: 0.00277094\n",
      "====> Epoch: 3000 Average loss: 0.00254486\n",
      "====> Epoch: 4000 Average loss: 0.00240701\n",
      "====> Epoch: 5000 Average loss: 0.00235318\n",
      "====> Epoch: 6000 Average loss: 0.00231776\n",
      "====> Epoch: 7000 Average loss: 0.00221853\n",
      "====> Epoch: 8000 Average loss: 0.00218643\n",
      "====> Epoch: 9000 Average loss: 0.00218974\n",
      "====> Epoch: 10000 Average loss: 0.00214288\n",
      "Training state:  False\n",
      "---- Done in  90.35397219657898  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00307324\n",
      "====> Epoch: 2000 Average loss: 0.00243564\n",
      "====> Epoch: 3000 Average loss: 0.00217653\n",
      "====> Epoch: 4000 Average loss: 0.00206228\n",
      "====> Epoch: 5000 Average loss: 0.00185799\n",
      "====> Epoch: 6000 Average loss: 0.00178318\n",
      "====> Epoch: 7000 Average loss: 0.00173146\n",
      "====> Epoch: 8000 Average loss: 0.00173983\n",
      "====> Epoch: 9000 Average loss: 0.00165354\n",
      "====> Epoch: 10000 Average loss: 0.00162844\n",
      "Training state:  False\n",
      "---- Done in  91.39981484413147  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00284881\n",
      "====> Epoch: 2000 Average loss: 0.00245429\n",
      "====> Epoch: 3000 Average loss: 0.00221698\n",
      "====> Epoch: 4000 Average loss: 0.00214734\n",
      "====> Epoch: 5000 Average loss: 0.00199372\n",
      "====> Epoch: 6000 Average loss: 0.00201745\n",
      "====> Epoch: 7000 Average loss: 0.00197158\n",
      "====> Epoch: 8000 Average loss: 0.00186209\n",
      "====> Epoch: 9000 Average loss: 0.00184459\n",
      "====> Epoch: 10000 Average loss: 0.00181096\n",
      "Training state:  False\n",
      "---- Done in  91.89672780036926  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 97\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00235874\n",
      "====> Epoch: 2000 Average loss: 0.00207966\n",
      "====> Epoch: 3000 Average loss: 0.00197207\n",
      "====> Epoch: 4000 Average loss: 0.00191193\n",
      "====> Epoch: 5000 Average loss: 0.00189308\n",
      "====> Epoch: 6000 Average loss: 0.00183727\n",
      "====> Epoch: 7000 Average loss: 0.00180733\n",
      "====> Epoch: 8000 Average loss: 0.00180091\n",
      "====> Epoch: 9000 Average loss: 0.00178574\n",
      "====> Epoch: 10000 Average loss: 0.00175345\n",
      "Training state:  False\n",
      "---- Done in  94.68492794036865  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00276609\n",
      "====> Epoch: 2000 Average loss: 0.00239498\n",
      "====> Epoch: 3000 Average loss: 0.00222985\n",
      "====> Epoch: 4000 Average loss: 0.00220210\n",
      "====> Epoch: 5000 Average loss: 0.00214999\n",
      "====> Epoch: 6000 Average loss: 0.00208855\n",
      "====> Epoch: 7000 Average loss: 0.00208167\n",
      "====> Epoch: 8000 Average loss: 0.00201609\n",
      "====> Epoch: 9000 Average loss: 0.00200875\n",
      "====> Epoch: 10000 Average loss: 0.00200834\n",
      "Training state:  False\n",
      "---- Done in  100.66421508789062  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00300424\n",
      "====> Epoch: 2000 Average loss: 0.00276313\n",
      "====> Epoch: 3000 Average loss: 0.00263448\n",
      "====> Epoch: 4000 Average loss: 0.00252680\n",
      "====> Epoch: 5000 Average loss: 0.00250154\n",
      "====> Epoch: 6000 Average loss: 0.00247801\n",
      "====> Epoch: 7000 Average loss: 0.00244835\n",
      "====> Epoch: 8000 Average loss: 0.00240765\n",
      "====> Epoch: 9000 Average loss: 0.00239493\n",
      "====> Epoch: 10000 Average loss: 0.00240251\n",
      "Training state:  False\n",
      "---- Done in  91.1860921382904  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00267442\n",
      "====> Epoch: 2000 Average loss: 0.00211000\n",
      "====> Epoch: 3000 Average loss: 0.00204283\n",
      "====> Epoch: 4000 Average loss: 0.00201433\n",
      "====> Epoch: 5000 Average loss: 0.00196937\n",
      "====> Epoch: 6000 Average loss: 0.00195563\n",
      "====> Epoch: 7000 Average loss: 0.00193203\n",
      "====> Epoch: 8000 Average loss: 0.00190914\n",
      "====> Epoch: 9000 Average loss: 0.00198723\n",
      "====> Epoch: 10000 Average loss: 0.00189619\n",
      "Training state:  False\n",
      "---- Done in  100.36339473724365  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00300672\n",
      "====> Epoch: 2000 Average loss: 0.00283768\n",
      "====> Epoch: 3000 Average loss: 0.00278329\n",
      "====> Epoch: 4000 Average loss: 0.00276851\n",
      "====> Epoch: 5000 Average loss: 0.00270140\n",
      "====> Epoch: 6000 Average loss: 0.00267159\n",
      "====> Epoch: 7000 Average loss: 0.00265296\n",
      "====> Epoch: 8000 Average loss: 0.00263869\n",
      "====> Epoch: 9000 Average loss: 0.00259607\n",
      "====> Epoch: 10000 Average loss: 0.00259571\n",
      "Training state:  False\n",
      "---- Done in  93.8894956111908  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00296879\n",
      "====> Epoch: 2000 Average loss: 0.00258435\n",
      "====> Epoch: 3000 Average loss: 0.00223521\n",
      "====> Epoch: 4000 Average loss: 0.00196249\n",
      "====> Epoch: 5000 Average loss: 0.00182512\n",
      "====> Epoch: 6000 Average loss: 0.00175155\n",
      "====> Epoch: 7000 Average loss: 0.00170802\n",
      "====> Epoch: 8000 Average loss: 0.00165222\n",
      "====> Epoch: 9000 Average loss: 0.00168783\n",
      "====> Epoch: 10000 Average loss: 0.00166161\n",
      "Training state:  False\n",
      "---- Done in  90.19228792190552  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00271628\n",
      "====> Epoch: 2000 Average loss: 0.00235900\n",
      "====> Epoch: 3000 Average loss: 0.00226381\n",
      "====> Epoch: 4000 Average loss: 0.00211296\n",
      "====> Epoch: 5000 Average loss: 0.00208300\n",
      "====> Epoch: 6000 Average loss: 0.00205718\n",
      "====> Epoch: 7000 Average loss: 0.00202785\n",
      "====> Epoch: 8000 Average loss: 0.00206579\n",
      "====> Epoch: 9000 Average loss: 0.00213379\n",
      "====> Epoch: 10000 Average loss: 0.00200987\n",
      "Training state:  False\n",
      "---- Done in  90.69056797027588  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 98\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00291419\n",
      "====> Epoch: 2000 Average loss: 0.00259500\n",
      "====> Epoch: 3000 Average loss: 0.00246470\n",
      "====> Epoch: 4000 Average loss: 0.00230705\n",
      "====> Epoch: 5000 Average loss: 0.00214967\n",
      "====> Epoch: 6000 Average loss: 0.00205542\n",
      "====> Epoch: 7000 Average loss: 0.00198035\n",
      "====> Epoch: 8000 Average loss: 0.00193157\n",
      "====> Epoch: 9000 Average loss: 0.00189373\n",
      "====> Epoch: 10000 Average loss: 0.00183094\n",
      "Training state:  False\n",
      "---- Done in  90.86355710029602  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00288928\n",
      "====> Epoch: 2000 Average loss: 0.00250542\n",
      "====> Epoch: 3000 Average loss: 0.00233623\n",
      "====> Epoch: 4000 Average loss: 0.00231778\n",
      "====> Epoch: 5000 Average loss: 0.00222392\n",
      "====> Epoch: 6000 Average loss: 0.00219185\n",
      "====> Epoch: 7000 Average loss: 0.00216954\n",
      "====> Epoch: 8000 Average loss: 0.00210697\n",
      "====> Epoch: 9000 Average loss: 0.00207663\n",
      "====> Epoch: 10000 Average loss: 0.00209552\n",
      "Training state:  False\n",
      "---- Done in  90.30645322799683  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00294580\n",
      "====> Epoch: 2000 Average loss: 0.00246458\n",
      "====> Epoch: 3000 Average loss: 0.00216578\n",
      "====> Epoch: 4000 Average loss: 0.00210413\n",
      "====> Epoch: 5000 Average loss: 0.00201054\n",
      "====> Epoch: 6000 Average loss: 0.00200302\n",
      "====> Epoch: 7000 Average loss: 0.00204154\n",
      "====> Epoch: 8000 Average loss: 0.00189496\n",
      "====> Epoch: 9000 Average loss: 0.00194978\n",
      "====> Epoch: 10000 Average loss: 0.00190060\n",
      "Training state:  False\n",
      "---- Done in  91.02967500686646  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00268558\n",
      "====> Epoch: 2000 Average loss: 0.00231868\n",
      "====> Epoch: 3000 Average loss: 0.00213281\n",
      "====> Epoch: 4000 Average loss: 0.00182081\n",
      "====> Epoch: 5000 Average loss: 0.00170972\n",
      "====> Epoch: 6000 Average loss: 0.00166615\n",
      "====> Epoch: 7000 Average loss: 0.00164424\n",
      "====> Epoch: 8000 Average loss: 0.00166321\n",
      "====> Epoch: 9000 Average loss: 0.00161137\n",
      "====> Epoch: 10000 Average loss: 0.00161587\n",
      "Training state:  False\n",
      "---- Done in  95.600417137146  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1000 Average loss: 0.00280285\n",
      "====> Epoch: 2000 Average loss: 0.00240544\n",
      "====> Epoch: 3000 Average loss: 0.00212776\n",
      "====> Epoch: 4000 Average loss: 0.00199745\n",
      "====> Epoch: 5000 Average loss: 0.00194268\n",
      "====> Epoch: 6000 Average loss: 0.00197092\n",
      "====> Epoch: 7000 Average loss: 0.00188999\n",
      "====> Epoch: 8000 Average loss: 0.00181875\n",
      "====> Epoch: 9000 Average loss: 0.00183391\n",
      "====> Epoch: 10000 Average loss: 0.00180639\n",
      "Training state:  False\n",
      "---- Done in  102.99099206924438  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00344108\n",
      "====> Epoch: 2000 Average loss: 0.00301006\n",
      "====> Epoch: 3000 Average loss: 0.00289821\n",
      "====> Epoch: 4000 Average loss: 0.00275760\n",
      "====> Epoch: 5000 Average loss: 0.00268710\n",
      "====> Epoch: 6000 Average loss: 0.00266223\n",
      "====> Epoch: 7000 Average loss: 0.00261958\n",
      "====> Epoch: 8000 Average loss: 0.00260639\n",
      "====> Epoch: 9000 Average loss: 0.00252550\n",
      "====> Epoch: 10000 Average loss: 0.00248503\n",
      "Training state:  False\n",
      "---- Done in  100.92832779884338  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00245365\n",
      "====> Epoch: 2000 Average loss: 0.00206360\n",
      "====> Epoch: 3000 Average loss: 0.00196278\n",
      "====> Epoch: 4000 Average loss: 0.00189215\n",
      "====> Epoch: 5000 Average loss: 0.00187183\n",
      "====> Epoch: 6000 Average loss: 0.00181597\n",
      "====> Epoch: 7000 Average loss: 0.00183863\n",
      "====> Epoch: 8000 Average loss: 0.00181473\n",
      "====> Epoch: 9000 Average loss: 0.00182593\n",
      "====> Epoch: 10000 Average loss: 0.00181152\n",
      "Training state:  False\n",
      "---- Done in  96.38118886947632  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n",
      "Starting run for Dataset 99\n",
      "-- Running for model name:  A_add_lin\n",
      "====> Epoch: 1000 Average loss: 0.00296863\n",
      "====> Epoch: 2000 Average loss: 0.00243624\n",
      "====> Epoch: 3000 Average loss: 0.00221622\n",
      "====> Epoch: 4000 Average loss: 0.00210592\n",
      "====> Epoch: 5000 Average loss: 0.00202238\n",
      "====> Epoch: 6000 Average loss: 0.00197525\n",
      "====> Epoch: 7000 Average loss: 0.00196922\n",
      "====> Epoch: 8000 Average loss: 0.00192760\n",
      "====> Epoch: 9000 Average loss: 0.00188162\n",
      "====> Epoch: 10000 Average loss: 0.00189060\n",
      "Training state:  False\n",
      "---- Done in  97.77632164955139  seconds\n",
      "\n",
      "-- Running for model name:  B_add_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00271159\n",
      "====> Epoch: 2000 Average loss: 0.00238104\n",
      "====> Epoch: 3000 Average loss: 0.00222493\n",
      "====> Epoch: 4000 Average loss: 0.00215327\n",
      "====> Epoch: 5000 Average loss: 0.00222341\n",
      "====> Epoch: 6000 Average loss: 0.00210199\n",
      "====> Epoch: 7000 Average loss: 0.00204672\n",
      "====> Epoch: 8000 Average loss: 0.00201923\n",
      "====> Epoch: 9000 Average loss: 0.00204426\n",
      "====> Epoch: 10000 Average loss: 0.00202760\n",
      "Training state:  False\n",
      "---- Done in  98.81362009048462  seconds\n",
      "\n",
      "-- Running for model name:  C_add_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00309367\n",
      "====> Epoch: 2000 Average loss: 0.00280732\n",
      "====> Epoch: 3000 Average loss: 0.00263187\n",
      "====> Epoch: 4000 Average loss: 0.00242779\n",
      "====> Epoch: 5000 Average loss: 0.00228194\n",
      "====> Epoch: 6000 Average loss: 0.00218222\n",
      "====> Epoch: 7000 Average loss: 0.00212890\n",
      "====> Epoch: 8000 Average loss: 0.00211057\n",
      "====> Epoch: 9000 Average loss: 0.00207983\n",
      "====> Epoch: 10000 Average loss: 0.00203292\n",
      "Training state:  False\n",
      "---- Done in  104.69221210479736  seconds\n",
      "\n",
      "-- Running for model name:  D_mild_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00275154\n",
      "====> Epoch: 2000 Average loss: 0.00238690\n",
      "====> Epoch: 3000 Average loss: 0.00232925\n",
      "====> Epoch: 4000 Average loss: 0.00227866\n",
      "====> Epoch: 5000 Average loss: 0.00224566\n",
      "====> Epoch: 6000 Average loss: 0.00221955\n",
      "====> Epoch: 7000 Average loss: 0.00214671\n",
      "====> Epoch: 8000 Average loss: 0.00216068\n",
      "====> Epoch: 9000 Average loss: 0.00211321\n",
      "====> Epoch: 10000 Average loss: 0.00214270\n",
      "Training state:  False\n",
      "---- Done in  107.2365710735321  seconds\n",
      "\n",
      "-- Running for model name:  E_mild_nadd_mild_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00285975\n",
      "====> Epoch: 2000 Average loss: 0.00248800\n",
      "====> Epoch: 3000 Average loss: 0.00238235\n",
      "====> Epoch: 4000 Average loss: 0.00233187\n",
      "====> Epoch: 5000 Average loss: 0.00232711\n",
      "====> Epoch: 6000 Average loss: 0.00226862\n",
      "====> Epoch: 7000 Average loss: 0.00220160\n",
      "====> Epoch: 8000 Average loss: 0.00214751\n",
      "====> Epoch: 9000 Average loss: 0.00217643\n",
      "====> Epoch: 10000 Average loss: 0.00206893\n",
      "Training state:  False\n",
      "---- Done in  105.88998079299927  seconds\n",
      "\n",
      "-- Running for model name:  F_mod_nadd_lin\n",
      "====> Epoch: 1000 Average loss: 0.00305270\n",
      "====> Epoch: 2000 Average loss: 0.00259874\n",
      "====> Epoch: 3000 Average loss: 0.00240886\n",
      "====> Epoch: 4000 Average loss: 0.00226648\n",
      "====> Epoch: 5000 Average loss: 0.00221202\n",
      "====> Epoch: 6000 Average loss: 0.00212596\n",
      "====> Epoch: 7000 Average loss: 0.00208927\n",
      "====> Epoch: 8000 Average loss: 0.00202680\n",
      "====> Epoch: 9000 Average loss: 0.00194759\n",
      "====> Epoch: 10000 Average loss: 0.00195541\n",
      "Training state:  False\n",
      "---- Done in  89.56929397583008  seconds\n",
      "\n",
      "-- Running for model name:  G_mod_nadd_mod_nlin\n",
      "====> Epoch: 1000 Average loss: 0.00321625\n",
      "====> Epoch: 2000 Average loss: 0.00275576\n",
      "====> Epoch: 3000 Average loss: 0.00256860\n",
      "====> Epoch: 4000 Average loss: 0.00243957\n",
      "====> Epoch: 5000 Average loss: 0.00237775\n",
      "====> Epoch: 6000 Average loss: 0.00232554\n",
      "====> Epoch: 7000 Average loss: 0.00244329\n",
      "====> Epoch: 8000 Average loss: 0.00230817\n",
      "====> Epoch: 9000 Average loss: 0.00229468\n",
      "====> Epoch: 10000 Average loss: 0.00229974\n",
      "Training state:  False\n",
      "---- Done in  84.84703612327576  seconds\n",
      "\n",
      "================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assignment_model_names = ['A_add_lin', 'B_add_mild_nlin', 'C_add_mod_nlin', 'D_mild_nadd_lin',\n",
    "                     'E_mild_nadd_mild_nlin', 'F_mod_nadd_lin', 'G_mod_nadd_mod_nlin']\n",
    "\n",
    "for dataset_number in range(90, 100):\n",
    "    print(\"Starting run for Dataset {}\".format(dataset_number))\n",
    "    \n",
    "    for model_name in assignment_model_names:\n",
    "        print(\"-- Running for model name: \", model_name)\n",
    "        \n",
    "        start = time()\n",
    "\n",
    "        dataset = CovariateDataset(\"n_{}_model_{}_v_{}_covar_data\", [1000, model_name, dataset_number])\n",
    "\n",
    "        trained_model, original_data, binary_mu_out, normal_mu_out, mu_latent, logvar_latent = \\\n",
    "            train_model(ModifiedVAE, dataset, dataset_number,verbose=True)\n",
    "\n",
    "        encode_data(trained_model, dataset)\n",
    "\n",
    "        print(\"---- Done in \", time() - start, \" seconds\\n\")\n",
    "                \n",
    "    print(\"================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1000/10000], loss:0.4875\n",
      "epoch [2000/10000], loss:0.4456\n",
      "epoch [3000/10000], loss:0.4104\n",
      "epoch [4000/10000], loss:0.3195\n",
      "epoch [5000/10000], loss:0.3145\n",
      "epoch [6000/10000], loss:0.3127\n",
      "epoch [7000/10000], loss:0.3118\n",
      "epoch [8000/10000], loss:0.3107\n",
      "epoch [9000/10000], loss:0.3096\n",
      "epoch [10000/10000], loss:0.3084\n",
      "Final loss: loss:0.3084\n",
      "epoch [1000/10000], loss:0.5869\n",
      "epoch [2000/10000], loss:0.5415\n",
      "epoch [3000/10000], loss:0.5173\n",
      "epoch [4000/10000], loss:0.4197\n",
      "epoch [5000/10000], loss:0.4216\n",
      "epoch [6000/10000], loss:0.4136\n",
      "epoch [7000/10000], loss:0.4132\n",
      "epoch [8000/10000], loss:0.4127\n",
      "epoch [9000/10000], loss:0.4123\n",
      "epoch [10000/10000], loss:0.4118\n",
      "Final loss: loss:0.4118\n"
     ]
    }
   ],
   "source": [
    "models_to_rerun = [('A_add_lin', 12, 'sparsity'), ('G_mod_nadd_mod_nlin', 40, 'sparsity')]\n",
    "\n",
    "for model_name, dataset_number, loss_type in models_to_rerun:\n",
    "    dataset = CovariateDataset(\"n_{}_model_{}_v_{}_covar_data\", [1000, model_name, dataset_number])\n",
    "    trained_model, final_loss = train_model(\n",
    "                                        autoencoder,\n",
    "                                        dataset,\n",
    "                                        loss=loss_type,\n",
    "                                        verbose=True)\n",
    "    encode_data(trained_model, dataset, loss=loss_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
