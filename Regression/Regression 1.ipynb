{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2, 3}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([1,2,3])^set([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = \"../Data/Raw/\"\n",
    "PROCESSED_DATA_DIR = \"../Data/Processed/Regression/\"\n",
    "\n",
    "class CovariateDataset(Dataset):\n",
    "    def __init__(self, file_name_pattern, file_name_args, train=0.8):\n",
    "        self.train = True\n",
    "        \n",
    "        self.file_name = file_name_pattern.format(*file_name_args, \"covar\")\n",
    "        self.assignment_file_name = file_name_pattern.format(*file_name_args, \"assignment\")\n",
    "        \n",
    "        self.data = np.loadtxt(RAW_DATA_DIR + self.file_name + \".csv\", delimiter=\",\")[:, 1:] # remove bias\n",
    "        self.assignment_data = np.loadtxt(RAW_DATA_DIR + self.assignment_file_name + \".csv\", delimiter=\",\")\n",
    "        \n",
    "        all_indeces = list(range(len(self.data)))\n",
    "        self.train_indeces = np.random.choice(all_indeces, int(len(self.data)*train))\n",
    "        self.test_indeces = list(set(all_indeces)^set(self.train_indeces))\n",
    "    \n",
    "    def active_data(self):\n",
    "        if self.train:\n",
    "            return self.data[self.train_indeces], self.assignment_data[self.train_indeces]\n",
    "        else:\n",
    "            return self.data[self.test_indeces], self.assignment_data[self.test_indeces]\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        covar_data, assignment_data = self.active_data()\n",
    "        return (covar_data[index], assignment_data[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.active_data()[0].shape[0]\n",
    "    \n",
    "    def save_processed_data(self, data):\n",
    "        name = PROCESSED_DATA_DIR + self.file_name+\".csv\"\n",
    "        np.savetxt(name, data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on an example from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "# Extended to place a different prior on binary vs normal vars\n",
    "\n",
    "INTERMEDIATE_DIMS_1 = 1024\n",
    "INTERMEDIATE_DIMS_2 = 512\n",
    "INTERMEDIATE_DIMS_3 = 512\n",
    "INTERMEDIATE_DIMS_4 = 256\n",
    "INTERMEDIATE_DIMS_5 = 128\n",
    "INTERMEDIATE_DIMS_6 = 64\n",
    "INTERMEDIATE_DIMS_7 = 32\n",
    "\n",
    "FEATURES = 10\n",
    "\n",
    "LOSS_SCALE = 1000000\n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regressor, self).__init__()\n",
    "        \n",
    "        # ENCODER LAYERS\n",
    "        self.dense1 = nn.Linear(FEATURES, INTERMEDIATE_DIMS_1)\n",
    "        self.dense2 = nn.Linear(INTERMEDIATE_DIMS_1, INTERMEDIATE_DIMS_2)\n",
    "        self.dense3 = nn.Linear(INTERMEDIATE_DIMS_2, INTERMEDIATE_DIMS_3)\n",
    "        self.dense4 = nn.Linear(INTERMEDIATE_DIMS_3, INTERMEDIATE_DIMS_4)\n",
    "        self.dense5 = nn.Linear(INTERMEDIATE_DIMS_4, INTERMEDIATE_DIMS_5)\n",
    "        self.dense6 = nn.Linear(INTERMEDIATE_DIMS_5, INTERMEDIATE_DIMS_6)\n",
    "        self.dense7 = nn.Linear(INTERMEDIATE_DIMS_6, INTERMEDIATE_DIMS_7)\n",
    "        self.dense8 = nn.Linear(INTERMEDIATE_DIMS_7, 1)\n",
    "        \n",
    "        # Activations\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = self.dropout(self.relu(self.dense1(x)))\n",
    "        h2 = self.dropout(self.relu(self.dense2(h1)))\n",
    "        h3 = self.dropout(self.relu(self.dense3(h2)))\n",
    "        h4 = self.dropout(self.relu(self.dense4(h3)))\n",
    "        h5 = self.dropout(self.relu(self.dense5(h4)))\n",
    "        h6 = self.dropout(self.relu(self.dense6(h5)))\n",
    "        h7 = self.relu(self.dense7(h6))\n",
    "        \n",
    "        return self.sigmoid(self.dense8(h7))\n",
    "\n",
    "def train(model, optimizer, epoch, train_loader, log_results=False):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target_class) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        target_class = Variable(target_class)\n",
    "        \n",
    "        data = data.float()\n",
    "        target_class = target_class.float()\n",
    "        \n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "            target_class = target_class.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_propensity = model(data)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = F.binary_cross_entropy(output_propensity, target_class.view(-1, 1))*LOSS_SCALE\n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        # Find the gradient and descend\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if log_results:\n",
    "        print('====> Epoch: {} Average loss: {:.8f}'.format(\n",
    "              epoch, train_loss / len(train_loader.dataset)))\n",
    "        \n",
    "        \n",
    "def test(model, epoch, test_loader):\n",
    "    # toggle model to test / inference mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    for i, (data, target_class) in enumerate(test_loader):\n",
    "        data = Variable(data, volatile=True)\n",
    "        target_class = Variable(target_class, volatile=True)\n",
    "        \n",
    "        data = data.float()\n",
    "        target_class = target_class.float()\n",
    "        \n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "            target_class = target_class.cuda()\n",
    "\n",
    "        output_propensity = model(data)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = F.binary_cross_entropy(output_propensity, target_class.view(-1, 1))*LOSS_SCALE\n",
    "        test_loss += loss.data[0]\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    \n",
    "def predict(model, test_loader):\n",
    "    # Show reconstruction\n",
    "    model.eval()\n",
    "    print(\"Training state: \", model.training)\n",
    "    \n",
    "    original_data, targets = next(iter(test_loader))\n",
    "    \n",
    "    original_data = Variable(original_data)\n",
    "    original_data = original_data.float()\n",
    "    \n",
    "    if CUDA:\n",
    "        original_data = original_data.cuda()\n",
    "        \n",
    "    return original_data, targets, model(original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_class, train_set, test_set, dataset_number, verbose=True, model=None):\n",
    "    if model is None:\n",
    "        model = model_class()\n",
    "        if CUDA:\n",
    "            model = model.cuda()\n",
    "\n",
    "    num_epochs = 10000\n",
    "    train_batch_size = 400\n",
    "    test_batch_size = 200\n",
    "    learning_rate = 1e-2\n",
    "    lr_sched = True\n",
    "         \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [int(num_epochs/4), int(num_epochs/2)], gamma=0.5)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=test_batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        checkpoint_interval = int(num_epochs/10)\n",
    "        \n",
    "        if lr_sched:\n",
    "            scheduler.step()\n",
    "\n",
    "        log = False\n",
    "        if epoch%checkpoint_interval == 0:\n",
    "            log = True\n",
    "            \n",
    "        train(model, optimizer, epoch, train_loader, log_results=log)\n",
    "        if log:\n",
    "            test(model, epoch, train_loader)\n",
    "    \n",
    "    original_data, targets, output = predict(model, test_loader)\n",
    "    \n",
    "    return model, original_data, targets, output\n",
    "\n",
    "def encode_data(model, dataset):\n",
    "    original_data, output = predict(model, dataset)\n",
    "    \n",
    "    if CUDA:\n",
    "        output = output.cpu()\n",
    "    \n",
    "    output = output.numpy()\n",
    "        \n",
    "    dataset.save_processed_data(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1000 Average loss: 1688.20226563\n",
      "====> Test set loss: 1689.8445\n",
      "====> Epoch: 2000 Average loss: 1689.42031250\n",
      "====> Test set loss: 1689.8373\n",
      "====> Epoch: 3000 Average loss: 1689.78710938\n",
      "====> Test set loss: 1689.8380\n",
      "====> Epoch: 4000 Average loss: 1773.02562500\n",
      "====> Test set loss: 1689.8438\n",
      "====> Epoch: 5000 Average loss: 1690.94406250\n",
      "====> Test set loss: 1689.8412\n",
      "====> Epoch: 6000 Average loss: 1689.37773438\n",
      "====> Test set loss: 1689.8438\n",
      "====> Epoch: 7000 Average loss: 1689.54195312\n",
      "====> Test set loss: 1689.8402\n",
      "====> Epoch: 8000 Average loss: 1689.26132813\n",
      "====> Test set loss: 1689.8404\n",
      "====> Epoch: 9000 Average loss: 1687.45156250\n",
      "====> Test set loss: 1689.8413\n",
      "====> Epoch: 10000 Average loss: 1852.04562500\n",
      "====> Test set loss: 1689.8564\n",
      "Training state:  False\n"
     ]
    }
   ],
   "source": [
    "train_set = CovariateDataset(\"n_{}_model_{}_v_{}_{}_data\", [1000, \"A_add_lin\", 1])\n",
    "test_set = CovariateDataset(\"n_{}_model_{}_v_{}_{}_data\", [1000, \"A_add_lin\", 1])\n",
    "test_set.train = False\n",
    "\n",
    "trained_model, original_data, targets, output = \\\n",
    "    train_model(Regressor, train_set, test_set, 1,verbose=True)\n",
    "\n",
    "# encode_data(trained_model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([169,  31]))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = np.abs(output.data.numpy().reshape(1, -1)[0] -  targets.numpy().reshape(1, -1)[0])> 0.5\n",
    "np.unique(res, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.195"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "39/200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
