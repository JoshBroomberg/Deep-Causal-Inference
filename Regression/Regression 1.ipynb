{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_indeces = np.array(range(len(self.data)))\n",
    "# treat_indeces = all_indeces[self.assignment_data.astype(int) == 1]\n",
    "# control_indeces = all_indeces[self.assignment_data.astype(int) == 0]\n",
    "\n",
    "# self.treat_train_indeces = np.random.choice(all_indeces, int(len(self.data)*train/2), replace=False)\n",
    "# self.treat_test_indeces = list(set(treat_indeces)^set(self.treat_train_indeces))\n",
    "\n",
    "# self.control_train_indeces = np.random.choice(control_indeces, int(len(self.data)*train/2), replace=False)\n",
    "# self.control_test_indeces = list(set(control_indeces)^set(self.control_train_indeces))\n",
    "\n",
    "# self.train_indeces = np.hstack([self.treat_train_indeces, self.control_train_indeces])\n",
    "# self.test_indeces = np.hstack([self.treat_test_indeces, self.control_test_indeces])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = \"../Data/Raw/\"\n",
    "PROCESSED_DATA_DIR = \"../Data/Processed/Regression/\"\n",
    "\n",
    "class CovariateDataset(Dataset):\n",
    "    def __init__(self, file_name_pattern, file_name_args, train=0.8):\n",
    "        self.train = True\n",
    "        \n",
    "        self.file_name = file_name_pattern.format(*file_name_args, \"covar\")\n",
    "        self.assignment_file_name = file_name_pattern.format(*file_name_args, \"assignment\")\n",
    "        \n",
    "        self.data = np.loadtxt(RAW_DATA_DIR + self.file_name + \".csv\", delimiter=\",\")[:, 1:] # remove bias\n",
    "        self.assignment_data = np.loadtxt(\n",
    "            RAW_DATA_DIR + self.assignment_file_name + \".csv\", delimiter=\",\").astype(int)\n",
    "        \n",
    "        self.all_indeces = np.array(range(len(self.data)))\n",
    "        treat_indeces = self.all_indeces[self.assignment_data.astype(int) == 1]\n",
    "        control_indeces = self.all_indeces[self.assignment_data.astype(int) == 0]\n",
    "        num_training = int(len(self.data)*train)\n",
    "        \n",
    "        self.train_indeces = np.random.choice(self.all_indeces, num_training, replace=False)\n",
    "        self.test_indeces = list(set(self.all_indeces)^set(self.train_indeces))\n",
    "        \n",
    "        num_treated_in_train = len(np.intersect1d(treat_indeces, self.train_indeces, assume_unique=True))\n",
    "        num_control_in_train = num_training - num_treated_in_train\n",
    "        \n",
    "        treat_weight = num_training / (2 * num_treated_in_train)\n",
    "        control_weight = num_training / (2 * num_control_in_train)\n",
    "        \n",
    "        weighter = np.vectorize(lambda index: treat_weight if index in\\\n",
    "            treat_indeces else control_weight)\n",
    "        \n",
    "        self.weights = weighter(self.all_indeces)\n",
    "        \n",
    "        self.test_on_all = False\n",
    "        \n",
    "    def active_data(self, index=0):\n",
    "        if self.train:\n",
    "            return self.data[self.train_indeces], self.assignment_data[self.train_indeces], \\\n",
    "                self.weights[self.train_indeces][index]\n",
    "        else:\n",
    "            if self.test_on_all:\n",
    "                indeces = self.all_indeces\n",
    "            else: \n",
    "                indeces = self.test_indeces\n",
    "            \n",
    "            return self.data[indeces], self.assignment_data[indeces], 1\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        covar_data, assignment_data, weight_data = self.active_data(index)\n",
    "        class_vector = np.zeros(2)\n",
    "        class_vector[int(assignment_data[index])] = 1\n",
    "        \n",
    "        return (covar_data[index], class_vector, weight_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.active_data()[0].shape[0]\n",
    "    \n",
    "    def save_processed_data(self, data):\n",
    "        name = PROCESSED_DATA_DIR + self.file_name+\".csv\"\n",
    "        np.savetxt(name, data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on an example from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "# Extended to place a different prior on binary vs normal vars\n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regressor, self).__init__()\n",
    "        \n",
    "        INTERMEDIATE_DIMS_1 = 32\n",
    "        INTERMEDIATE_DIMS_2 = 32\n",
    "        INTERMEDIATE_DIMS_3 = 16\n",
    "        INTERMEDIATE_DIMS_4 = 16\n",
    "#         INTERMEDIATE_DIMS_5 = 16\n",
    "#         INTERMEDIATE_DIMS_6 = 8\n",
    "\n",
    "        FEATURES = 10\n",
    "\n",
    "        LOSS_SCALE = 1\n",
    "\n",
    "        # ENCODER LAYERS\n",
    "        self.dense1 = nn.Linear(FEATURES, INTERMEDIATE_DIMS_1)\n",
    "        self.dense2 = nn.Linear(INTERMEDIATE_DIMS_1, INTERMEDIATE_DIMS_2)\n",
    "        self.dense3 = nn.Linear(INTERMEDIATE_DIMS_2, INTERMEDIATE_DIMS_3)\n",
    "        self.dense4 = nn.Linear(INTERMEDIATE_DIMS_3, INTERMEDIATE_DIMS_4)\n",
    "#         self.dense5 = nn.Linear(INTERMEDIATE_DIMS_4, INTERMEDIATE_DIMS_5)\n",
    "#         self.dense6 = nn.Linear(INTERMEDIATE_DIMS_5, INTERMEDIATE_DIMS_6)\n",
    "        self.dense5 = nn.Linear(INTERMEDIATE_DIMS_4, 2)\n",
    "        \n",
    "        # Activations\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = self.dropout(self.relu(self.dense1(x)))\n",
    "        h2 = self.dropout(self.relu(self.dense2(h1)))\n",
    "        h3 = self.dropout(self.relu(self.dense3(h2)))\n",
    "        h4 = self.dropout(self.relu(self.dense4(h3)))\n",
    "#         h5 = self.dropout(self.relu(self.dense5(h4)))\n",
    "#         h6 = self.dropout(self.relu(self.dense6(h5)))\n",
    "        \n",
    "        return self.softmax(self.dense5(h4))\n",
    "\n",
    "\n",
    "\n",
    "def train(model, optimizer, epoch, train_loader, log_results=False):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target_class, weights) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        target_class = Variable(target_class)\n",
    "        weights = Variable(weights)\n",
    "        \n",
    "        data = data.float()\n",
    "        target_class = target_class.float()\n",
    "        weights = weights.float()\n",
    "        \n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "            target_class = target_class.cuda()\n",
    "            weights = weights.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_propensity = model(data)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss_criterion = nn.BCELoss(weight=weights.view(weights.shape[0], 1), size_average=False)\n",
    "        loss = loss_criterion(output_propensity, target_class)\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        # Find the gradient and descend\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if log_results:\n",
    "        print('====> Epoch: {} Average loss: {:.8f}'.format(\n",
    "              epoch, train_loss / len(train_loader.dataset)))\n",
    "        \n",
    "        \n",
    "def test(model, epoch, test_loader):\n",
    "    # toggle model to test / inference mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    for i, (data, target_class, weights) in enumerate(test_loader):\n",
    "        data = Variable(data, volatile=True)\n",
    "        target_class = Variable(target_class, volatile=True)\n",
    "        weights = Variable(weights, volatile=True)\n",
    "        \n",
    "        data = data.float()\n",
    "        target_class = target_class.float()\n",
    "        weights = weights.float()\n",
    "        \n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "            target_class = target_class.cuda()\n",
    "            weights = weights.cuda()\n",
    "\n",
    "        output_propensity = model(data)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss_criterion = nn.BCELoss(weight=weights.view(weights.shape[0], 1), size_average=False)\n",
    "        loss = loss_criterion(output_propensity, target_class)\n",
    "        test_loss += loss.data[0]\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    \n",
    "def predict(model, predict_loader):\n",
    "    # Show reconstruction\n",
    "    model.eval()\n",
    "    print(\"Training state: \", model.training)\n",
    "    \n",
    "    original_data, targets, _ = next(iter(predict_loader))\n",
    "    \n",
    "    original_data = Variable(original_data)\n",
    "    original_data = original_data.float()\n",
    "    \n",
    "    if CUDA:\n",
    "        original_data = original_data.cuda()\n",
    "        \n",
    "    return original_data, targets, model(original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_class, train_set, test_set, dataset_number, verbose=True, model=None):\n",
    "    if model is None:\n",
    "        model = model_class()\n",
    "        if CUDA:\n",
    "            model = model.cuda()\n",
    "\n",
    "    num_epochs = 1000\n",
    "    train_batch_size = 128\n",
    "    test_batch_size = 200\n",
    "    learning_rate = 1e-3\n",
    "    lr_sched = True\n",
    "         \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [int(num_epochs/10), int(num_epochs/2)], gamma=0.1)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=test_batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        checkpoint_interval = int(num_epochs/10)\n",
    "        \n",
    "        if lr_sched:\n",
    "            scheduler.step()\n",
    "\n",
    "        log = False\n",
    "        if epoch%checkpoint_interval == 0:\n",
    "            log = True\n",
    "            \n",
    "        train(model, optimizer, epoch, train_loader, log_results=log)\n",
    "        if log:\n",
    "            test(model, epoch, test_loader)\n",
    "    \n",
    "    original_data, targets, output = predict(model, test_loader)\n",
    "    \n",
    "    return model, original_data, targets, output\n",
    "\n",
    "def encode_data(model, dataset):\n",
    "    original_data, output = predict(model, dataset)\n",
    "    \n",
    "    if CUDA:\n",
    "        output = output.cpu()\n",
    "    \n",
    "    output = output.numpy()\n",
    "        \n",
    "    dataset.save_processed_data(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 100 Average loss: 1.22150821\n",
      "====> Test set loss: 1.1577\n",
      "====> Epoch: 200 Average loss: 1.21463204\n",
      "====> Test set loss: 1.1428\n",
      "====> Epoch: 300 Average loss: 1.23397925\n",
      "====> Test set loss: 1.1381\n",
      "====> Epoch: 400 Average loss: 1.19502444\n",
      "====> Test set loss: 1.1323\n",
      "====> Epoch: 500 Average loss: 1.22594293\n",
      "====> Test set loss: 1.1313\n",
      "====> Epoch: 600 Average loss: 1.19383130\n",
      "====> Test set loss: 1.1304\n",
      "====> Epoch: 700 Average loss: 1.24556785\n",
      "====> Test set loss: 1.1295\n",
      "====> Epoch: 800 Average loss: 1.21064123\n",
      "====> Test set loss: 1.1287\n",
      "====> Epoch: 900 Average loss: 1.20183202\n",
      "====> Test set loss: 1.1277\n",
      "====> Epoch: 1000 Average loss: 1.17191403\n",
      "====> Test set loss: 1.1271\n",
      "Training state:  False\n"
     ]
    }
   ],
   "source": [
    "# G_mod_nadd_mod_nlin\n",
    "import copy\n",
    "train_set = CovariateDataset(\"n_{}_model_{}_v_{}_{}_data\", [1000, \"A_add_lin\", 1], train=0.8)\n",
    "test_set = copy.deepcopy(train_set)\n",
    "test_set.train = False\n",
    "\n",
    "trained_model, original_data, targets, output = \\\n",
    "    train_model(Regressor, train_set, test_set, 1,verbose=True)\n",
    "# encode_data(trained_model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall 0.765\n",
      "Class 1:  0.8135593220338984\n",
      "Class 0:  0.6951219512195121\n"
     ]
    }
   ],
   "source": [
    "# res = np.abs(output.data.numpy().reshape(1, -1)[0] -  targets.numpy().reshape(1, -1)[0])> 0.5\n",
    "# np.unique(res, return_counts=True)\n",
    "\n",
    "classes = np.argmax(output.data.numpy(), axis=1)\n",
    "\n",
    "treat = 0\n",
    "treat_hits = 0\n",
    "control = 0\n",
    "control_hits = 0\n",
    "\n",
    "checker = lambda target, klass: target[klass] == 1\n",
    "for target, klass in (zip(targets.numpy(), classes)):\n",
    "    if klass == 1:\n",
    "        treat += 1\n",
    "        if checker(target, klass):\n",
    "            treat_hits += 1\n",
    "    else:\n",
    "        control += 1\n",
    "        if checker(target, klass):\n",
    "            control_hits += 1\n",
    "        \n",
    "print(\"Overall\", (treat_hits + control_hits)/len(classes))\n",
    "print(\"Class 1: \", treat_hits/treat)\n",
    "print(\"Class 0: \", control_hits/control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79, 0.18, 0.87, 0.77, 0.38, 0.43, 0.66, 0.97, 0.88, 0.97, 0.33,\n",
       "       0.21, 0.29, 0.76, 0.88, 0.65, 0.55, 0.6 , 0.65, 0.4 , 0.36, 0.48,\n",
       "       0.48, 0.87, 0.43, 0.09, 0.82, 0.61, 0.77, 0.34, 0.27, 0.62, 0.15,\n",
       "       0.29, 0.11, 0.81, 0.38, 0.84, 0.84, 0.91, 0.65, 0.4 , 0.33, 0.59,\n",
       "       0.28, 0.79, 0.35, 0.37, 0.28, 0.48, 0.7 , 0.37, 0.45, 0.37, 0.55,\n",
       "       0.32, 0.87, 0.29, 0.38, 0.74, 0.43, 0.31, 0.71, 0.58, 0.39, 0.33,\n",
       "       0.34, 0.51, 0.94, 0.68, 0.55, 0.53, 0.3 , 0.65, 0.51, 0.04, 0.69,\n",
       "       0.61, 0.15, 0.63, 0.18, 0.2 , 0.32, 0.18, 0.73, 0.73, 0.37, 0.39,\n",
       "       0.67, 0.65, 0.27, 0.72, 0.7 , 0.84, 0.95, 0.49, 0.38, 0.84, 0.31,\n",
       "       0.44, 0.66, 0.85, 0.11, 0.52, 0.53, 0.77, 0.74, 0.58, 0.38, 0.79,\n",
       "       0.6 , 0.57, 0.76, 0.55, 0.48, 0.81, 0.61, 0.81, 0.61, 0.34, 0.36,\n",
       "       0.78, 0.58, 0.35, 0.59, 0.23, 0.54, 0.47, 0.4 , 0.31, 0.15, 0.82,\n",
       "       0.53, 0.43, 0.89, 0.87, 0.55, 0.32, 0.63, 0.59, 0.28, 0.27, 0.78,\n",
       "       0.8 , 0.72, 0.91, 0.35, 0.68, 0.72, 0.36, 0.75, 0.77, 0.36, 0.8 ,\n",
       "       0.38, 0.58, 0.99, 0.39, 0.58, 0.47, 0.55, 0.31, 0.66, 0.33, 0.33,\n",
       "       0.2 , 0.82, 0.84, 0.47, 0.78, 0.19, 0.31, 0.39, 0.36, 0.49, 0.36,\n",
       "       0.73, 0.96, 0.49, 0.37, 0.65, 0.98, 0.68, 0.57, 0.83, 0.94, 0.66,\n",
       "       0.73, 0.35, 0.22, 0.69, 0.24, 0.9 , 0.57, 0.59, 0.54, 0.62, 0.29,\n",
       "       0.51, 0.4 ], dtype=float32)"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.round(output.data.numpy()[:, 1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
