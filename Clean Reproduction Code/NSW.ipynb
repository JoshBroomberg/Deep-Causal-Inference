{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "import pickle\n",
    "from time import time\n",
    "import os\n",
    "from itertools import product\n",
    "\n",
    "# RPY2 is used an interconnect between Python and R. It allows\n",
    "# my to run R code from python which makes this experimentation\n",
    "# process smoother.\n",
    "from rpy2.robjects import IntVector, FloatVector, Formula\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import numpy2ri\n",
    "numpy2ri.activate()\n",
    "\n",
    "stats = importr('stats') # standard regression package\n",
    "matching = importr('Matching') # GenMatch package\n",
    "snow = importr('snow') # cluster manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define an Experimental Framework\n",
    "\n",
    "### A. Cluster Compute\n",
    "\n",
    "See the simulations file for details on this cluster compute code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_MASTER_DNS=\"ip-172-31-42-147.ec2.internal\"\n",
    "AWS_SLAVE_1 = \"ubuntu@ip-172-31-43-193.ec2.internal\"\n",
    "AWS_SLAVE_2 = \"ubuntu@ip-172-31-81-244.ec2.internal\"\n",
    "AWS_MASTER_PORT_RANGE = list(range(11305, 11340))\n",
    "\n",
    "class ClusterProvider(object):\n",
    "    def __init__(self, n_nodes=8, remote_hosts=None, ports=None):\n",
    "        if remote_hosts is None:\n",
    "            self.cl = snow.makeSOCKcluster([\"localhost\"]*n_nodes)\n",
    "        else:\n",
    "            # Set the acceptable ports for connection\n",
    "            # from the slaves\n",
    "            if not ports:\n",
    "                ports = AWS_MASTER_PORT_RANGE\n",
    "            \n",
    "            # Construct the connection string\n",
    "            addresses = []\n",
    "            for remote_host, n_nodes in remote_hosts:\n",
    "                addresses+=[remote_host]*n_nodes\n",
    "                \n",
    "            self.cl = snow.makeSOCKcluster(addresses, rscript=\"Rscript\", manual=False, snowlib=\"/usr/local/lib/R/site-library\",\n",
    "                                           port=IntVector(ports), master=AWS_MASTER_DNS, outfile=\"/dev/stdout\", timeout=10)\n",
    "    \n",
    "    def get_cluster(self):\n",
    "        return self.cl\n",
    "    \n",
    "    def kill_cluster(self):\n",
    "        snow.stopCluster(self.cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local cluster\n",
    "cluster_provider = ClusterProvider(n_nodes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remote cluster\n",
    "# cluster_provider = ClusterProvider(remote_hosts=[(AWS_SLAVE_1, 8)],\n",
    "#                                     ports = list(range(11305, 11314)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this with True to kill the cluster\n",
    "kill = False # termination protection\n",
    "if kill:\n",
    "    cluster_provider.kill_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Flexible Causal Inference Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimators\n",
    "\n",
    "In this file, we copy across the GenMatch and Logistic Propensity score methods implemented in the simulations file. See the Simulations notebook for details on these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function which runs logistic regression in R\n",
    "# to determine propensity scores for a dataset. This is used\n",
    "# in the propensity score matching method and in GenMatch\n",
    "def get_propensity_scores(assignments, covariate_data):\n",
    "    # Setup\n",
    "    print(assignments.shape, covariate_data.shape)\n",
    "    y = IntVector(assignments)\n",
    "    fmla = Formula('y ~ X')\n",
    "    env = fmla.environment\n",
    "    \n",
    "    # Run propensiy regression\n",
    "    env['X'] = covariate_data\n",
    "    env['y'] = y\n",
    "    fit = stats.glm(fmla, family=\"binomial\")\n",
    "    \n",
    "    # DEBUG: fit.rx(\"coefficients\")\n",
    "    return fit.rx2(\"fitted.values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Logisic Regression Propensity Matching\n",
    "def logistic_prop_matching_est(outcomes, assignments, covariate_data, *args, **kwargs):\n",
    "    global gm_warnings\n",
    "    logistic_propensity_scores = get_propensity_scores(assignments, covariate_data)\n",
    "    \n",
    "    # Use prop scores from the neural network regression\n",
    "    # if supplied\n",
    "    nn_p_scores = kwargs.get(\"nn_p_scores\", None)\n",
    "    if nn_p_scores is not None:\n",
    "        if gm_warnings:\n",
    "            print(\"Using p-scores from neural net\")\n",
    "        propensity_scores = nn_p_scores\n",
    "    else:\n",
    "        propensity_scores = logistic_propensity_scores\n",
    "    \n",
    "    # Run matching on prop scores\n",
    "    match_out = matching.Match(\n",
    "        Y=FloatVector(outcomes),\n",
    "        Tr=IntVector(assignments),\n",
    "        X=propensity_scores,\n",
    "        replace=True)\n",
    "    \n",
    "    gm_warnings = False # only warn once\n",
    "    return (np.array(match_out.rx2(\"est\").rx(1,1))[0], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. GenMatch Matching\n",
    "def genmatch_est(outcomes, assignments, covariate_data, *args, **kwargs):\n",
    "    \n",
    "    # Get the singleton cluster\n",
    "    cl = cluster_provider.get_cluster()\n",
    "    \n",
    "    # Flag on whethert or not to use propensity scores\n",
    "    # in GenMatch\n",
    "    if kwargs.get(\"genmatch_with_prop_scores\", True):\n",
    "        print(\"Genmatch running with p scores\")\n",
    "        \n",
    "        # Parameter to allow prop scores to\n",
    "        # be derived from custom data\n",
    "        propensity_vars = kwargs.get(\"propensity_vars\", None)\n",
    "        if propensity_vars is None:\n",
    "            print(\"Finding propensity score on matching vars\")\n",
    "            propensity_vars = covariate_data\n",
    "        else:\n",
    "            print(\"Finding propensity scores with custom vars\")\n",
    "\n",
    "        logistic_p_scores = np.array(get_propensity_scores(assignments, propensity_vars))\n",
    "        \n",
    "       \n",
    "        nn_p_scores = kwargs.get(\"nn_p_scores\", None)\n",
    "        print(\"KWARGS P\", nn_p_scores)\n",
    "        # Use either the prop scores from the neural or the logistic regression\n",
    "        # or both!\n",
    "        if (nn_p_scores is not None) and kwargs.get(\"nn_p_scores_with_logistic\", False):\n",
    "            print(\"Using neural net and logistic p scores\")\n",
    "            matching_data = np.hstack([covariate_data, nn_p_scores.reshape(-1, 1),\n",
    "                                       logistic_p_scores.reshape(-1, 1)])\n",
    "            \n",
    "        elif (nn_p_scores is not None):\n",
    "            print(\"Using neural net  p scores\")\n",
    "            print(nn_p_scores.shape, covariate_data.shape)\n",
    "            matching_data = np.hstack([covariate_data, nn_p_scores.reshape(-1, 1)])\n",
    "        else:\n",
    "            print(\"Using logistic p scores\")\n",
    "            matching_data = np.hstack([covariate_data, logistic_p_scores.reshape(-1, 1)])\n",
    "             \n",
    "    else:\n",
    "        print(\"Not using prop scores\")\n",
    "        matching_data = covariate_data\n",
    "    \n",
    "    # Allow evaluation of balance on custom vars\n",
    "    balance_vars = kwargs.get(\"balance_vars\", None)\n",
    "    if balance_vars is None:\n",
    "        print(\"Evaluating balance on matching covars\")\n",
    "        balance_vars = covariate_data\n",
    "    else:\n",
    "        print(\"Evaluating balance on custom vars\")\n",
    "    \n",
    "    # Run GenMatch\n",
    "    print(\"Matching data shape:\", matching_data.shape)\n",
    "    print(\"Balance vars shape:\", balance_vars.shape)\n",
    "    \n",
    "    start = time()\n",
    "    gen_out = matching.GenMatch(\n",
    "        Tr=IntVector(assignments),\n",
    "        X=matching_data,\n",
    "        BalanceMatrix=balance_vars,\n",
    "        wait_generations=5,\n",
    "        pop_size=1000,\n",
    "        print_level=0,\n",
    "        cluster=cl)\n",
    "    \n",
    "    print(\"GenMatch Time: \", time() - start)\n",
    "    \n",
    "    match_out = matching.Match(\n",
    "        Y=FloatVector(outcomes),\n",
    "        Tr=IntVector(assignments),\n",
    "        X=matching_data,\n",
    "        replace=True,\n",
    "        Weight_matrix=gen_out)\n",
    "    \n",
    "    return np.array(match_out.rx2(\"est\").rx(1,1))[0], \\\n",
    "            np.array(gen_out.rx2(\"value\")), \\\n",
    "            np.diag(np.array(gen_out.rx2(\"Weight.matrix\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# est = logistic_prop_matching_est(assignments, X[:, 1:]) # exclude the bias term\n",
    "# np.array(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# est = genmatch_est(assignments, X[:, 1:]) # exclude the bias term\n",
    "# np.array(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Data Interconnect\n",
    "\n",
    "CSV files are used to pass information between this file and the Neural Network files. The code below defines helper functions which save data generated in this file to CSVs for neural net training and  functions which read in the processed data which results from the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = \"../Data/NSW/Raw/\"\n",
    "PROCESSED_DATA_DIR = \"../Data/NSW/Processed/\"\n",
    "VAE = \"VAE/\"\n",
    "REG = \"Regression/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create assignment, outcome and covar data files based on\n",
    "# the data available at http://users.nber.org/~rdehejia/data/nswdata2.html\n",
    "def process_raw_nsw_files():\n",
    "    treated = np.loadtxt(RAW_DATA_DIR + \"/nswre74_treated.txt\", delimiter=\"  \", dtype=str)[:, 1:].astype(float)\n",
    "    control = np.loadtxt(RAW_DATA_DIR + \"/nswre74_control.txt\", delimiter=\"  \", dtype=str)[:, 1:].astype(float)\n",
    "    cps_control = np.loadtxt(RAW_DATA_DIR + \"/cps_controls.txt\", delimiter=\"  \", dtype=str)[:, 1:].astype(float)\n",
    "\n",
    "    experimental_data = np.vstack([treated, control])\n",
    "    experimental_assignments = experimental_data[:, 0]\n",
    "    experimental_outcomes = experimental_data[:, 9]\n",
    "    experimental_covars = experimental_data[:, 1:9]\n",
    "\n",
    "    exp_effect = np.mean(experimental_outcomes[experimental_assignments==1]) - \\\n",
    "        np.mean(experimental_outcomes[experimental_assignments==0])\n",
    "    print(\"Exp treat effect:\", exp_effect)\n",
    "    assert(int(exp_effect) == 1794)\n",
    "    \n",
    "    np.savetxt(RAW_DATA_DIR + \"nsw74_experiment_assignments.csv\", experimental_assignments, delimiter=\",\")\n",
    "    np.savetxt(RAW_DATA_DIR + \"nsw74_experiment_outcomes.csv\", experimental_outcomes, delimiter=\",\")\n",
    "    np.savetxt(RAW_DATA_DIR + \"nsw74_experiment_covars.csv\", experimental_covars, delimiter=\",\")\n",
    "    \n",
    "    \n",
    "    observational_data = np.vstack([treated, cps_control])\n",
    "    observational_assignments = observational_data[:, 0]\n",
    "    observational_outcomes = observational_data[:, 9]\n",
    "    observational_covars = observational_data[:, 1:9]\n",
    "    \n",
    "    obs_effect = np.mean(observational_outcomes[observational_assignments==1]) - \\\n",
    "        np.mean(observational_outcomes[observational_assignments==0])\n",
    "    print(\"Obs treat effect:\", obs_effect)\n",
    "    \n",
    "    np.savetxt(RAW_DATA_DIR + \"nsw74_all_assignments.csv\", observational_assignments, delimiter=\",\")\n",
    "    np.savetxt(RAW_DATA_DIR + \"nsw74_all_outcomes.csv\", observational_outcomes, delimiter=\",\")\n",
    "    np.savetxt(RAW_DATA_DIR + \"nsw74_all_covars.csv\", observational_covars, delimiter=\",\")\n",
    "    \n",
    "    return exp_effect\n",
    "\n",
    "# Retrieve processed and unprocessed data from files in order to run experiments. \n",
    "def get_data_from_file(loss_type=None, nn_p_regression=False, original_mode=False):\n",
    "    \n",
    "    original_covariates = np.loadtxt(RAW_DATA_DIR + \"nsw74_all_covars.csv\", delimiter=\",\")\n",
    "    outcomes = np.loadtxt(RAW_DATA_DIR + \"nsw74_all_outcomes.csv\", delimiter=\",\")\n",
    "    assignments = np.loadtxt(RAW_DATA_DIR + \"nsw74_all_assignments.csv\", delimiter=\",\")\n",
    "    \n",
    "    extra_data = {}\n",
    "    if not (loss_type or nn_p_regression or original_mode):\n",
    "        raise Exception(\"Invalid config. Need loss type, p regression, or original mode option\")\n",
    "    \n",
    "    if not original_mode:\n",
    "        if loss_type:\n",
    "            if loss_type in [\"reconstruction\", \"sparsity\"]:\n",
    "                covariate_file_name = PROCESSED_DATA_DIR + \"nsw74_all_covars_{}.csv\".format(loss_type)\n",
    "                covariates = np.loadtxt(covariate_file_name, delimiter=\",\")\n",
    "\n",
    "            elif loss_type in [\"vae\"]:\n",
    "                covariate_file_name = PROCESSED_DATA_DIR + VAE + \"nsw74_all_covars.csv\"\n",
    "                covariates_with_std = np.loadtxt(covariate_file_name, delimiter=\",\")\n",
    "\n",
    "                # Split means and covariance\n",
    "                column_count = covariates_with_std.shape[1]\n",
    "                covar_marker = int(column_count/2)\n",
    "\n",
    "                covariates = covariates_with_std[:, :covar_marker]\n",
    "                extra_data[\"covariate_covariance\"] = covariates_with_std[:, covar_marker:]\n",
    "\n",
    "            if not nn_p_regression:\n",
    "                # If we aren't accessing NN prop scores, return now.\n",
    "                return assignments, outcomes, covariates, original_covariates, extra_data\n",
    "\n",
    "        if nn_p_regression:\n",
    "            reg_file_name = PROCESSED_DATA_DIR + REG + \"nsw74_all_covars.csv\"\n",
    "            regression_prop_scores = np.loadtxt(reg_file_name, delimiter=\",\")\n",
    "\n",
    "            extra_data[\"nn_p_scores\"] = regression_prop_scores\n",
    "\n",
    "            if loss_type:\n",
    "                return assignments, outcomes, covariates, original_covariates, extra_data\n",
    "    else:\n",
    "        # Add interaction and quadratic terms\n",
    "        var_numbers = list(range(8))\n",
    "        for i in var_numbers:\n",
    "            new_var = original_covariates[:, i]**2\n",
    "            original_covariates = np.hstack([original_covariates, new_var.reshape(-1, 1)])\n",
    "            \n",
    "        for i, j in combinations(var_numbers, 2):\n",
    "            new_var = original_covariates[:, i]*original_covariates[:, j]\n",
    "            original_covariates = np.hstack([original_covariates, new_var.reshape(-1, 1)])\n",
    "    \n",
    "    # Original data path\n",
    "    return assignments, outcomes, original_covariates, extra_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp treat effect: 1794.342404270271\n",
      "Obs treat effect: -8497.516142636992\n"
     ]
    }
   ],
   "source": [
    "# Write data files for 1000 runs all models\n",
    "# Careful with this, it writes ~3GB of data. \n",
    "process_files = True\n",
    "if process_files:\n",
    "    nsw_effect = process_raw_nsw_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16177, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-9.43083882e-01,  5.36252937e+01,  4.08237000e+01,\n",
       "        -7.54744949e+01],\n",
       "       [-2.20184159e+00,  3.22563629e+01,  2.60338554e+01,\n",
       "        -4.57855377e+01],\n",
       "       [-3.23585248e+00,  4.28735809e+01,  3.54630890e+01,\n",
       "        -5.97982407e+01],\n",
       "       ...,\n",
       "       [-8.55814453e+02,  1.31106848e+03,  1.09766016e+03,\n",
       "         1.58020435e+03],\n",
       "       [-5.30705029e+03,  1.02402266e+04,  1.05055029e+04,\n",
       "         1.43952393e+04],\n",
       "       [-7.35459766e+03,  1.04665645e+04,  8.29260254e+03,\n",
       "         1.30563252e+04]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignments, outcomes, covariates, original_covariates, extra_data = \\\n",
    "get_data_from_file(loss_type=\"reconstruction\")\n",
    "print(covariates.shape)\n",
    "covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities to store and retrive pickled results dictionaries. This allows us to persist and sync results across different machines/processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results_dict(results, name):\n",
    "    pickle.dump(results, open(\"../Results/NSW/{}.p\".format(name), \"wb\" ))\n",
    "    \n",
    "def retrieve_results_dict(name):\n",
    "    try:\n",
    "        return pickle.load(open( \"../Results/NSW/{}.p\".format(name), \"rb\" ))\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Experiment Runner Code\n",
    "\n",
    "This code is largely a duplication of the the code in the Simulations file, but simplified for a single run on the NSW dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for all three of the matching methods above. This allows the matching function\n",
    "# to be passed into the experiment running code without concern over the method API.\n",
    "def get_estimate(outcomes, assignments, covar_data, method, *args, **kwargs):\n",
    "    return method(outcomes, assignments, covar_data, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(estimator=logistic_prop_matching_est,\n",
    "                   store_name=None,\n",
    "                   overwrite=False,\n",
    "                   verbose=True,\n",
    "                   *args, **kwargs):\n",
    "    \n",
    "    global gm_warnings\n",
    "    gm_warnings = True\n",
    "    \n",
    "    result_data = None\n",
    "    \n",
    "    if store_name:\n",
    "        result_data = retrieve_results_dict(store_name)\n",
    "    \n",
    "    if overwrite or (not result_data):\n",
    "        print(\"No valid, existant results found. Beggining trial.\\n\")\n",
    "    \n",
    "        # Prepare config for matching estimators\n",
    "        balance_vars = None\n",
    "        propensity_vars = None\n",
    "        extra_data = {}\n",
    "\n",
    "        # Prepare data for matching\n",
    "        loss_type = kwargs.get(\"loss_type\", None)\n",
    "        nn_p_regression = kwargs.get(\"nn_p_regression\", None)\n",
    "        original_mode = kwargs.get(\"original_mode\", None)\n",
    "\n",
    "        if not (loss_type or nn_p_regression or original_mode):\n",
    "            raise Exception(\"Must supply loss type or p regression option to read from files\")\n",
    "\n",
    "        if loss_type:\n",
    "            assignments, outcomes, covar_data, original_covars, extra_data = \\\n",
    "                get_data_from_file(loss_type=loss_type, nn_p_regression=nn_p_regression)\n",
    "\n",
    "            if kwargs.get(\"evaluate_on_original_covars\", False):\n",
    "                balance_vars=original_covars\n",
    "\n",
    "            if kwargs.get(\"propensity_on_original_covars\", False):\n",
    "                propensity_vars=original_covars\n",
    "\n",
    "        else:\n",
    "            assignments, outcomes, covar_data, extra_data =\\\n",
    "                get_data_from_file(\n",
    "                    loss_type=loss_type,\n",
    "                    nn_p_regression=nn_p_regression,\n",
    "                    original_mode=original_mode)\n",
    "\n",
    "        # Run matching\n",
    "        print(extra_data)\n",
    "        result_tuple  = get_estimate(outcomes,\n",
    "                                  assignments,\n",
    "                                  covar_data,\n",
    "                                  estimator,\n",
    "                                  balance_vars=balance_vars,\n",
    "                                  propensity_vars=propensity_vars,\n",
    "                                  *args,\n",
    "                                  **extra_data,\n",
    "                                  **kwargs)\n",
    "            \n",
    "        result = result_tuple[0]\n",
    "        \n",
    "        bias = np.abs((nsw_effect-result)/nsw_effect * 100)\n",
    "\n",
    "        result_data = {\"result\": result, \"Bias\": bias}\n",
    "        \n",
    "        if estimator == genmatch_est:\n",
    "            result_data[\"fitnesses\"] = result_tuple[1]\n",
    "            result_data[\"weights\"] = result_tuple[2]\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Fitnesses: \", result_data[\"fitnesses\"][:10])\n",
    "                print(\"Weights: \",  result_data[\"weights\"])\n",
    "        \n",
    "        if store_name:\n",
    "            store_results_dict(result_data, store_name)\n",
    "    else:   \n",
    "        if verbose:\n",
    "            print(\"Displaying cached results.\\n\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Bias\", result_data[\"Bias\"])\n",
    "        print(\"Estimated effect ${}\".format(result_data[\"result\"]))\n",
    "        print(\"Min p:\",result_data[\"fitnesses\"][0])\n",
    "        \n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla GenMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying cached results.\n",
      "\n",
      "Bias 7.53552590659057\n",
      "Estimated effect $1929.5555409969973\n",
      "Min p: 0.18837925553825718\n"
     ]
    }
   ],
   "source": [
    "_ = run_simulation(\n",
    "    estimator=genmatch_est,\n",
    "    store_name=\"original_mode\",\n",
    "    original_mode=True,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying cached results.\n",
      "\n",
      "Bias 5.202436597037277\n",
      "Estimated effect $1887.691930186186\n",
      "Min p: 0.20652956976381498\n"
     ]
    }
   ],
   "source": [
    "_ = run_simulation(\n",
    "    estimator=genmatch_est,\n",
    "    store_name=\"original_mode_2\",\n",
    "    original_mode=True,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying cached results.\n",
      "\n",
      "Bias 4.697325755514021\n",
      "Estimated effect $1878.6285121681678\n",
      "Min p: 0.2102924241804689\n"
     ]
    }
   ],
   "source": [
    "_ = run_simulation(\n",
    "    estimator=genmatch_est,\n",
    "    store_name=\"original_mode_3\",\n",
    "    original_mode=True,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AE with only Reconstruction Loss\n",
    "\n",
    "\n",
    "#### Config 1\n",
    "\n",
    "Pure reconstruction without propensity score estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying cached results.\n",
      "\n",
      "Bias 10.552266558362124\n",
      "Estimated effect $1604.9986108019482\n",
      "Min p: 7.901347900407529e-07\n"
     ]
    }
   ],
   "source": [
    "_ =run_simulation(\n",
    "    estimator=genmatch_est,\n",
    "    loss_type=\"reconstruction\",\n",
    "    store_name=\"AE_reconstruction_plain\",\n",
    "    genmatch_with_prop_scores=False,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config 2\n",
    "Pure reconstruction *with* propensity score estimateson uncompressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying cached results.\n",
      "\n",
      "Bias 66.44590893591551\n",
      "Estimated effect $602.0752843303297\n",
      "Min p: 0.36341567821079\n"
     ]
    }
   ],
   "source": [
    "_ = run_simulation(\n",
    "    estimator=genmatch_est,\n",
    "    loss_type=\"reconstruction\",\n",
    "    store_name=\"AE_reconstruction_withp\",\n",
    "    genmatch_with_prop_scores=True,\n",
    "    propensity_on_original_covars=True,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config 3\n",
    "Pure reconstruction, with propensity scores on uncompressed, evaluating balance on uncompressed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying cached results.\n",
      "\n",
      "Bias 6.63547942007092\n",
      "Estimated effect $1913.4056252312305\n",
      "Min p: 0.022861744110306503\n"
     ]
    }
   ],
   "source": [
    "_ = run_simulation(\n",
    "    estimator=genmatch_est,\n",
    "    loss_type=\"reconstruction\",\n",
    "    store_name=\"AE_reconstruction_withp_evalonoriginal\",\n",
    "    genmatch_with_prop_scores=True,\n",
    "    propensity_on_original_covars=True,\n",
    "    evaluate_on_original_covars=True,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying cached results.\n",
      "\n",
      "Bias 5.9810171457491235\n",
      "Estimated effect $1901.6623311231228\n",
      "Min p: 0.013653451239564962\n"
     ]
    }
   ],
   "source": [
    "_ = run_simulation(\n",
    "    estimator=genmatch_est,\n",
    "    loss_type=\"reconstruction\",\n",
    "    store_name=\"AE_reconstruction_withp_evalonoriginalX\",\n",
    "    genmatch_with_prop_scores=True,\n",
    "    propensity_on_original_covars=True,\n",
    "    evaluate_on_original_covars=True,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Propensity Score Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying cached results.\n",
      "\n",
      "Bias 27.736597754240222\n",
      "Estimated effect $1296.652869264063\n",
      "Min p: 0.3241284907834834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Bias': 27.736597754240222,\n",
       " 'fitnesses': array([0.32412849, 0.37165754, 0.39241145, 0.39907434, 0.43998404,\n",
       "        0.43998404, 0.76058847, 0.82152893]),\n",
       " 'result': 1296.652869264063,\n",
       " 'weights': array([2.72854650e+02, 8.45744194e+02, 8.49180523e+02, 9.42477681e+02,\n",
       "        2.34662677e-02])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_simulation(\n",
    "    estimator=genmatch_est,\n",
    "    loss_type=\"reconstruction\",\n",
    "    store_name=\"AE_reconstruction_withnn_prop\",\n",
    "    nn_p_regression=True,\n",
    "    verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "python3_anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "433px",
    "left": "619px",
    "right": "20px",
    "top": "114px",
    "width": "451px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
