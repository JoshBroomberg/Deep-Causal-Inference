{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from time import time\n",
    "\n",
    "# Train on the GPU if one is available.\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "NSW_MODE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "\n",
    "The code below implements an Autoencoder as described in Section 3 of the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "CovariateDataset extends the base Pytorch dataset. It loads saved covariate files from the data folder and serves them to the Pytorch model during the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NSW_MODE:\n",
    "    RAW_DATA_DIR = \"../Data/NSW/Raw/\"\n",
    "    PROCESSED_DATA_DIR = \"../Data/NSW/Processed/\"\n",
    "    FILE_TYPE = \".csv\"\n",
    "    DELIM = \",\"\n",
    "else:\n",
    "    RAW_DATA_DIR = \"../Data/Raw/\"\n",
    "    PROCESSED_DATA_DIR = \"../Data/Processed/\"\n",
    "    FILE_TYPE = \".csv\"\n",
    "    DELIM = \",\"\n",
    "\n",
    "class CovariateDataset(Dataset):\n",
    "    def __init__(self, file_name_pattern, file_name_args=[]):\n",
    "        self.file_name = file_name_pattern.format(*file_name_args)\n",
    "        self.data = np.loadtxt(RAW_DATA_DIR + self.file_name + FILE_TYPE, delimiter=DELIM)\n",
    "        if not NSW_MODE:\n",
    "            self.data = self.data[:, 1:] # remove bias\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index].astype(float), 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def save_processed_data(self, data, loss):\n",
    "        name = PROCESSED_DATA_DIR + self.file_name+\"_{}.csv\".format(loss)\n",
    "        np.savetxt(name, data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CovariateDataset(\"nsw74_all_covars\").data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "The finalized Autoencoder had one hidden layer with 128 units and ReLu activations. This architecture results from experimentation with deeper networks which performed substantially worse when measured using the loss functions described below. This lines up with the theory outlined in the Section 3 of the paper and described further in Goodfellow (2016). A single hidden layer tends to work best in Autoencoders. The choice is use 4 units in the inner most layer was largely arbitrary. There is nothing which serves to guide this choice a priori. The rough rule of thumb I used was around 50% of the original number of dimensions. \n",
    "\n",
    "A summary of the complete architecture is presented below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NSW_MODE:\n",
    "    FEATURES = 8\n",
    "else:\n",
    "    FEATURES = 10\n",
    "    \n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(FEATURES, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 4))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, FEATURES))\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded_values = self.encoder(x)\n",
    "        x = self.decoder(encoded_values)\n",
    "        return x, encoded_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=128, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Linear(in_features=128, out_features=4, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Linear(in_features=128, out_features=8, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(autoencoder())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss Functions\n",
    "\n",
    "Two flavours of autoencoder were implemented in this paper. The first was an unregularized autoencoder and the second was regularized through an L1 measure of sparsity in the latent space. The loss for the first version is based purely on quality of reconstruction as measured through mean squared error between the output and input (as suggested in Goodfellow, 2016). The regularized AE adds the L1 norm of the compressed representation to the reconstruction loss. It was found that a hyper-parameter was required to scale the regularizer. Without this parameter, the model tended to produce latent representations extremely close to zero in all dimensions and had very poor reconstruction performence. IE, it disgarded most information in order to satisfy the regularization. The hyperparameter was tuned via experimentation in order to bring the reconstruction loss roughly in line with the results from the unregularized AE with some disparity allowed/encouraged as a positive result of regularization on the latent space. In the sparse encoder, perfect reconstruction is not the goal. See Section 3 above for more on this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_sparsity_loss(output, target, encoded_values):\n",
    "        l1_norm_scale = torch.FloatTensor([0.0005])\n",
    "        sparsity_scalar = Variable(l1_norm_scale)\n",
    "        if cuda:\n",
    "            sparsity_scalar = sparsity_scalar.cuda()\n",
    "            \n",
    "        mse_loss = nn.MSELoss()\n",
    "        reconstruction_loss = mse_loss(output, target)\n",
    "        sparsity_loss = encoded_values.abs().sum()*sparsity_scalar\n",
    "        return reconstruction_loss + sparsity_loss \n",
    "    \n",
    "def reconstruction_loss(output, target, encoded_values):\n",
    "    mse_loss = nn.MSELoss()\n",
    "    reconstruction_loss = mse_loss(output, target)\n",
    "    return reconstruction_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Code\n",
    "\n",
    "The code below follows a fairly standard template for training a PyTorch model. We run a specified number of epochs made up of minibatches drawn from the DataLoader instance. For each minibatch, a forward pass is performed through the model and loss is calculated. Then backpropagation is performed to find the gradient of the loss with regard to the model parameters. The ADAM optimizer is used to adjust the network parameters. \n",
    "\n",
    "Two important hyperparameters are set on the optimizer. Firstly, the learning rate, which controls how big an adjustment takes place in the weights for a given gradient and second is the weight decay which acts as a a secondary regularizer. The learning rate needs to be large enough for meaningful adjust to take place based on the information coming from the loss value but also small enough to allow convergence in the stochastic gradient descent process. The trade off between these two goals is somehwat mitigated by the implementation of learning rate annealing: the learning rate starts off relatively high (at 0.1 in this case) and is reduced by a factor of 10 after 2000 epochs and again at 500 epochs. This allows large updates to the weights during the start of training and smaller updates as the network approaches optima later on. Ideally, we would implement 'restarts' which involve increasing the learning rate and repeatedly annealing. This can prevent the network from converging in a relatively poor local optima by forcing it to explore the loss space more widely - see Huang (2017) for more on this. This was not implemented here because it is isn't a feature of PyTorch and I didn't feel it justified the effort. The value of `weight_decay` specifies how quickly the network weights should 'decay' to zero if not other update is made to them. It was found that a small value for this parameter aided in the quality of reconstruction. \n",
    "\n",
    "The batch size is set to 1000 (the size of the dataset) because the model is small enough that the GPU I was using could handle this. This improves the accuracy of the 'stochastic' gradient (which is actually the true gradient in this case given a batch is the full sample). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_functions = [\"reconstruction\", \"sparsity\"]\n",
    "\n",
    "def train_model(model_class, dataset, dataset_tag, loss=\"reconstruction\", verbose=True):\n",
    "    model = model_class()\n",
    "    if cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    if NSW_MODE:\n",
    "        num_epochs = 20000\n",
    "        batch_size = 1024\n",
    "        learning_rate = 1e-2\n",
    "        schedule = [1500, 7000]\n",
    "    else:\n",
    "        num_epochs = 10000\n",
    "        batch_size = 1000\n",
    "        learning_rate = 1e-1\n",
    "        schedule = [2000, 5000]\n",
    "    \n",
    "    lr_sched = True\n",
    "    \n",
    "    # Set the loss function\n",
    "    if loss == loss_functions[0]:\n",
    "        criterion = reconstruction_loss\n",
    "    elif loss == loss_functions[1]:\n",
    "        criterion = reconstruction_sparsity_loss\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, schedule, gamma=0.1)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    final_loss = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if lr_sched:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Iterate over batches in the epoch (only 1 in this model)\n",
    "        for data in dataloader:\n",
    "            data_batch, _ = data\n",
    "            data_batch = Variable(data_batch)\n",
    "            data_batch = data_batch.float()\n",
    "\n",
    "            if cuda:\n",
    "                data_batch = data_batch.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            output, encoded_values = model(data_batch)\n",
    "            \n",
    "            # Find the loss\n",
    "            loss = criterion(output, data_batch, encoded_values)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # ===================log========================\n",
    "        if epoch%int(num_epochs/10) == int(num_epochs/10)-1 and verbose:\n",
    "            print('epoch [{}/{}], loss:{:.4f}'\n",
    "                  .format(epoch + 1, num_epochs, loss.data[0]))\n",
    "        \n",
    "        if epoch == (num_epochs-1):\n",
    "            final_loss = loss.data[0]\n",
    "            print(\"Final loss: loss:{:.4f}\".format(final_loss))\n",
    "\n",
    "    torch.save(model.state_dict(), \"../Models/simple_autoencoder_{}.pth\".format(dataset_tag))\n",
    "    return model, final_loss\n",
    "\n",
    "def encode_data(model, dataset, loss):\n",
    "    all_data = torch.from_numpy(dataset.data)\n",
    "    all_data = Variable(all_data)\n",
    "    all_data = all_data.float()\n",
    "    \n",
    "    if cuda:\n",
    "        all_data = all_data.cuda()\n",
    "\n",
    "    output = model.encoder(all_data)\n",
    "    \n",
    "    if cuda:\n",
    "        output = output.cpu()\n",
    "        \n",
    "    dataset.save_processed_data(output.data.numpy(), loss)\n",
    "    return output.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform a single training run on Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_number = 1\n",
    "dataset = CovariateDataset(\"n_{}_model_{}_v_{}_covar_data\", [1000, \"A_add_lin\", ds_number])\n",
    "trained_model, final_loss = train_model(\n",
    "                                    autoencoder,\n",
    "                                    dataset,\n",
    "                                    ds_number,\n",
    "                                    loss=\"sparsity\",\n",
    "                                    verbose=True)\n",
    "encode_data(trained_model, dataset, \"sparsity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform a single training run on NSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [2000/20000], loss:9.4558\n",
      "epoch [4000/20000], loss:7.0777\n",
      "epoch [6000/20000], loss:9.1079\n",
      "epoch [8000/20000], loss:1.8893\n",
      "epoch [10000/20000], loss:1.6152\n",
      "epoch [12000/20000], loss:1.1869\n",
      "epoch [14000/20000], loss:0.8571\n",
      "epoch [16000/20000], loss:0.7110\n",
      "epoch [18000/20000], loss:0.6589\n",
      "epoch [20000/20000], loss:0.4651\n",
      "Final loss: loss:0.4651\n"
     ]
    }
   ],
   "source": [
    "dataset = CovariateDataset(\"nsw74_all_covars\")\n",
    "trained_model, final_loss = train_model(\n",
    "                                    autoencoder,\n",
    "                                    dataset,\n",
    "                                    \"NSW\",\n",
    "                                    loss=\"reconstruction\",\n",
    "                                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encode_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-419ff4ad7059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencode_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"reconstruction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'encode_data' is not defined"
     ]
    }
   ],
   "source": [
    "encode_data(trained_model, dataset, \"reconstruction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Encode for Monte Carlo\n",
    "\n",
    "This is utility code which trains and Autoencoder model for a given range of persisted datasets and then persistents the encoded data back to disk. This was run in order to create input for the Monte Carlo Experiment. The only nuance worth mentioning is that the function stores and reports the dataset parameters for any models which had an end loss higher than prespecified targets. This can occur by pure chance if a model gets stuck in a flat, high-loss part of the loss space. These models were rerun later and, in all cases, better network parameters which satisfied the loss criteria were found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_for_range(start, end):\n",
    "    models_to_rerun = []\n",
    "    datasets_to_process = range(start, end)\n",
    "    assignment_model_names = ['A_add_lin', 'B_add_mild_nlin', 'C_add_mod_nlin', 'D_mild_nadd_lin',\n",
    "                         'E_mild_nadd_mild_nlin', 'F_mod_nadd_lin', 'G_mod_nadd_mod_nlin']\n",
    "\n",
    "    for dataset_number in datasets_to_process:\n",
    "        print(\"Starting run for Dataset {}\".format(dataset_number))\n",
    "\n",
    "        for model_name in assignment_model_names:\n",
    "            print(\"-- Running for model name: \", model_name)\n",
    "\n",
    "            for loss_type in loss_functions:\n",
    "                print(\"---- Running for loss: \", loss_type)\n",
    "\n",
    "                start = time()\n",
    "\n",
    "                dataset = CovariateDataset(\"n_{}_model_{}_v_{}_covar_data\", [1000, model_name, dataset_number])\n",
    "                trained_model, final_loss = train_model(\n",
    "                                                    autoencoder,\n",
    "                                                    dataset,\n",
    "                                                    dataset_number,\n",
    "                                                    loss=loss_type,\n",
    "                                                    verbose=True)\n",
    "                encode_data(trained_model, dataset, loss=loss_type)\n",
    "\n",
    "                print(\"---- Done in \", time() - start, \" seconds\\n\")\n",
    "\n",
    "                # Catch bad runs\n",
    "                if loss_type == loss_functions[0] and final_loss > 0.30:\n",
    "                    models_to_rerun.append((model_name, dataset_number, loss_type))\n",
    "                elif loss_type == loss_functions[1] and final_loss > 1.0:\n",
    "                    models_to_rerun.append((model_name, dataset_number, loss_type))\n",
    "\n",
    "        print(\"================\\n\\n\")\n",
    "\n",
    "    print(\"Rerun: \", models_to_rerun)\n",
    "    return models_to_rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_name, dataset_number, loss_type in models_to_rerun:\n",
    "    dataset = CovariateDataset(\"n_{}_model_{}_v_{}_covar_data\", [1000, model_name, dataset_number])\n",
    "    trained_model, final_loss = train_model(\n",
    "                                        autoencoder,\n",
    "                                        dataset,\n",
    "                                        dataset_number,\n",
    "                                        loss=loss_type,\n",
    "                                        verbose=True)\n",
    "    encode_data(trained_model, dataset, loss=loss_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
