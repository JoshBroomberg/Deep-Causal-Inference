{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "import pickle\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results_dict(results, name):\n",
    "    pickle.dump(results, open(\"./Results/{}.p\".format(name), \"wb\" ))\n",
    "    \n",
    "def retrieve_results_dict(name):\n",
    "    try:\n",
    "        return pickle.load(open( \"./Results/{}.p\".format(name), \"rb\" ))\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL CONFIG\n",
    "\n",
    "# Var count\n",
    "n_vars = 10\n",
    "\n",
    "# Data types (default is standard normal)\n",
    "binary_indeces = [1, 3, 6, 8, 9]\n",
    "binarize = True\n",
    "\n",
    "# Associations between vars an treat/outcome\n",
    "treat_vars = [0,1,2,3,4,5,6,7]\n",
    "outcome_vars = [0,1,2,3,4,8,9,10]\n",
    "\n",
    "# Treat/outcome generation weights\n",
    "assignment_weights = np.array([0, 0.8, -0.25, 0.6, -0.4, -0.8, -0.5, 0.7])\n",
    "outcome_weights = np.array([-3.85, 0.3, -0.36, -0.73, -0.2, 0.71, -0.19, 0.26])\n",
    "true_treat_effect = -0.4\n",
    "\n",
    "def generate_data(n_samples=1000):\n",
    "    # Generate 10 Random Vars\n",
    "    # 1-4 are confounders: associated with outcome + treatment\n",
    "    # 5-7 are exposure predictors\n",
    "    # 8-10 are outcome predictors\n",
    "    X = np.random.normal(loc=0.0, scale=1.0, size=(n_samples, n_vars))\n",
    "\n",
    "    # Binarize specified vars if requested.\n",
    "    if binarize:\n",
    "        for var in binary_indeces:\n",
    "            X[:, var-1] = (X[:, var -1] > 0).astype(int)\n",
    "\n",
    "    # Add dummy for bias param     \n",
    "    X = np.hstack([np.ones((n_samples, 1)), X])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# X = generate_data(2000)\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the models\n",
    "\n",
    "assignment_models={}\n",
    "\n",
    "def nonlinear_transform(X, B, quad_indeces):\n",
    "    for quad_index in quad_indeces:\n",
    "        quad = X[:, quad_index]**2\n",
    "        X = np.hstack([X, quad.reshape(-1, 1)])\n",
    "        B = np.append(B, B[quad_index])\n",
    "    \n",
    "    return X, B\n",
    "\n",
    "def nonadditive_transform(X, B, interaction_indeces, interaction_weights=None):\n",
    "    for interaction_index, var_indeces in enumerate(interaction_indeces):\n",
    "        int_1, int_2 = var_indeces\n",
    "        interaction_val = X[:, int_1]*X[:, int_2]\n",
    "        \n",
    "        if not interaction_weights:\n",
    "            interaction_val = interaction_val*0.5\n",
    "        else:\n",
    "            interaction_val = interaction_val*interaction_weights[interaction_index]\n",
    "            \n",
    "        X = np.hstack([X, interaction_val.reshape(-1, 1)])\n",
    "        B = np.append(B, B[int_1])\n",
    "    \n",
    "    return X, B\n",
    "\n",
    "# Scenario 1\n",
    "assignment_models[\"A_add_lin\"] = lambda B, X: np.dot(X, B)\n",
    "\n",
    "# Scenario 2:     \n",
    "assignment_models[\"B_add_mild_nlin\"] = lambda B, X: np.dot(*nonlinear_transform(X, B,\n",
    "                                                       quad_indeces=[2]))\n",
    "# Scenario 3:\n",
    "assignment_models[\"C_add_mod_nlin\"] = lambda B, X: np.dot(*nonlinear_transform(X, B,\n",
    "                                                       quad_indeces=[2, 4, 7]))\n",
    "# Scenario 4:\n",
    "assignment_models[\"D_mild_nadd_lin\"] = lambda B, X: np.dot(*nonadditive_transform(X, B,\n",
    "                                                       interaction_indeces=[(1,3), (2, 4), (4,5), (5,6)]))\n",
    "\n",
    "# Scenario 5:\n",
    "assignment_models[\"E_mild_nadd_mild_nlin\"] = lambda B, X: np.dot(*nonlinear_transform(*nonadditive_transform(X, B,\n",
    "                                                       interaction_indeces=[(1,3), (2, 4), (4,5), (5,6)]), quad_indeces=[2]))\n",
    "# Scenario 6\n",
    "assignment_models[\"F_mod_nadd_lin\"] = lambda B, X: np.dot(*nonadditive_transform(X, B,\n",
    "                                                       interaction_indeces=[(1,3), (2, 4), (3,5), (4,6), (5,7), (1,6), (2,3),\n",
    "                                                                            (3,4), (4,5), (5,6)],\n",
    "                                                       interaction_weights=[0.5, 0.7, 0.5, 0.7, 0.5, 0.5, 0.7, 0.5, 0.5, 0.5]))\n",
    "# Scenario 7\n",
    "assignment_models[\"G_mod_nadd_mod_nlin\"] = lambda B, X: np.dot(*nonlinear_transform(*nonadditive_transform(X, B,\n",
    "                                                       interaction_indeces=[(1,3), (2, 4), (3,5), (4,6), (5,7), (1,6), (2,3),\n",
    "                                                                            (3,4), (4,5), (5,6)],\n",
    "                                                       interaction_weights=[0.5, 0.7, 0.5, 0.7, 0.5, 0.5, 0.7, 0.5, 0.5, 0.5]), \n",
    "                                                                            quad_indeces=[2, 4, 7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests \n",
    "assert(set(assignment_models[\"A_add_lin\"](np.array([2, 0.5, 1.5]),\n",
    "                                                np.array([[1, 2,4], [1, 10, 20]]))) == set([9, 37]))\n",
    "\n",
    "assert(set(assignment_models[\"B_add_mild_nlin\"](np.array([2, 0.5, 1.5]),\n",
    "                                                np.array([[1, 2,4], [1, 10, 20]]))) == set([33, 637]))\n",
    "\n",
    "assert(set(assignment_models[\"C_add_mod_nlin\"](np.array([2, 0.5, 1.5, 1, 1, 1, 2, 3]),\n",
    "                                                np.array([[1, 2,4,5,6,7,8,9], [1, 10, 20, 30, 40, 50, 60, 60]]))) == set([373, 13457]))\n",
    "\n",
    "assert(set(assignment_models[\"D_mild_nadd_lin\"](np.array([2, 0.5, 1.5, 1, 1, 1, 2, 3]),\n",
    "                                                np.array([[1, 2,4,5,6,7,8,9], [1, 10, 20, 30, 40, 50, 60, 60]]))) == set([139.5, 3632]))\n",
    "\n",
    "assert(set(assignment_models[\"E_mild_nadd_mild_nlin\"](np.array([2, 0.5, 1.5, 1, 1, 1, 2, 3]),\n",
    "                                                np.array([[1, 2,4,5,6,7,8,9], [1, 10, 20, 30, 40, 50, 60, 60]]))) == set([163.5, 4232]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outcome and Assignment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assignments(B, X, n_samples, scenario=\"A_add_lin\"):\n",
    "    X_usable = X[:, treat_vars]\n",
    "    \n",
    "    # Calculate the probabilities of assignment\n",
    "    linear_assignment_data = assignment_models[scenario](B, X_usable)\n",
    "    p_treat = 1.0/(1+np.exp(-1*linear_assignment_data))\n",
    "\n",
    "    # Assign\n",
    "    rand = np.random.random(n_samples)\n",
    "    assignments = (rand < p_treat).astype(int)\n",
    "    \n",
    "    return assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outcomes(B, X, assignments, effect=true_treat_effect):\n",
    "    X_usable = X[:, outcome_vars]\n",
    "    return effect*assignments + np.dot(X_usable, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# assignments = get_assignments(assignment_weights, X, \"mild_nonaddititive_mild_nonlinear\")\n",
    "# outcomes = get_outcomes(outcome_weights, X, assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpy2.robjects import IntVector, FloatVector, Formula\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import numpy2ri\n",
    "numpy2ri.activate()\n",
    "\n",
    "stats = importr('stats')\n",
    "matching = importr('Matching')\n",
    "snow = importr('snow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some code is going to >48 hours to ran. Lucky it's highly parellalisable so we can use a compute cluster. The one option is local to split across CPU cores. The better option is to go remote and explote 32 cores on multiple AWS machines.\n",
    "\n",
    "On AWS, this is straightforward. Manually you need to port forward!. This allows the remote machine to connect to ports on the master via it's localhost loopback. \n",
    "\n",
    "```\n",
    "# ~/.bash_profile\n",
    "# Allow remote host to connect to local machine\n",
    "# usage: $ remote_pfwd hostname {6000..6009}\n",
    "function remote_pfwd {\n",
    "  for i in ${@:2}\n",
    "  do\n",
    "    ssh -N -R $i:localhost:$i $1 &\n",
    "  done\n",
    "}\n",
    "```\n",
    "`remote_pfwd ubuntu@52.90.20.45 {11305..11307}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_MASTER_DNS=\"ip-172-31-42-147.ec2.internal\"\n",
    "AWS_SLAVE_1 = \"ubuntu@ip-172-31-43-193.ec2.internal\"\n",
    "AWS_SLAVE_2 = \"ubuntu@ip-172-31-81-244.ec2.internal\"\n",
    "AWS_MASTER_PORT_RANGE = list(range(11305, 11340))\n",
    "\n",
    "class ClusterProvider(object):\n",
    "    def __init__(self, n_nodes=8, remote_hosts=None, ports=None):\n",
    "        if remote_hosts is None:\n",
    "            self.cl = snow.makeSOCKcluster([\"localhost\"]*n_nodes)\n",
    "        else:\n",
    "            # Set the acceptable ports for connection\n",
    "            # from the slaves\n",
    "            if not ports:\n",
    "                ports = AWS_MASTER_PORT_RANGE\n",
    "            \n",
    "            # Construct the connection string\n",
    "            addresses = []\n",
    "            for remote_host, n_nodes in remote_hosts:\n",
    "                addresses+=[remote_host]*n_nodes\n",
    "                \n",
    "            self.cl = snow.makeSOCKcluster(addresses, rscript=\"Rscript\", manual=False, snowlib=\"/usr/local/lib/R/site-library\",\n",
    "                                           port=IntVector(ports), master=AWS_MASTER_DNS, outfile=\"/dev/stdout\", timeout=10)\n",
    "    \n",
    "    def get_cluster(self):\n",
    "        return self.cl\n",
    "    \n",
    "    def kill_cluster(self):\n",
    "        snow.stopCluster(self.cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local cluster\n",
    "cluster_provider = ClusterProvider(n_nodes=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remote cluster\n",
    "cluster_provider = ClusterProvider(remote_hosts=[(AWS_SLAVE_1, 8)],\n",
    "                                    ports = list(range(11305, 11314)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this with True to kill the cluster\n",
    "kill = True\n",
    "if kill:\n",
    "    cluster_provider.kill_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimators\n",
    "\n",
    "Define methods which can process outcomes, assignments and covariate data into a treatment effect estimate. \n",
    "\n",
    "1. Logistic Regression\n",
    "2. GenMatch\n",
    "3. VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_propensity_scores(assignments, covariate_data):\n",
    "    # Setup\n",
    "    y = IntVector(assignments)\n",
    "    fmla = Formula('y ~ X')\n",
    "    env = fmla.environment\n",
    "    \n",
    "    # Run propensiy regression\n",
    "    env['X'] = covariate_data\n",
    "    env['y'] = y\n",
    "    fit = stats.glm(fmla, family=\"binomial\")\n",
    "    \n",
    "    # DEBUG: fit.rx(\"coefficients\")\n",
    "    return fit.rx2(\"fitted.values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Logisic Regression Propensity Matching\n",
    "def logistic_prop_matching_est(outcomes, assignments, covariate_data):\n",
    "    \n",
    "    propensity_scores = get_propensity_scores(assignments, covariate_data)\n",
    "    \n",
    "    # Run matching\n",
    "    match_out = matching.Match(\n",
    "        Y=FloatVector(outcomes),\n",
    "        Tr=IntVector(assignments),\n",
    "        X=propensity_scores,\n",
    "        replace=True)\n",
    "    \n",
    "    return np.array(match_out.rx2(\"est\").rx(1,1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. GenMatch Matching\n",
    "def genmatch_est(outcomes, assignments, covariate_data):\n",
    "    \n",
    "    # Get the singleton cluster\n",
    "    cl = cluster_provider.get_cluster()\n",
    "    \n",
    "    # Add prop scores to covar data\n",
    "    propensity_scores = np.array(get_propensity_scores(assignments, covariate_data))\n",
    "    matching_data = np.hstack([covariate_data, propensity_scores.reshape(-1, 1)])\n",
    "    \n",
    "    start = time()\n",
    "    gen_out = matching.GenMatch(\n",
    "        Tr=IntVector(assignments),\n",
    "        X=matching_data,\n",
    "        BalanceMatrix=covariate_data,\n",
    "        print_level=0,\n",
    "        cluster=cl)\n",
    "    print(\"GenMatch Time: \", time() - start)\n",
    "    \n",
    "    match_out = matching.Match(\n",
    "        Y=FloatVector(outcomes),\n",
    "        Tr=IntVector(assignments),\n",
    "        X=matching_data,\n",
    "        replace=True,\n",
    "        Weight_matrix=gen_out)\n",
    "    \n",
    "    return np.array(match_out.rx2(\"est\").rx(1,1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# est = logistic_prop_matching_est(assignments, X[:, 1:]) # exclude the bias term\n",
    "# np.array(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# est = genmatch_est(assignments, X[:, 1:]) # exclude the bias term\n",
    "# np.array(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo Runner Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(n_samples, assignment_model):\n",
    "    X = generate_data(n_samples)\n",
    "    assignments = get_assignments(assignment_weights, X,\n",
    "                                  n_samples, assignment_model)\n",
    "\n",
    "    outcomes = get_outcomes(outcome_weights, X, assignments)\n",
    "    \n",
    "    return assignments, outcomes, X\n",
    "\n",
    "def get_estimate(outcomes, assignments, covar_data, method, **args):\n",
    "    return method(outcomes, assignments, covar_data, **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(runs=1000, n_samples=1000,\n",
    "                   assignment_model=\"additive_linear\",\n",
    "                   estimator=logistic_prop_matching_est,\n",
    "                   verbose=True,\n",
    "                   **args):\n",
    "    \n",
    "    progress_tick = max(1, int(runs/10))\n",
    "    results = np.zeros(runs)\n",
    "\n",
    "    for i in range(runs):\n",
    "        assignments, outcomes, covar_data = get_data(n_samples, assignment_model)\n",
    "        \n",
    "        covar_data = covar_data[:, 1:] #exclude bias term\n",
    "        results[i] = get_estimate(outcomes, assignments,\n",
    "                                  covar_data, estimator, **args)\n",
    "        \n",
    "        if i%progress_tick == progress_tick-1 and verbose:\n",
    "            print(\"Done {} of {}\".format(i+1, runs))\n",
    "    \n",
    "    bias = np.abs(np.mean((true_treat_effect-results)/true_treat_effect)*100)\n",
    "    rmse = np.mean((true_treat_effect-results)**2)**0.5\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nRMSE\", rmse)\n",
    "        print(\"Bias\", bias)\n",
    "        print(\"===============\\n\\n\")\n",
    "    \n",
    "    return {\"RMSE\": rmse, \"Bias\": bias}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages/rpy2/rinterface/__init__.py:145: RRuntimeWarning: Loading required namespace: rgenoud\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages/rpy2/rinterface/__init__.py:145: RRuntimeWarning: Loading required namespace: parallel\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenMatch Time:  11.083404302597046\n",
      "GenMatch Time:  31.12960457801819\n",
      "Done 2 of 20\n",
      "GenMatch Time:  11.981449365615845\n",
      "GenMatch Time:  16.15269660949707\n",
      "Done 4 of 20\n",
      "GenMatch Time:  14.55371356010437\n",
      "GenMatch Time:  13.442266702651978\n",
      "Done 6 of 20\n",
      "GenMatch Time:  14.062756061553955\n",
      "GenMatch Time:  15.857194662094116\n",
      "Done 8 of 20\n",
      "GenMatch Time:  8.22305178642273\n",
      "GenMatch Time:  12.44659686088562\n",
      "Done 10 of 20\n",
      "GenMatch Time:  10.008002996444702\n",
      "GenMatch Time:  13.095482110977173\n",
      "Done 12 of 20\n",
      "GenMatch Time:  11.391975164413452\n",
      "GenMatch Time:  15.019485712051392\n",
      "Done 14 of 20\n",
      "GenMatch Time:  10.63561201095581\n",
      "GenMatch Time:  14.826309204101562\n",
      "Done 16 of 20\n",
      "GenMatch Time:  13.209826946258545\n",
      "GenMatch Time:  12.118821620941162\n",
      "Done 18 of 20\n",
      "GenMatch Time:  17.138674020767212\n",
      "GenMatch Time:  10.279953002929688\n",
      "Done 20 of 20\n",
      "\n",
      "RMSE 0.043918225298051665\n",
      "Bias 0.3755681319660123\n",
      "===============\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Bias': 0.3755681319660123, 'RMSE': 0.043918225298051665}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG:\n",
    "run_simulation(runs=20, n_samples=1000, assignment_model=\"D_mild_nadd_lin\",\n",
    "              estimator=genmatch_est, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MC trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assignment_model_names = ['A_add_lin', 'B_add_mild_nlin', 'C_add_mod_nlin', 'D_mild_nadd_lin',\n",
    "                     'E_mild_nadd_mild_nlin', 'F_mod_nadd_lin', 'G_mod_nadd_mod_nlin']\n",
    "\n",
    "def get_store_name(models_being_run, est, runs, n_samples):\n",
    "    # Storage\n",
    "    \n",
    "    if set(models_being_run) == set(assignment_model_names):\n",
    "        store_name = \"est_{}_runs_{}_n_{}\".format(\n",
    "            est.__name__,\n",
    "            runs,\n",
    "            n_samples)\n",
    "    else:\n",
    "        store_name = \"est_{}_runs_{}_n_{}_models_{}\".format(\n",
    "            est.__name__,\n",
    "            runs,\n",
    "            n_samples,\n",
    "            \"_\".join(models_being_run))\n",
    "    \n",
    "    return store_name\n",
    "\n",
    "def run_test_battery(est,\n",
    "                     store_name=None, \n",
    "                     runs=1000,\n",
    "                     n_samples=1000,\n",
    "                     models=assignment_models,\n",
    "                     overwrite=False, verbosity=1,\n",
    "                     **args):\n",
    "    # Logging\n",
    "    def printer(level, *args):\n",
    "        if level <= verbosity:\n",
    "            print(*args)\n",
    "    \n",
    "    # Storage config\n",
    "    if store_name is None:\n",
    "        store_name = get_store_name(models, est, runs, n_samples)\n",
    "            \n",
    "    results = retrieve_results_dict(store_name)\n",
    "\n",
    "    if overwrite or (not results):\n",
    "        printer(1, \"No valid, existant results found. Beggining battery.\\n\")\n",
    "        results = {}\n",
    "        for model in models:\n",
    "            printer(1, \"Running: \", model)\n",
    "            results[model] = run_simulation(\n",
    "                                runs=runs,\n",
    "                                n_samples=n_samples,\n",
    "                                assignment_model=model,\n",
    "                                estimator=est,\n",
    "                                verbose=(verbosity==2),\n",
    "                                **args)\n",
    "            store_results_dict(results[model], store_name+\"_checkpoint_\"+model)\n",
    "            printer(1, \"Done.\\n\")\n",
    "\n",
    "        store_results_dict(results, store_name)\n",
    "    else:\n",
    "        printer(1, \"Displaying cached results.\\n\")\n",
    "    \n",
    "    printer(1, \"Results\")\n",
    "    for model, results in results.items():\n",
    "        printer(1, \"Model: \", model)\n",
    "        print(1, \"Bias: \", results[\"Bias\"])\n",
    "        print(1, \"RMSE: \", results[\"RMSE\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Logistic Regression Battery\n",
    "\n",
    "This one is easy. So we run on one machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying cached results.\n",
      "\n",
      "Results\n",
      "Model:  A_add_lin\n",
      "1 Bias:  0.045874914703647685\n",
      "1 RMSE:  0.07310500057973227 \n",
      "\n",
      "Model:  B_add_mild_nlin\n",
      "1 Bias:  3.1844355433209786\n",
      "1 RMSE:  0.06588422028138122 \n",
      "\n",
      "Model:  C_add_mod_nlin\n",
      "1 Bias:  10.094350684204597\n",
      "1 RMSE:  0.07650839711310455 \n",
      "\n",
      "Model:  D_mild_nadd_lin\n",
      "1 Bias:  6.720731771408928\n",
      "1 RMSE:  0.08531717119502563 \n",
      "\n",
      "Model:  E_mild_nadd_mild_nlin\n",
      "1 Bias:  10.36168716658826\n",
      "1 RMSE:  0.09094245826533698 \n",
      "\n",
      "Model:  F_mod_nadd_lin\n",
      "1 Bias:  3.1228082403965436\n",
      "1 RMSE:  0.07605107262377982 \n",
      "\n",
      "Model:  G_mod_nadd_mod_nlin\n",
      "1 Bias:  11.830178367664905\n",
      "1 RMSE:  0.07798212919046259 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_test_battery(\n",
    "    est=logistic_prop_matching_est,\n",
    "    runs=1000,\n",
    "    n_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the GenMatch Battery\n",
    "\n",
    "We split this across three machines using remote clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['est_genmatch_est_runs_1_n_200_models_A_add_lin_B_add_mild_nlin_C_add_mod_nlin',\n",
       " 'est_genmatch_est_runs_1_n_200_models_D_mild_nadd_lin_E_mild_nadd_mild_nlin',\n",
       " 'est_genmatch_est_runs_1_n_200_models_F_mod_nadd_lin_G_mod_nadd_mod_nlin']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm_est = genmatch_est\n",
    "gm_runs = 1\n",
    "gm_n_samples = 200\n",
    "gm_models_sets = [assignment_model_names[:3], assignment_model_names[3:5], assignment_model_names[5:]]\n",
    "gm_files_to_be_produced = []\n",
    "\n",
    "for model_set in gm_models_sets:\n",
    "    gm_files_to_be_produced.append(get_store_name(model_set, gm_est, gm_runs, gm_n_samples))\n",
    "\n",
    "gm_files_to_be_produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying cached results.\n",
      "\n",
      "Results\n",
      "Model:  A_add_lin\n",
      "1 Bias:  20.023235988165702\n",
      "1 RMSE:  0.08009294395266281 \n",
      "\n",
      "Model:  B_add_mild_nlin\n",
      "1 Bias:  13.717727327601459\n",
      "1 RMSE:  0.054870909310405835 \n",
      "\n",
      "Model:  C_add_mod_nlin\n",
      "1 Bias:  20.087548601352452\n",
      "1 RMSE:  0.0803501944054098 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_test_battery(\n",
    "    est=gm_est,\n",
    "    runs=gm_runs,\n",
    "    n_samples=gm_n_samples,\n",
    "    models=gm_models_sets[0],\n",
    "    verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying cached results.\n",
      "\n",
      "Results\n",
      "Model:  D_mild_nadd_lin\n",
      "1 Bias:  13.127238676118091\n",
      "1 RMSE:  0.052508954704472366 \n",
      "\n",
      "Model:  E_mild_nadd_mild_nlin\n",
      "1 Bias:  4.4125520199797545\n",
      "1 RMSE:  0.017650208079919016 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_test_battery(\n",
    "    est=gm_est,\n",
    "    runs=gm_runs,\n",
    "    n_samples=gm_n_samples,\n",
    "    models=gm_models_sets[1],\n",
    "    verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying cached results.\n",
      "\n",
      "Results\n",
      "Model:  F_mod_nadd_lin\n",
      "1 Bias:  17.275217121640445\n",
      "1 RMSE:  0.06910086848656177 \n",
      "\n",
      "Model:  G_mod_nadd_mod_nlin\n",
      "1 Bias:  0.26776452898076564\n",
      "1 RMSE:  0.0010710581159230625 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_test_battery(\n",
    "    est=gm_est,\n",
    "    runs=gm_runs,\n",
    "    n_samples=gm_n_samples,\n",
    "    models=gm_models_sets[2],\n",
    "    verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A_add_lin': {'Bias': 20.023235988165702, 'RMSE': 0.08009294395266281},\n",
       " 'B_add_mild_nlin': {'Bias': 13.717727327601459, 'RMSE': 0.054870909310405835},\n",
       " 'C_add_mod_nlin': {'Bias': 20.087548601352452, 'RMSE': 0.0803501944054098},\n",
       " 'D_mild_nadd_lin': {'Bias': 13.127238676118091, 'RMSE': 0.052508954704472366},\n",
       " 'E_mild_nadd_mild_nlin': {'Bias': 4.4125520199797545,\n",
       "  'RMSE': 0.017650208079919016},\n",
       " 'F_mod_nadd_lin': {'Bias': 17.275217121640445, 'RMSE': 0.06910086848656177},\n",
       " 'G_mod_nadd_mod_nlin': {'Bias': 0.26776452898076564,\n",
       "  'RMSE': 0.0010710581159230625}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "for file in gm_files_to_be_produced:\n",
    "    results.update(retrieve_results_dict(file))\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 FastAI",
   "language": "python",
   "name": "python3-fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "433px",
    "left": "619px",
    "right": "20px",
    "top": "114px",
    "width": "451px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
