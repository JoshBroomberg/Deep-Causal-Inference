{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "import pickle\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results_dict(results, name):\n",
    "    pickle.dump(results, open(\"./Results/{}.p\".format(name), \"wb\" ))\n",
    "    \n",
    "def retrieve_results_dict(name):\n",
    "    try:\n",
    "        return pickle.load(open( \"./Results/{}.p\".format(name), \"rb\" ))\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL CONFIG\n",
    "\n",
    "# Var count\n",
    "n_vars = 10\n",
    "\n",
    "# Data types (default is standard normal)\n",
    "binary_indeces = [1, 3, 6, 8, 9]\n",
    "binarize = True\n",
    "\n",
    "# Associations between vars an treat/outcome\n",
    "treat_vars = [0,1,2,3,4,5,6,7]\n",
    "outcome_vars = [0,1,2,3,4,8,9,10]\n",
    "\n",
    "# Treat/outcome generation weights\n",
    "assignment_weights = np.array([0, 0.8, -0.25, 0.6, -0.4, -0.8, -0.5, 0.7])\n",
    "outcome_weights = np.array([-3.85, 0.3, -0.36, -0.73, -0.2, 0.71, -0.19, 0.26])\n",
    "true_treat_effect = -0.4\n",
    "\n",
    "def generate_data(n_samples=1000):\n",
    "    # Generate 10 Random Vars\n",
    "    # 1-4 are confounders: associated with outcome + treatment\n",
    "    # 5-7 are exposure predictors\n",
    "    # 8-10 are outcome predictors\n",
    "    X = np.random.normal(loc=0.0, scale=1.0, size=(n_samples, n_vars))\n",
    "\n",
    "    # Binarize specified vars if requested.\n",
    "    if binarize:\n",
    "        for var in binary_indeces:\n",
    "            X[:, var-1] = (X[:, var -1] > 0).astype(int)\n",
    "\n",
    "    # Add dummy for bias param     \n",
    "    X = np.hstack([np.ones((n_samples, 1)), X])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# X = generate_data(2000)\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the models\n",
    "\n",
    "assignment_models={}\n",
    "\n",
    "def nonlinear_transform(X, B, quad_indeces):\n",
    "    for quad_index in quad_indeces:\n",
    "        quad = X[:, quad_index]**2\n",
    "        X = np.hstack([X, quad.reshape(-1, 1)])\n",
    "        B = np.append(B, B[quad_index])\n",
    "    \n",
    "    return X, B\n",
    "\n",
    "def nonadditive_transform(X, B, interaction_indeces, interaction_weights=None):\n",
    "    for interaction_index, var_indeces in enumerate(interaction_indeces):\n",
    "        int_1, int_2 = var_indeces\n",
    "        interaction_val = X[:, int_1]*X[:, int_2]\n",
    "        \n",
    "        if not interaction_weights:\n",
    "            interaction_val = interaction_val*0.5\n",
    "        else:\n",
    "            interaction_val = interaction_val*interaction_weights[interaction_index]\n",
    "            \n",
    "        X = np.hstack([X, interaction_val.reshape(-1, 1)])\n",
    "        B = np.append(B, B[int_1])\n",
    "    \n",
    "    return X, B\n",
    "\n",
    "# Scenario 1\n",
    "assignment_models[\"A_add_lin\"] = lambda B, X: np.dot(X, B)\n",
    "\n",
    "# Scenario 2:     \n",
    "assignment_models[\"B_add_mild_nlin\"] = lambda B, X: np.dot(*nonlinear_transform(X, B,\n",
    "                                                       quad_indeces=[2]))\n",
    "# Scenario 3:\n",
    "assignment_models[\"C_add_mod_nlin\"] = lambda B, X: np.dot(*nonlinear_transform(X, B,\n",
    "                                                       quad_indeces=[2, 4, 7]))\n",
    "# Scenario 4:\n",
    "assignment_models[\"D_mild_nadd_lin\"] = lambda B, X: np.dot(*nonadditive_transform(X, B,\n",
    "                                                       interaction_indeces=[(1,3), (2, 4), (4,5), (5,6)]))\n",
    "\n",
    "# Scenario 5:\n",
    "assignment_models[\"E_mild_nadd_mild_nlin\"] = lambda B, X: np.dot(*nonlinear_transform(*nonadditive_transform(X, B,\n",
    "                                                       interaction_indeces=[(1,3), (2, 4), (4,5), (5,6)]), quad_indeces=[2]))\n",
    "# Scenario 6\n",
    "assignment_models[\"F_mod_nadd_lin\"] = lambda B, X: np.dot(*nonadditive_transform(X, B,\n",
    "                                                       interaction_indeces=[(1,3), (2, 4), (3,5), (4,6), (5,7), (1,6), (2,3),\n",
    "                                                                            (3,4), (4,5), (5,6)],\n",
    "                                                       interaction_weights=[0.5, 0.7, 0.5, 0.7, 0.5, 0.5, 0.7, 0.5, 0.5, 0.5]))\n",
    "# Scenario 7\n",
    "assignment_models[\"G_mod_nadd_mod_nlin\"] = lambda B, X: np.dot(*nonlinear_transform(*nonadditive_transform(X, B,\n",
    "                                                       interaction_indeces=[(1,3), (2, 4), (3,5), (4,6), (5,7), (1,6), (2,3),\n",
    "                                                                            (3,4), (4,5), (5,6)],\n",
    "                                                       interaction_weights=[0.5, 0.7, 0.5, 0.7, 0.5, 0.5, 0.7, 0.5, 0.5, 0.5]), \n",
    "                                                                            quad_indeces=[2, 4, 7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests \n",
    "assert(set(assignment_models[\"A_add_lin\"](np.array([2, 0.5, 1.5]),\n",
    "                                                np.array([[1, 2,4], [1, 10, 20]]))) == set([9, 37]))\n",
    "\n",
    "assert(set(assignment_models[\"B_add_mild_nlin\"](np.array([2, 0.5, 1.5]),\n",
    "                                                np.array([[1, 2,4], [1, 10, 20]]))) == set([33, 637]))\n",
    "\n",
    "assert(set(assignment_models[\"C_add_mod_nlin\"](np.array([2, 0.5, 1.5, 1, 1, 1, 2, 3]),\n",
    "                                                np.array([[1, 2,4,5,6,7,8,9], [1, 10, 20, 30, 40, 50, 60, 60]]))) == set([373, 13457]))\n",
    "\n",
    "assert(set(assignment_models[\"D_mild_nadd_lin\"](np.array([2, 0.5, 1.5, 1, 1, 1, 2, 3]),\n",
    "                                                np.array([[1, 2,4,5,6,7,8,9], [1, 10, 20, 30, 40, 50, 60, 60]]))) == set([139.5, 3632]))\n",
    "\n",
    "assert(set(assignment_models[\"E_mild_nadd_mild_nlin\"](np.array([2, 0.5, 1.5, 1, 1, 1, 2, 3]),\n",
    "                                                np.array([[1, 2,4,5,6,7,8,9], [1, 10, 20, 30, 40, 50, 60, 60]]))) == set([163.5, 4232]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outcome and Assignment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assignments(B, X, n_samples, scenario=\"A_add_lin\"):\n",
    "    X_usable = X[:, treat_vars]\n",
    "    \n",
    "    # Calculate the probabilities of assignment\n",
    "    linear_assignment_data = assignment_models[scenario](B, X_usable)\n",
    "    p_treat = 1.0/(1+np.exp(-1*linear_assignment_data))\n",
    "\n",
    "    # Assign\n",
    "    rand = np.random.random(n_samples)\n",
    "    assignments = (rand < p_treat).astype(int)\n",
    "    \n",
    "    return assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outcomes(B, X, assignments, effect=true_treat_effect):\n",
    "    X_usable = X[:, outcome_vars]\n",
    "    return effect*assignments + np.dot(X_usable, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# assignments = get_assignments(assignment_weights, X, \"mild_nonaddititive_mild_nonlinear\")\n",
    "# outcomes = get_outcomes(outcome_weights, X, assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpy2.robjects import IntVector, FloatVector, Formula\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import numpy2ri\n",
    "numpy2ri.activate()\n",
    "\n",
    "stats = importr('stats')\n",
    "matching = importr('Matching')\n",
    "snow = importr('snow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to port forward!:\n",
    "\n",
    "```\n",
    "# ~/.bash_profile\n",
    "# Allow remote host to connect to local machine\n",
    "# usage: $ remote_pfwd hostname {6000..6009}\n",
    "function remote_pfwd {\n",
    "  for i in ${@:2}\n",
    "  do\n",
    "    ssh -N -R $i:localhost:$i $1 &\n",
    "  done\n",
    "}\n",
    "```\n",
    "`remote_pfwd ubuntu@52.90.20.45 {11305..11307}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterProvider(object):\n",
    "    def __init__(self, n_nodes=8, remote_host=None):\n",
    "        if remote_host is None:\n",
    "            self.cl = snow.makeSOCKcluster([\"localhost\"]*n_nodes)\n",
    "        else:\n",
    "            ports = list(range(11305, 11313)) + list(range(11314, 11327))\n",
    "            addresses = [remote_host]*n_nodes\n",
    "            self.cl = snow.makeSOCKcluster(addresses, rscript=\"Rscript\", manual=False, snowlib=\"/usr/local/lib/R/site-library\",\n",
    "                                           port=IntVector(ports), master=\"localhost\", outfile=\"/dev/stdout\")\n",
    "    \n",
    "    def get_cluster(self):\n",
    "        return self.cl\n",
    "    \n",
    "    def kill_cluster(self):\n",
    "        snow.stopCluster(self.cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remote_host=\"ubuntu@184.73.51.120\"\n",
    "cluster_provider = ClusterProvider(n_nodes=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_provider.kill_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimators\n",
    "\n",
    "Define methods which can process outcomes, assignments and covariate data into a treatment effect estimate. \n",
    "\n",
    "1. Logistic Regression\n",
    "2. GenMatch\n",
    "3. VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Logisic Regression Propensity Matching\n",
    "def logistic_prop_matching_est(outcomes, assignments, covariate_data):\n",
    "    # Setup\n",
    "    y = IntVector(assignments)\n",
    "    fmla = Formula('y ~ X')\n",
    "    env = fmla.environment\n",
    "    \n",
    "    # Run propensiy regression\n",
    "    env['X'] = covariate_data\n",
    "    env['y'] = y\n",
    "    fit = stats.glm(fmla, family=\"binomial\")\n",
    "    \n",
    "    # DEBUG: fit.rx(\"coefficients\")\n",
    "    propensity_scores = fit.rx2(\"fitted.values\")\n",
    "    \n",
    "    # Run matching\n",
    "    match_out = matching.Match(\n",
    "        Y=FloatVector(outcomes),\n",
    "        Tr=IntVector(assignments),\n",
    "        X=propensity_scores,\n",
    "        replace=True)\n",
    "    \n",
    "    return np.array(match_out.rx2(\"est\").rx(1,1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. GenMatch Matching\n",
    "def genmatch_est(outcomes, assignments, covariate_data):\n",
    "    \n",
    "    cl = cluster_provider.get_cluster()\n",
    "    \n",
    "    start = time()\n",
    "    gen_out = matching.GenMatch(\n",
    "        Tr=IntVector(assignments),\n",
    "        X=covariate_data,\n",
    "        # pop_size=10, max_generations=10, wait_generations=1, (use default params)\n",
    "        print_level=0,\n",
    "        cluster=cl)\n",
    "#     print(\"GenMatch Time: \", time() - start)\n",
    "    \n",
    "    match_out = matching.Match(\n",
    "        Y=FloatVector(outcomes),\n",
    "        Tr=IntVector(assignments),\n",
    "        X=covariate_data,\n",
    "        replace=True,\n",
    "        Weight_matrix=gen_out)\n",
    "    \n",
    "    return np.array(match_out.rx2(\"est\").rx(1,1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# est = logistic_prop_matching_est(assignments, X[:, 1:]) # exclude the bias term\n",
    "# np.array(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# est = genmatch_est(assignments, X[:, 1:]) # exclude the bias term\n",
    "# np.array(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo Runner Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(n_samples, assignment_model):\n",
    "    X = generate_data(n_samples)\n",
    "    assignments = get_assignments(assignment_weights, X,\n",
    "                                  n_samples, assignment_model)\n",
    "\n",
    "    outcomes = get_outcomes(outcome_weights, X, assignments)\n",
    "    \n",
    "    return assignments, outcomes, X\n",
    "\n",
    "def get_estimate(outcomes, assignments, covar_data, method):\n",
    "    return method(outcomes, assignments, covar_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(runs=1000, n_samples=1000,\n",
    "                   assignment_model=\"additive_linear\",\n",
    "                   estimator=logistic_prop_matching_est,\n",
    "                   verbose=True):\n",
    "    \n",
    "    progress_tick = max(1, int(runs/10))\n",
    "    results = np.zeros(runs)\n",
    "\n",
    "    for i in range(runs):\n",
    "        assignments, outcomes, covar_data = get_data(n_samples, assignment_model)\n",
    "        \n",
    "        covar_data = covar_data[:, 1:] #exclude bias term\n",
    "        results[i] = get_estimate(outcomes, assignments,\n",
    "                                  covar_data, estimator)\n",
    "        \n",
    "        if i%progress_tick == progress_tick-1 and verbose:\n",
    "            print(\"Done {} of {}\".format(i+1, runs))\n",
    "    \n",
    "    bias = np.abs(np.mean((true_treat_effect-results)/true_treat_effect)*100)\n",
    "    rmse = np.mean((true_treat_effect-results)**2)**0.5\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nRMSE\", rmse)\n",
    "        print(\"Bias\", bias)\n",
    "        print(\"===============\\n\\n\")\n",
    "    \n",
    "    return {\"RMSE\": rmse, \"Bias\": bias}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages/rpy2/rinterface/__init__.py:145: RRuntimeWarning: Loading required namespace: rgenoud\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages/rpy2/rinterface/__init__.py:145: RRuntimeWarning: Loading required namespace: parallel\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1 of 5\n",
      "Done 2 of 5\n",
      "Done 3 of 5\n",
      "Done 4 of 5\n",
      "Done 5 of 5\n",
      "\n",
      "RMSE 0.05338280524745419\n",
      "Bias 6.914048384306362\n",
      "===============\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Bias': 6.914048384306362, 'RMSE': 0.05338280524745419}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG:\n",
    "run_simulation(runs=5, n_samples=1000, assignment_model=\"A_add_lin\",\n",
    "              estimator=genmatch_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MC trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assignment_model_names = ['A_add_lin', 'B_add_mild_nlin', 'C_add_mod_nlin', 'D_mild_nadd_lin',\n",
    "                     'E_mild_nadd_mild_nlin', 'F_mod_nadd_lin', 'G_mod_nadd_mod_nlin']\n",
    "def run_test_battery(est,\n",
    "                     store_name=None, \n",
    "                     runs=1000,\n",
    "                     n_samples=1000,\n",
    "                     models=assignment_models,\n",
    "                     overwrite=False, verbosity=1):\n",
    "    # Logging\n",
    "    def printer(level, *args):\n",
    "        if level <= verbosity:\n",
    "            print(*args)\n",
    "    \n",
    "    # Storage\n",
    "    if store_name is None:\n",
    "        if set(models) == set(assignment_model_names):\n",
    "            store_name = \"est_{}_runs_{}_n_{}\".format(\n",
    "                est.__name__,\n",
    "                runs,\n",
    "                n_samples)\n",
    "        else:\n",
    "            store_name = \"est_{}_runs_{}_n_{}_models_{}\".format(\n",
    "                est.__name__,\n",
    "                runs,\n",
    "                n_samples,\n",
    "                \"_\".join(models))\n",
    "\n",
    "    results = retrieve_results_dict(store_name)\n",
    "\n",
    "    if overwrite or (not results):\n",
    "        printer(1, \"No valid, existant results found. Beggining battery.\\n\")\n",
    "        results = {}\n",
    "        for model in models:\n",
    "            printer(1, \"Running: \", model)\n",
    "            results[model] = run_simulation(\n",
    "                                runs=runs,\n",
    "                                n_samples=n_samples,\n",
    "                                assignment_model=model,\n",
    "                                estimator=est,\n",
    "                                verbose=(verbosity==2))\n",
    "            store_results_dict(results[model], store_name+\"_checkpoint_\"+model)\n",
    "            printer(1, \"Done.\\n\")\n",
    "\n",
    "        store_results_dict(results, store_name)\n",
    "    else:\n",
    "        printer(1, \"Displaying cached results.\\n\")\n",
    "    \n",
    "    printer(1, \"Results\")\n",
    "    for model, results in results.items():\n",
    "        printer(1, \"Model: \", model)\n",
    "        print(1, \"Bias: \", results[\"Bias\"])\n",
    "        print(1, \"RMSE: \", results[\"RMSE\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying cached results.\n",
      "\n",
      "Results\n",
      "Model:  A_add_lin\n",
      "1 Bias:  0.045874914703647685\n",
      "1 RMSE:  0.07310500057973227 \n",
      "\n",
      "Model:  B_add_mild_nlin\n",
      "1 Bias:  3.1844355433209786\n",
      "1 RMSE:  0.06588422028138122 \n",
      "\n",
      "Model:  C_add_mod_nlin\n",
      "1 Bias:  10.094350684204597\n",
      "1 RMSE:  0.07650839711310455 \n",
      "\n",
      "Model:  D_mild_nadd_lin\n",
      "1 Bias:  6.720731771408928\n",
      "1 RMSE:  0.08531717119502563 \n",
      "\n",
      "Model:  E_mild_nadd_mild_nlin\n",
      "1 Bias:  10.36168716658826\n",
      "1 RMSE:  0.09094245826533698 \n",
      "\n",
      "Model:  F_mod_nadd_lin\n",
      "1 Bias:  3.1228082403965436\n",
      "1 RMSE:  0.07605107262377982 \n",
      "\n",
      "Model:  G_mod_nadd_mod_nlin\n",
      "1 Bias:  11.830178367664905\n",
      "1 RMSE:  0.07798212919046259 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_test_battery(\n",
    "    est=logistic_prop_matching_est,\n",
    "    runs=1000,\n",
    "    n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid, existant results found. Beggining battery.\n",
      "\n",
      "Running:  E_mild_nadd_mild_nlin\n",
      "Done.\n",
      "\n",
      "Running:  F_mod_nadd_lin\n",
      "Done.\n",
      "\n",
      "Running:  G_mod_nadd_mod_nlin\n",
      "Done.\n",
      "\n",
      "Results\n",
      "Model:  E_mild_nadd_mild_nlin\n",
      "1 Bias:  8.787579142475641\n",
      "1 RMSE:  0.07120373820325623 \n",
      "\n",
      "Model:  F_mod_nadd_lin\n",
      "1 Bias:  13.91407083491776\n",
      "1 RMSE:  0.08401017158314694 \n",
      "\n",
      "Model:  G_mod_nadd_mod_nlin\n",
      "1 Bias:  13.39721795200823\n",
      "1 RMSE:  0.07772022650156114 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_test_battery(\n",
    "    est=genmatch_est,\n",
    "    runs=1000,\n",
    "    n_samples=1000,\n",
    "    models=assignment_model_names[4:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 FastAI",
   "language": "python",
   "name": "python3-fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "433px",
    "left": "619px",
    "right": "20px",
    "top": "114px",
    "width": "451px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
